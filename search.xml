<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Kafka之获取指定Topic-Partition的Leader</title>
    <url>/2020/03/17/Kafka%E4%B9%8B%E8%8E%B7%E5%8F%96%E6%8C%87%E5%AE%9ATopic-Partition%E7%9A%84Leader/</url>
    <content><![CDATA[<hr>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul>
<li>kafka版本：2.11-2.1.1</li>
<li>基于Kafka Java API</li>
<li>主要是通过封装<code>Consumer.partitionsFor</code>方法实现此功能</li>
</ul>
<a id="more"></a>

<hr>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;</span></span><br><span class="line"><span class="comment">     * 用于寻找指定Brokers-Topic-Partition对应的Leader.</span></span><br><span class="line"><span class="comment">     * 主要是封装了&#123;<span class="doctag">@link</span> KafkaConsumer#partitionsFor&#125;方法.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> brokers   指定的Kafka集群,格式为&lt;"host","port"&gt;</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topic     指定的Topic</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> partition 指定的Partition ID</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> Node &#123;<span class="doctag">@link</span> Node&#125;</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@date</span> 2020/3/17</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Node <span class="title">findLeader</span><span class="params">(Map&lt;String, Integer&gt; brokers, String topic, <span class="keyword">int</span> partition)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 1.通过接收到的配置信息创建KafkaConsumer实例</span></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        <span class="keyword">for</span> (String broker : brokers.keySet()) &#123;</span><br><span class="line">            props.put(<span class="string">"bootstrap.servers"</span>,</span><br><span class="line">                    broker + <span class="string">":"</span> + brokers.getOrDefault(broker, <span class="number">9092</span>));</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 指定Consumer所属的ConsumerGroup</span></span><br><span class="line">        props.put(<span class="string">"group.id"</span>, <span class="string">"test"</span>);</span><br><span class="line">        <span class="comment">// 设置手动提交Offset,由于只是获取部分数据所以不需要自动提交Offset</span></span><br><span class="line">        props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"false"</span>);</span><br><span class="line">        <span class="comment">// 设置键值对&lt;K,V&gt;反序列化器,使用String对象反序列化器</span></span><br><span class="line">        props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.定义KafkaConsumer实例,采用try&#123;&#125;catch&#123;&#125;finally&#123;&#125;的方式获取资源</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 3.创建KafkaConsumer实例</span></span><br><span class="line">            kafkaConsumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">            <span class="comment">// 4.通过KafkaConsumer实例获取指定Topic对应的Partition信息</span></span><br><span class="line">            List&lt;PartitionInfo&gt; partitionInfos = kafkaConsumer.partitionsFor(topic);</span><br><span class="line">            <span class="comment">// 5.遍历返回的指定Topic的所有Partition信息</span></span><br><span class="line">            <span class="keyword">for</span> (PartitionInfo partitionInfo : partitionInfos) &#123;</span><br><span class="line">                <span class="comment">// 若当前Partition是指定的Partition,则保存此Partition的Replication并将其Leader返回</span></span><br><span class="line">                <span class="keyword">if</span> (partitionInfo.partition() == partition) &#123;</span><br><span class="line">                    <span class="comment">/*// 保存指定Partition的所有副本</span></span><br><span class="line"><span class="comment">                    replications = partitionInfo.replicas();*/</span></span><br><span class="line">                    <span class="comment">// 6.返回指定的TopicPartition对应的Leader</span></span><br><span class="line">                    <span class="keyword">return</span> partitionInfo.leader();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (kafkaConsumer != <span class="keyword">null</span>) kafkaConsumer.close();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 如果没有找到Leader则返回null</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Kafka</tag>
        <tag>Practice</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka之获取指定Topic-Partition的lastOffset</title>
    <url>/2020/03/17/Kafka%E4%B9%8B%E8%8E%B7%E5%8F%96%E6%8C%87%E5%AE%9ATopic-Partition%E7%9A%84lastOffset/</url>
    <content><![CDATA[<hr>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul>
<li>kafka版本：2.11-2.1.1</li>
<li>基于Kafka Java API</li>
<li>主要是通过封装<code>Conusmer.endOffsets</code>方法实现此功能</li>
</ul>
<a id="more"></a>

<hr>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;根据指定的Topic-Partition获取对应的endOffset.</span></span><br><span class="line"><span class="comment">     * 主要是封装了KafkaConsumer的&#123;<span class="doctag">@link</span> KafkaConsumer#endOffsets&#125;方法.</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;endOffsets Javadoc:</span></span><br><span class="line"><span class="comment">     * Get the end offsets for the given partitions. In the default &#123;<span class="doctag">@code</span> read_uncommitted&#125; isolation level, the end</span></span><br><span class="line"><span class="comment">     * offset is the high watermark (that is, the offset of the last successfully replicated message plus one). For</span></span><br><span class="line"><span class="comment">     * &#123;<span class="doctag">@code</span> read_committed&#125; consumers, the end offset is the last stable offset (LSO), which is the minimum of</span></span><br><span class="line"><span class="comment">     * the high watermark and the smallest offset of any open transaction. Finally, if the partition has never been</span></span><br><span class="line"><span class="comment">     * written to, the end offset is 0.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> brokers   指定的Kafka集群</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topic     指定的Topic</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> partition 指定的Partition ID</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> TopicPartition对应最新消息的Offset</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@date</span> 2020/3/17</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> KafkaConsumer#endOffsets</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">long</span> <span class="title">getLastOffset</span><span class="params">(Map&lt;String, Integer&gt; brokers, String topic, <span class="keyword">int</span> partition)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 1.配置Consumer</span></span><br><span class="line">        <span class="comment">// 通过接收到的配置信息创建KafkaConsumer实例</span></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        <span class="keyword">for</span> (String broker : brokers.keySet()) &#123;</span><br><span class="line">            props.put(<span class="string">"bootstrap.servers"</span>,</span><br><span class="line">                    broker + <span class="string">":"</span> + brokers.getOrDefault(broker, <span class="number">9092</span>));</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 指定Consumer所属的ConsumerGroup</span></span><br><span class="line">        props.put(<span class="string">"group.id"</span>, <span class="string">"test"</span>);</span><br><span class="line">        <span class="comment">// 设置手动提交Offset,由于只是获取部分数据所以不需要自动提交Offset</span></span><br><span class="line">        props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"false"</span>);</span><br><span class="line">        <span class="comment">// 设置键值对&lt;K,V&gt;反序列化器,直接使用String对象反序列化器</span></span><br><span class="line">        props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.定义消费者KafkaConsumer对象</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">null</span>;</span><br><span class="line">        <span class="comment">// 3.创建TopicPartition实例</span></span><br><span class="line">        TopicPartition topicPartition = <span class="keyword">new</span> TopicPartition(topic, partition);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 4.创建kafkaTopic实例</span></span><br><span class="line">            kafkaConsumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">            <span class="comment">// 5.通过Consumer.endOffsets(Collections&lt;TopicPartition&gt;)方法获取</span></span><br><span class="line">            <span class="comment">// 指定TopicPartition对应的lastOffset</span></span><br><span class="line">            Map offsets = kafkaConsumer.endOffsets(</span><br><span class="line">                    Collections.singletonList(topicPartition));</span><br><span class="line">            <span class="comment">// 6.返回对应Partition的Offset,如果不存在则直接返回0</span></span><br><span class="line">            <span class="keyword">return</span> (Long) offsets.getOrDefault(topicPartition, <span class="number">0</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (kafkaConsumer != <span class="keyword">null</span>) kafkaConsumer.close();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 获取lastOffset失败则返回-1</span></span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Kafka</tag>
        <tag>Practice</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka之读取指定Topic-Partition-Offset之后的消息</title>
    <url>/2020/03/17/Kafka%E4%B9%8B%E8%AF%BB%E5%8F%96%E6%8C%87%E5%AE%9ATopic-Partition-Offset%E4%B9%8B%E5%90%8E%E7%9A%84%E6%B6%88%E6%81%AF/</url>
    <content><![CDATA[<hr>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul>
<li>kafka版本：2.11-2.1.1</li>
<li>基于Kafka Java API</li>
<li>主要是通过封装<code>Conusmer.assign</code>和<code>Consumer.seek</code>方法实现此功能。前者的作用是将指定Topic-Partition分配给对应的Consumer，后者的作用是将Consumer在指定的Topic-Partition中的Offset置于指定位置</li>
</ul>
<a id="more"></a>

<hr>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;主要用于从指定的Topic-Partition-Offset读取指定数量的消息.</span></span><br><span class="line"><span class="comment">     * 主要是用于获取部分数据,而不是持续消费数据.主要是封装了&#123;<span class="doctag">@link</span> KafkaConsumer#assign&#125;</span></span><br><span class="line"><span class="comment">     * 和&#123;<span class="doctag">@link</span> KafkaConsumer#seek&#125;方法.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> brokers     指定的Kafka集群</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topic       指定消费的Topic</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> partition   指定消费的Partition ID</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> beginOffset 指定开始消费的Offset</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> timeInMills 指定消费的时长</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> maxReads    指定消费的最大消息数</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@date</span> 2020/3/17</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> List&lt;ConsumerRecord&gt; <span class="title">getRecordsFrom</span><span class="params">(Map&lt;String, Integer&gt; brokers, String topic, <span class="keyword">int</span> partition, <span class="keyword">long</span> beginOffset, <span class="keyword">long</span> timeInMills, <span class="keyword">long</span> maxReads)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 1.通过接收到的配置信息创建KafkaConsumer实例</span></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        <span class="keyword">for</span> (String broker : brokers.keySet()) &#123;</span><br><span class="line">            props.put(<span class="string">"bootstrap.servers"</span>,</span><br><span class="line">                    broker + <span class="string">":"</span> + brokers.getOrDefault(broker, <span class="number">9092</span>));</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 设置手动提交Offset,由于只是获取部分数据所以不需要自动提交Offset</span></span><br><span class="line">        props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"false"</span>);</span><br><span class="line">        <span class="comment">// 设置键值对&lt;K,V&gt;反序列化器,因为都是String类型,所以使用String对象反序列化器</span></span><br><span class="line">        props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.定义KafkaConsumer对象,以及创建用于返回的ConsumerRecord集合</span></span><br><span class="line">        KafkaConsumer&lt;Object, Object&gt; kafkaConsumer = <span class="keyword">null</span>;</span><br><span class="line">        List&lt;ConsumerRecord&gt; recordList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 3.创建KafkaConsumer实例</span></span><br><span class="line">            kafkaConsumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">            <span class="comment">// 4.创建TopicPartition实例</span></span><br><span class="line">            TopicPartition topicPartition = <span class="keyword">new</span> TopicPartition(topic, partition);</span><br><span class="line">            <span class="comment">// 5.给Consumer指定消费的Topic和Partition(十分重要)</span></span><br><span class="line">            <span class="comment">// 如果无对应topic或者partition,则会抛出异常IllegalArgumentException</span></span><br><span class="line">            <span class="comment">// 如果此consumer之前已经有过订阅行为且未解除之前所有的订阅,则会抛出异常IllegalStateException</span></span><br><span class="line">            kafkaConsumer.assign(Collections.singletonList(topicPartition));</span><br><span class="line">            <span class="comment">// 6.覆盖原始的Consumer-Topic-Partition对应的Offset,将其设置为指定Offset值</span></span><br><span class="line">            kafkaConsumer.seek(topicPartition, beginOffset);</span><br><span class="line">            <span class="comment">// 7.获取消息</span></span><br><span class="line">            ConsumerRecords&lt;Object, Object&gt; consumerRecords =</span><br><span class="line">                    kafkaConsumer.poll(Duration.ofMillis(timeInMills));</span><br><span class="line">            <span class="comment">// 8.处理消息</span></span><br><span class="line">            <span class="comment">// 将Consumer获取到的消息保存在集合中准备返回</span></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord record : consumerRecords) &#123;</span><br><span class="line">                <span class="comment">// 如果已经读完最多消息限制则直接跳出循环</span></span><br><span class="line">                <span class="keyword">if</span> (recordList.size() == maxReads) <span class="keyword">break</span>;</span><br><span class="line">                <span class="comment">// 如果还需要读取则继续读取</span></span><br><span class="line">                recordList.add(record);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            <span class="comment">// 9.释放资源</span></span><br><span class="line">            <span class="keyword">if</span> (kafkaConsumer != <span class="keyword">null</span>) kafkaConsumer.close();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> recordList;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Kafka</tag>
        <tag>Practice</tag>
      </tags>
  </entry>
  <entry>
    <title>SSH连接服务器时出现Permission denied (publickey,gssapi-keyex,gssapi-with-mic)</title>
    <url>/2020/03/15/SSH%E8%BF%9E%E6%8E%A5%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%97%B6%E5%87%BA%E7%8E%B0Permission%20denied%20(publickey,gssapi-keyex,gssapi-with-mic)/</url>
    <content><![CDATA[<hr>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>SSH工具连接服务器的身份验证方式有两种，一种是基于口令的身份验证，一种是基于密钥的身份认证。当你直接使用SSH工具连接服务器时，SSH首先会在<code>~/.ssh/</code>路径下按照默认的密钥（私钥）文件名（如id_rsa）查找对应的密钥并尝试使用本地的私钥去配对服务器端的公钥，如果配对成功则不需要进行密码验证（这也是通常实现SSH免密登录的原理），如果配对失败则尝试使用密码验证的方式进行身份验证。但是如果ssh配置文件中关闭了密码验证，那么当密钥无法配对成功时，就会报如题的错误。同样的如果在没有开启密码验证的时候，尝试使用<code>ssh-copy-id</code>，也会报此错误。</p>
<a id="more"></a>

<hr>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><h3 id="1）使用对应私钥连接服务端"><a href="#1）使用对应私钥连接服务端" class="headerlink" title="1）使用对应私钥连接服务端"></a>1）使用对应私钥连接服务端</h3><p><strong>命令格式如下：</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh &lt;remote_username&gt;@&lt;remote_ip&gt; -i &lt;your_local_private_key&gt;</span><br></pre></td></tr></table></figure>

<p><strong>示例：</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh admin@114.67.40.28 -i /home/tom/id_rsa</span><br></pre></td></tr></table></figure>

<p><strong>注意：如果是在Windows平台下使用Git Bash，则默认密钥（私钥）一般存放在<code>C:\Users\&lt;your_account&gt;\.ssh</code>目录下。如果没有对应私钥是没有办法连接到远端的。</strong></p>
<h3 id="2）修改SSH配置文件"><a href="#2）修改SSH配置文件" class="headerlink" title="2）修改SSH配置文件"></a>2）修改SSH配置文件</h3><p><strong>修改/etc/ssh/sshd_config：</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo vim /etc/ssh/sshd_config</span><br></pre></td></tr></table></figure>

<p><strong>将PasswordAuthentication设置成为yes（一般在文件的末尾几行）：</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#	X11Forwarding no</span></span><br><span class="line"><span class="comment">#	AllowTcpForwarding no</span></span><br><span class="line"><span class="comment">#	PermitTTY no</span></span><br><span class="line"><span class="comment">#	ForceCommand cvs server</span></span><br><span class="line">UseDNS no</span><br><span class="line">AddressFamily inet</span><br><span class="line">PermitRootLogin yes</span><br><span class="line">SyslogFacility AUTHPRIV</span><br><span class="line"><span class="comment">#PasswordAuthentication no</span></span><br><span class="line">PasswordAuthentication yes</span><br></pre></td></tr></table></figure>

<p><strong>注意：不是修改<code>/etc/ssh/ssh_config</code>文件</strong></p>
<h3 id="3）使用密码登录"><a href="#3）使用密码登录" class="headerlink" title="3）使用密码登录"></a>3）使用密码登录</h3><p><strong>修改配置之后就可以正常使用密码的方式进行身份验证登录服务器了</strong></p>
<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Shell</tag>
        <tag>ssh</tag>
      </tags>
  </entry>
  <entry>
    <title>Shell之使用ssh工具在远端执行多条命令</title>
    <url>/2020/03/15/Shell%E4%B9%8B%E4%BD%BF%E7%94%A8ssh%E5%B7%A5%E5%85%B7%E5%9C%A8%E8%BF%9C%E7%AB%AF%E6%89%A7%E8%A1%8C%E5%A4%9A%E6%9D%A1%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<hr>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul>
<li><p><strong>①多行命令使用双引号或者首尾定界符来囊括，当使用双引号囊括时，若其中同时也使用了双引号就需要使用转义字符对远程执行命令中的引号进行转义。所以如果远程命令本身包含双引号，建议还是使用首尾定界符，首尾定界符是一个自定义的字符串，可以自定义内容，并且命令结尾处的首尾定界符需要放置于当前行行首</strong></p>
</li>
<li><p><strong>②如果远程执行命令中的美元符号<code>$</code>读取的是远程终端的变量，而非当前终端的变量就需要增加转义字符来将此美元符号转义，即定义为读取远程终端中设置的变量</strong></p>
</li>
</ul>
<a id="more"></a>

<hr>
<h2 id="脚本示例"><a href="#脚本示例" class="headerlink" title="脚本示例"></a>脚本示例</h2><h3 id="示例1：使用双引号"><a href="#示例1：使用双引号" class="headerlink" title="示例1：使用双引号"></a>示例1：使用双引号</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment"># 此脚本用于测试ssh工具远程执行多条命令方式</span></span><br><span class="line"><span class="comment"># 使用的前提是已经设置密钥对并以默认私钥文件名存放在默认路径~/.ssh/下(即ssh免密登录)</span></span><br><span class="line"><span class="comment"># 或者使用-i参数手动指定私钥位置</span></span><br><span class="line"></span><br><span class="line">host=192.168.126.101</span><br><span class="line"></span><br><span class="line">ssh tomandersen@<span class="variable">$host</span> <span class="string">"</span></span><br><span class="line"><span class="string">source /etc/profile</span></span><br><span class="line"><span class="string">echo \$JAVA_HOME</span></span><br><span class="line"><span class="string">echo 'this is a test!'</span></span><br><span class="line"><span class="string">"</span></span><br></pre></td></tr></table></figure>

<p><strong>注意：每行结尾不加分号<code>;</code>，若远程执行的多行命令中存在使用<code>双引号&quot;&quot;</code>或者<code>美元符号$</code>的情况，需要使用<code>转义字符\</code>进行转义，前者是因为双引号在本示例中是作为首位定界符只能出现在命令头尾，后者是因为不给美元符号附上转义字符时是默认读取本地变量。同理也可以使用单引号</strong></p>
<hr>
<h3 id="示例2：使用首尾定界符"><a href="#示例2：使用首尾定界符" class="headerlink" title="示例2：使用首尾定界符"></a>示例2：使用首尾定界符</h3><p><strong>首尾定界符是用于圈定命令的字符串，字符串内容可以自己定义，如常用的EOF，甚至命名为TEST都行。配合使用输入从定向&lt;&lt;，能够将其圈定的内容作为命令参数输入</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment"># 此脚本用于测试ssh工具远程执行多条命令方式</span></span><br><span class="line"><span class="comment"># 使用的前提是已经设置密钥对并以默认私钥文件名存放在默认路径~/.ssh/下(即ssh免密登录)</span></span><br><span class="line"><span class="comment"># 或者使用-i参数手动指定私钥位置</span></span><br><span class="line"></span><br><span class="line">host=192.168.126.101</span><br><span class="line"></span><br><span class="line">ssh tomandersen@<span class="variable">$host</span> &lt;&lt; EOF</span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br><span class="line"><span class="built_in">echo</span> \<span class="variable">$JAVA_HOME</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">'this is a test!'</span></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<p><strong>注意：首尾定界符的结尾符号需要写在行首，否则会报错</strong></p>
<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Shell</tag>
        <tag>ssh</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka之入门级安装部署</title>
    <url>/2020/03/15/Kafka%E4%B9%8B%E5%85%A5%E9%97%A8%E7%BA%A7%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<hr>
<h2 id="1-下载Kafka安装包"><a href="#1-下载Kafka安装包" class="headerlink" title="1. 下载Kafka安装包"></a>1. 下载Kafka安装包</h2><p><a href="http://archive.apache.org/dist/" target="_blank" rel="noopener">Apache所有项目历史版本镜像地址</a></p>
<p><a href="http://archive.apache.org/dist/kafka/" target="_blank" rel="noopener">Apache Kafka下载地址</a></p>
<a id="more"></a>

<hr>
<h2 id="2-解压到指定路径下"><a href="#2-解压到指定路径下" class="headerlink" title="2. 解压到指定路径下"></a>2. 解压到指定路径下</h2><p><strong>本次安装的Kafka为2.1.1版本，注意：压缩包前面的版本号为Scala版本号，而后面的才是Kafka版本号</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar -xzvf kafka_2.11-2.1.1.tgz -C /opt/module/</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="3-配置环境变量"><a href="#3-配置环境变量" class="headerlink" title="3. 配置环境变量"></a>3. 配置环境变量</h2><p><strong>通过修改<code>/etc/profile</code>文件设置KAFKA_HOME环境变量，并将bin目录增加到PATH环境变量中</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo vim /etc/profile</span><br></pre></td></tr></table></figure>

<p><strong>添加以下内容</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Set $KAFKA_HOME</span></span><br><span class="line"><span class="built_in">export</span> KAFKA_HOME=/opt/module/kafka_2.11-2.1.1</span><br><span class="line"><span class="comment"># Add $KAFKA_HOME/bin to PATH</span></span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$KAFKA_HOME</span>/bin</span><br></pre></td></tr></table></figure>

<p><strong>重新加载profile文件，验证环境变量</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/profile</span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$KAFKA_HOME</span></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="4-修改配置文件config-server-properties"><a href="#4-修改配置文件config-server-properties" class="headerlink" title="4. 修改配置文件config/server.properties"></a>4. 修改配置文件<code>config/server.properties</code></h2><h3 id="1）设置Broker-ID"><a href="#1）设置Broker-ID" class="headerlink" title="1）设置Broker ID"></a>1）设置Broker ID</h3><p><strong>Kafka集群中每个主机即Broker的ID必须不同</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># The id of the broker. This must be set to a unique integer for each broker.</span></span><br><span class="line">broker.id=0</span><br></pre></td></tr></table></figure>



<h3 id="2）设置Kafka日志以及数据存放路径"><a href="#2）设置Kafka日志以及数据存放路径" class="headerlink" title="2）设置Kafka日志以及数据存放路径"></a>2）设置Kafka日志以及数据存放路径</h3><p><strong>此路径设置的日志和数据的共同存储路径，即各种Partition的数据也存储在此路径下</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">log.dirs=/opt/module/kafka_2.11-2.1.1/logs</span><br></pre></td></tr></table></figure>



<h3 id="3）开启管理工具的删除Topic功能"><a href="#3）开启管理工具的删除Topic功能" class="headerlink" title="3）开启管理工具的删除Topic功能"></a>3）开启管理工具的删除Topic功能</h3><p><strong>如果不开启开启此功能，那么使用命令行删除Topic时只会是标记删除，实际上并未删除，当再次创建同名Topic时就会报错无法创建</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">delete.topic.enable=<span class="literal">true</span></span><br></pre></td></tr></table></figure>



<h3 id="4）配置Zookeeper集群"><a href="#4）配置Zookeeper集群" class="headerlink" title="4）配置Zookeeper集群"></a>4）配置Zookeeper集群</h3><p><strong>Zookeeper服务器集群的客户端端口要与Zookeeper中配置的端口一致</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">zookeeper.connect=zkServer1:2181,zkServer2.2181,zkServer3.2181</span><br></pre></td></tr></table></figure>



<h3 id="5）具体配置"><a href="#5）具体配置" class="headerlink" title="5）具体配置"></a>5）具体配置</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#broker的全局唯一编号，不能重复</span></span><br><span class="line">broker.id=1</span><br><span class="line"><span class="comment">#删除topic功能使能</span></span><br><span class="line">delete.topic.enable=<span class="literal">true</span></span><br><span class="line"><span class="comment">#处理网络请求的线程数量</span></span><br><span class="line">num.network.threads=3</span><br><span class="line"><span class="comment">#用来处理磁盘IO的现成数量</span></span><br><span class="line">num.io.threads=8</span><br><span class="line"><span class="comment">#发送套接字的缓冲区大小</span></span><br><span class="line">socket.send.buffer.bytes=102400</span><br><span class="line"><span class="comment">#接收套接字的缓冲区大小</span></span><br><span class="line">socket.receive.buffer.bytes=102400</span><br><span class="line"><span class="comment">#请求套接字的缓冲区大小</span></span><br><span class="line">socket.request.max.bytes=104857600</span><br><span class="line"><span class="comment">#kafka运行日志存放的路径	</span></span><br><span class="line">log.dirs=/opt/module/kafka_2.11-2.1.1/logs</span><br><span class="line"><span class="comment">#topic在当前broker上的分区个数</span></span><br><span class="line">num.partitions=1</span><br><span class="line"><span class="comment">#用来恢复和清理data下数据的线程数量</span></span><br><span class="line">num.recovery.threads.per.data.dir=1</span><br><span class="line"><span class="comment">#segment文件保留的最长时间，超时将被删除</span></span><br><span class="line">log.retention.hours=168</span><br><span class="line"><span class="comment">#log segment滚动条件:文件大小</span></span><br><span class="line">log.segment.bytes=1073741824</span><br><span class="line"><span class="comment">#周期性检查log segment文件是否可以删除</span></span><br><span class="line">log.retention.check.interval.ms=300000</span><br><span class="line"><span class="comment">#配置连接Zookeeper集群地址</span></span><br><span class="line">zookeeper.connect=hadoop102:2181,hadoop103:2181,hadoop104:2181</span><br><span class="line"><span class="comment">#zookeeper连接认定超时时间</span></span><br><span class="line">zookeeper.connection.timeout.ms=6000</span><br><span class="line"><span class="comment">#设置rebalance的延迟时间</span></span><br><span class="line">group.initial.rebalance.delay.ms=0</span><br></pre></td></tr></table></figure>



<h3 id="6）测试"><a href="#6）测试" class="headerlink" title="6）测试"></a>6）测试</h3><p><strong>启动当前主机Kafka</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/kafka-server-start.sh config/server.properties</span><br></pre></td></tr></table></figure>

<p><strong>关闭当前主机Kafka</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/kafka-server-stop.sh</span><br></pre></td></tr></table></figure>



<h3 id="7）将Kafka分发给其他主机并修改对应的Broker-ID即可"><a href="#7）将Kafka分发给其他主机并修改对应的Broker-ID即可" class="headerlink" title="7）将Kafka分发给其他主机并修改对应的Broker ID即可"></a>7）将Kafka分发给其他主机并修改对应的Broker ID即可</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">scp -r /opt/module/kafka_2.11-2.1.1 hadoop102:/opt/module/</span><br><span class="line">scp -r /opt/module/kafka_2.11-2.1.1 hadoop103:/opt/module/</span><br></pre></td></tr></table></figure>



<h3 id="8）编写脚本控制Kafka集群"><a href="#8）编写脚本控制Kafka集群" class="headerlink" title="8）编写脚本控制Kafka集群"></a>8）编写脚本控制Kafka集群</h3><p><strong>编写Kafka群起脚本，通过脚本控制Kafka集群。此处提供脚本模板，仅供参考，具体使用之前记得更改Kafka安装路径，以及实现主机间ssh免密登录或者更改ssh命令指定每次连接时使用的私钥</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 此脚本用于启动/停止Kafka集群</span></span><br><span class="line"><span class="comment"># 输入的参数只能为start/stop</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 判断参数个数</span></span><br><span class="line"><span class="keyword">if</span> ((<span class="variable">$#</span> &gt; 1)); <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"Wrongs parameters!"</span></span><br><span class="line">    <span class="built_in">exit</span> 1</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取当前时间(相对时间)</span></span><br><span class="line">start_time=$(date +%s)</span><br><span class="line"><span class="comment"># 获取操作方式</span></span><br><span class="line">operation=<span class="variable">$1</span></span><br><span class="line"><span class="comment"># 设置Kafka集群</span></span><br><span class="line">cluster=<span class="variable">$&#123;KAFKA_CLUSTER:-"hadoop101 hadoop102 hadoop103"&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对kafka集群进行对应的操作</span></span><br><span class="line"><span class="keyword">case</span> <span class="string">"<span class="variable">$operation</span>"</span> <span class="keyword">in</span></span><br><span class="line">start)</span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"----------Starting kafka cluster----------"</span></span><br><span class="line">    <span class="keyword">for</span> host <span class="keyword">in</span> <span class="variable">$cluster</span>; <span class="keyword">do</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"----------Starting kafka in [<span class="variable">$host</span>]----------"</span></span><br><span class="line">        ssh <span class="variable">$host</span> <span class="string">"source /etc/profile;</span></span><br><span class="line"><span class="string">        KAFKA_HOME=\$&#123;KAFKA_HOME:-'/opt/module/kafka_2.11-2.1.1'&#125;;</span></span><br><span class="line"><span class="string">        cd \$KAFKA_HOME;</span></span><br><span class="line"><span class="string">        nohup ./bin/kafka-server-start.sh config/server.properties &gt; /dev/null 2&gt;&amp;1 &amp;</span></span><br><span class="line"><span class="string">        "</span></span><br><span class="line">    <span class="keyword">done</span></span><br><span class="line">    ;;</span><br><span class="line">stop)</span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"----------Stopping kafka cluster----------"</span></span><br><span class="line">    <span class="keyword">for</span> host <span class="keyword">in</span> <span class="variable">$cluster</span>; <span class="keyword">do</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"----------Starting kafka in [<span class="variable">$host</span>]----------"</span></span><br><span class="line">        ssh <span class="variable">$host</span> <span class="string">"source /etc/profile;</span></span><br><span class="line"><span class="string">        KAFKA_HOME=\$&#123;KAFKA_HOME:-'/opt/module/kafka_2.11-2.1.1'&#125;;</span></span><br><span class="line"><span class="string">        cd \$KAFKA_HOME;</span></span><br><span class="line"><span class="string">        ./bin/kafka-server-stop.sh</span></span><br><span class="line"><span class="string">        "</span></span><br><span class="line">    <span class="keyword">done</span></span><br><span class="line">    ;;</span><br><span class="line">*)</span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"Worong Parameter!"</span></span><br><span class="line">    <span class="built_in">exit</span> 1</span><br><span class="line">    ;;</span><br><span class="line"><span class="keyword">esac</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取结束时间</span></span><br><span class="line">end_time=$(date +%s)</span><br><span class="line">execution_time=$((<span class="variable">$&#123;end_time&#125;</span> - <span class="variable">$&#123;start_time&#125;</span>))</span><br><span class="line"><span class="built_in">echo</span> -e <span class="string">"\n----------<span class="variable">$operation</span> kafka in [<span class="variable">$cluster</span>] takes <span class="variable">$&#123;execution_time&#125;</span> seconds----------\n"</span></span><br></pre></td></tr></table></figure>



<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Kafka</tag>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka之命令行的基本操作</title>
    <url>/2020/03/15/Kafka%E4%B9%8B%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul>
<li>kafka版本：2.11-2.1.1</li>
</ul>
<a id="more"></a>

<hr>
<h2 id="1-启动-关闭Kafka"><a href="#1-启动-关闭Kafka" class="headerlink" title="1. 启动/关闭Kafka"></a>1. 启动/关闭Kafka</h2><p><strong>注意：在启动Kafka进程之前记得先启动Zookeeper集群</strong></p>
<p><strong>启动当前主机Kafka进程</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/kafka-server-start.sh config/server.properties</span><br></pre></td></tr></table></figure>

<p><strong>可以使用<code>-daemon</code>参数，表示以守护线程的方式启动，日志不打印到控制台</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/kafka-server-start.sh -daemon config/server.properties</span><br></pre></td></tr></table></figure>

<p><strong>关闭当前主机Kafka进程</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/kafka-server-stop.sh</span><br></pre></td></tr></table></figure>



<h2 id="2-查看所有Topic信息"><a href="#2-查看所有Topic信息" class="headerlink" title="2. 查看所有Topic信息"></a>2. 查看所有Topic信息</h2><p><strong>需要指定Zookeeper集群和其提供的客户端端口号。如果想尝试使用多个Zookeeper服务器，各个服务器之间使用逗号隔开</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper zkServer1:2181,zkServer2:2181 --list</span><br></pre></td></tr></table></figure>



<h2 id="3-创建Topic"><a href="#3-创建Topic" class="headerlink" title="3. 创建Topic"></a>3. 创建Topic</h2><p><strong>向Zookeeper服务器集群注册Topic</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper zkServer1:2181 --create --replication-factor 3 --partitions 1 --topic first</span><br></pre></td></tr></table></figure>

<p><strong>注意：自Kafka 2.2.x及之后都是向Kafka集群注册Topic，而不是直接向Zookeeper注册Tpoic，所以命令行有所不同，将Zookeeper集群改成指定Kafka集群及其客户端端口</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 3 --partitions 1 --topic first</span><br></pre></td></tr></table></figure>



<h2 id="4-删除Topic"><a href="#4-删除Topic" class="headerlink" title="4. 删除Topic"></a>4. 删除Topic</h2><p><strong>需要server.properties中设置<code>delete.topic.enable=true</code>，否则只是标记删除而实际未删除，会导致再次创建同名Topic时失败</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper zkServer1:2181 --delete --topic first</span><br></pre></td></tr></table></figure>



<h2 id="5-生产消息"><a href="#5-生产消息" class="headerlink" title="5. 生产消息"></a>5. 生产消息</h2><p><strong>启动生产者进程</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/kafka-console-producer.sh --broker-list localhost:9092 --topic first</span><br></pre></td></tr></table></figure>



<h2 id="6-消费消息"><a href="#6-消费消息" class="headerlink" title="6. 消费消息"></a>6. 消费消息</h2><p><strong>启动消费者进程，实时消费Topic中的最新消息</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic first</span><br></pre></td></tr></table></figure>

<p><strong>加上<code>from-beginning</code>则是将Topic中已保存的历史消息全部取出</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic first</span><br></pre></td></tr></table></figure>

<p><strong>注意1：如果使用了多个Partition，取出的消息只能保证单个Partition中的消息有序，多个Partition之间消息是不保证有序的</strong></p>
<p><strong>注意2：在Kafka 0.8.1及之前，Consumer消费分区消息的offset偏移量也通过Zookeeper来保存，在此版本之后就不建议或者禁止使用zk来保存offset。在Kafka 0.8.2及之后，Kafka通过建立一个Internal Topic即内部Topic来保存Consumer Offset信息（topic名为_consumer_offset），同样存储在log.dirs路径下。因此Kafka 0.8.2及之后是通过指定Kafka集群来消费消息，而Kafka 0.8.1及之前是通过指定Zookeeper集群来消费消息</strong></p>
<h2 id="7-查看指定Topic具体信息"><a href="#7-查看指定Topic具体信息" class="headerlink" title="7. 查看指定Topic具体信息"></a>7. 查看指定Topic具体信息</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper zkServer1:2181 --describe --topic first</span><br></pre></td></tr></table></figure>

<p><strong>其中<code>ISR</code>表示已经和Partition Leader实现同步的Partition副本（in-sync replication）</strong></p>
<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
        <tag>Shell</tag>
        <tag>Practice</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo之NexT主题设置背景图片</title>
    <url>/2020/03/15/Hexo%E4%B9%8BNexT%E4%B8%BB%E9%A2%98%E8%AE%BE%E7%BD%AE%E8%83%8C%E6%99%AF%E5%9B%BE%E7%89%87/</url>
    <content><![CDATA[<hr>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul>
<li><strong>Hexo版本</strong>：4.2.0</li>
<li><strong>NexT版本</strong>：7.7.1</li>
<li><strong>GitHub</strong>：<a href="https://github.com/theme-next/theme-next-pace" target="_blank" rel="noopener">theme-next-pace</a></li>
</ul>
<a id="more"></a>

<hr>
<h2 id="配置步骤"><a href="#配置步骤" class="headerlink" title="配置步骤"></a>配置步骤</h2><h3 id="1）修改NexT配置文件"><a href="#1）修改NexT配置文件" class="headerlink" title="1）修改NexT配置文件"></a>1）修改NexT配置文件</h3><p>在NexT的配置文件中的<code>custom_file_path</code>参数下设置CSS样式文件<code>style.styl</code>的存储路径，我们可以在此文件中自定义使用自己的各种样式，此路径必须在Hexo根目录source文件夹下，建议设置成默认路径<code>source/_data/style.styl</code>，然后在对应路径下创建对应的<code>style.styl</code>文件。</p>
<h3 id="2）配置style-styl文件"><a href="#2）配置style-styl文件" class="headerlink" title="2）配置style.styl文件"></a>2）配置style.styl文件</h3><p><strong>此脚本内容遵循CSS语法，以下是配置示例：</strong></p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line">// Custom styles.</span><br><span class="line">// 整体背景设置</span><br><span class="line"><span class="selector-tag">body</span> &#123;</span><br><span class="line"> 	background:url(/images/adrian-Xft-JdC-Jbc-unsplash.jpg);// 设定背景图片,images同处于source文件夹下</span><br><span class="line"> 	background-repeat: no-repeat;// 设定背景图片非重复填充</span><br><span class="line">    background-attachment:fixed;// 设置背景图片不随页面滚动</span><br><span class="line">    background-position:50% 50%;// 设置背景图片位置</span><br><span class="line">	background-size: cover// 设置保持图像的纵横比并将图像缩放成将完全覆盖背景定位区域的最小大小</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 页面头样式属性</span><br><span class="line"><span class="selector-class">.header-inner</span> &#123;</span><br><span class="line">  // 也可以同时定义背景色</span><br><span class="line">  // background: #ddd </span><br><span class="line">  // 透明度</span><br><span class="line">  // opacity: 0.8;</span><br><span class="line">&#125;</span><br><span class="line">// sidebar侧边工具栏样式属性</span><br><span class="line"><span class="selector-class">.sidebar</span>&#123;</span><br><span class="line">    // 动画过渡时间</span><br><span class="line">	<span class="selector-tag">transition-duration</span>: 0<span class="selector-class">.4s</span>;</span><br><span class="line">    // 透明度</span><br><span class="line">	<span class="selector-tag">opacity</span>: 0<span class="selector-class">.8</span></span><br><span class="line">&#125;</span><br><span class="line">// 标题样式</span><br><span class="line"><span class="selector-class">.posts-expand</span> <span class="selector-class">.post-title-link</span> &#123;</span><br><span class="line">	// 设置字体颜色</span><br><span class="line">	<span class="selector-tag">color</span>: <span class="selector-id">#222</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 文章版块样式</span><br><span class="line"><span class="selector-class">.post-block</span> &#123;</span><br><span class="line">    //background: var(--content-bg-color);</span><br><span class="line">	<span class="selector-tag">background</span>: <span class="selector-id">#fff</span></span><br><span class="line">    <span class="selector-tag">border-radius</span>: <span class="selector-tag">initial</span>;</span><br><span class="line">    <span class="selector-tag">box-shadow</span>: 0 2<span class="selector-tag">px</span> 2<span class="selector-tag">px</span> 0 <span class="selector-tag">rgba</span>(0,0,0,0<span class="selector-class">.12</span>), 0 3<span class="selector-tag">px</span> 1<span class="selector-tag">px</span> <span class="selector-tag">-2px</span> <span class="selector-tag">rgba</span>(0,0,0,0<span class="selector-class">.06</span>), 0 1<span class="selector-tag">px</span> 5<span class="selector-tag">px</span> 0 <span class="selector-tag">rgba</span>(0,0,0,0<span class="selector-class">.12</span>);</span><br><span class="line">    <span class="selector-tag">padding</span>: 40<span class="selector-tag">px</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="3）重新生成Hexo博客"><a href="#3）重新生成Hexo博客" class="headerlink" title="3）重新生成Hexo博客"></a>3）重新生成Hexo博客</h3><p><strong>重新生成Hexo博客，并验证配置结果</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo clean &amp;&amp; hexo g &amp;&amp; hexo s</span><br></pre></td></tr></table></figure>



<h3 id="4）More"><a href="#4）More" class="headerlink" title="4）More"></a>4）More</h3><p><strong>类似的，我们也可以通过浏览器的F12进入开发者模式并使用鼠标点击来获取CSS样式名，并在此文件中设置对应的样式属性来实现各种自定义修改的CSS样式</strong></p>
<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>个人博客搭建</category>
      </categories>
      <tags>
        <tag>Blog</tag>
        <tag>Hexo</tag>
        <tag>NexT</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo之NexT主题中设置加载进度条</title>
    <url>/2020/03/06/Hexo%E4%B9%8BNexT%E4%B8%BB%E9%A2%98%E4%B8%AD%E8%AE%BE%E7%BD%AE%E5%8A%A0%E8%BD%BD%E8%BF%9B%E5%BA%A6%E6%9D%A1/</url>
    <content><![CDATA[<hr>
<hr>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul>
<li><strong>Hexo版本</strong>：4.2.0</li>
<li><strong>NexT版本</strong>：7.7.1</li>
<li><strong>GitHub</strong>：<a href="https://github.com/theme-next/theme-next-pace" target="_blank" rel="noopener">theme-next-pace</a></li>
<li><strong>各种进度条样式参考</strong>：<a href="https://blog.pangao.vip/Hexo博客NexT主题美化之顶部加载进度条/" target="_blank" rel="noopener">Hexo博客NexT主题美化之顶部加载进度条</a></li>
</ul>
<hr>
<a id="more"></a>

<h2 id="安装部署"><a href="#安装部署" class="headerlink" title="安装部署"></a>安装部署</h2><h3 id="1）进入NexT主题文件夹"><a href="#1）进入NexT主题文件夹" class="headerlink" title="1）进入NexT主题文件夹"></a>1）进入NexT主题文件夹</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> themes/next/</span><br><span class="line">$ ls</span><br><span class="line">_config.yml  docs/        languages/  LICENSE.md    README.md  <span class="built_in">source</span>/</span><br><span class="line">crowdin.yml  gulpfile.js  layout/     package.json  scripts/</span><br></pre></td></tr></table></figure>



<h3 id="2）克隆Github仓库（如果使用CDN可跳过此步骤）"><a href="#2）克隆Github仓库（如果使用CDN可跳过此步骤）" class="headerlink" title="2）克隆Github仓库（如果使用CDN可跳过此步骤）"></a>2）克隆Github仓库（如果使用CDN可跳过此步骤）</h3><p><strong>将仓库克隆至<code>themes/next/source/lib</code>路径下</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/theme-next/theme-next-pace <span class="built_in">source</span>/lib/pace</span><br></pre></td></tr></table></figure>



<h3 id="3）配置NexT中的-config-xml"><a href="#3）配置NexT中的-config-xml" class="headerlink" title="3）配置NexT中的_config.xml"></a>3）配置NexT中的<code>_config.xml</code></h3><p><strong>开启pace选项</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Progress bar in the top during page loading.</span></span><br><span class="line"><span class="comment"># 设置页面加载时顶部进度条</span></span><br><span class="line"><span class="comment"># Dependencies: https://github.com/theme-next/theme-next-pace</span></span><br><span class="line"><span class="comment"># For more information: https://github.com/HubSpot/pace</span></span><br><span class="line">pace:</span><br><span class="line">  <span class="comment"># enable: false</span></span><br><span class="line">  <span class="built_in">enable</span>: <span class="literal">true</span></span><br><span class="line">  <span class="comment"># Themes list:</span></span><br><span class="line">  <span class="comment"># big-counter | bounce | barber-shop | center-atom | center-circle | center-radar | center-simple</span></span><br><span class="line">  <span class="comment"># corner-indicator | fill-left | flat-top | flash | loading-bar | mac-osx | material | minimal</span></span><br><span class="line">  theme: minimal</span><br></pre></td></tr></table></figure>



<h3 id="4）配置进度条CDN地址"><a href="#4）配置进度条CDN地址" class="headerlink" title="4）配置进度条CDN地址"></a>4）配置进度条CDN地址</h3><p><strong>在NexT主题的<code>_config.xml</code>文件中找到<code>vendors</code>选项，设置pace的cdn地址（本人设置的进度条为黑色主题，可以在<a href="https://www.jsdelivr.com/package/npm/pace-js?path=themes" target="_blank" rel="noopener">jsdelivr</a>中找到对应的样式最新版cdn地址）</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vendors:</span><br><span class="line">  ...</span><br><span class="line">  pace: https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js</span><br><span class="line">  pace_css: https://cdn.jsdelivr.net/npm/pace-js@1.0.2/themes/black/pace-theme-loading-bar.css</span><br></pre></td></tr></table></figure>









<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>个人博客搭建</category>
      </categories>
      <tags>
        <tag>Blog</tag>
        <tag>Hexo</tag>
        <tag>NexT</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo之NexT主题中设置canvas-nest特效</title>
    <url>/2020/03/06/Hexo%E4%B9%8BNexT%E4%B8%BB%E9%A2%98%E4%B8%AD%E8%AE%BE%E7%BD%AEcanvas-nest%E7%89%B9%E6%95%88/</url>
    <content><![CDATA[<hr>
<hr>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul>
<li><strong>Hexo版本</strong>：4.2.0</li>
<li><strong>NexT版本</strong>：7.7.1</li>
<li><strong>NexT中集成有canvas_nest插件</strong></li>
<li><strong>GitHub</strong>：<a href="https://github.com/theme-next/theme-next-canvas-nest" target="_blank" rel="noopener">theme-next-canvas-nest</a></li>
<li><a href="https://git.hust.cc/canvas-nest.js" target="_blank" rel="noopener">canvas-nest特效展示</a></li>
</ul>
<a id="more"></a>

<hr>
<h2 id="安装部署"><a href="#安装部署" class="headerlink" title="安装部署"></a>安装部署</h2><h3 id="1）配置NexT主题配置文件themes-next-config-yml"><a href="#1）配置NexT主题配置文件themes-next-config-yml" class="headerlink" title="1）配置NexT主题配置文件themes/next/_config.yml"></a>1）配置NexT主题配置文件<code>themes/next/_config.yml</code></h3><p><strong>a）开启canvas_nest</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Canvas-nest</span></span><br><span class="line"><span class="comment"># Dependencies: https://github.com/theme-next/theme-next-canvas-nest</span></span><br><span class="line"><span class="comment"># For more information: https://github.com/hustcc/canvas-nest.js</span></span><br><span class="line"><span class="comment"># 若要开启canvas_nest,除了此处设置成true,还需要设置canvas_nest的vendors提供商</span></span><br><span class="line">canvas_nest:</span><br><span class="line">  <span class="comment">#enable: false</span></span><br><span class="line">  <span class="built_in">enable</span>: <span class="literal">true</span></span><br><span class="line">  onmobile: <span class="literal">true</span> <span class="comment"># Display on mobile or not</span></span><br><span class="line">  color: <span class="string">"0,0,255"</span> <span class="comment"># RGB values, use `,` to separate</span></span><br><span class="line">  opacity: 0.5 <span class="comment"># The opacity of line: 0~1</span></span><br><span class="line">  zIndex: -1 <span class="comment"># z-index property of the background</span></span><br><span class="line">  count: 99 <span class="comment"># The number of lines</span></span><br></pre></td></tr></table></figure>

<p><strong>b）设置canvas_nest脚本来源：在配置文件vendors选项下取消canvas_nest cdn注释</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Internal version: 1.0.0</span></span><br><span class="line"><span class="comment"># 设置canvas_nest的来源</span></span><br><span class="line">canvas_nest: //cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-nest@1/canvas-nest.min.js</span><br><span class="line">canvas_nest_nomobile: //cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-nest@1/canvas-nest-nomobile.min.js</span><br><span class="line"><span class="comment"># canvas_nest:</span></span><br><span class="line"><span class="comment"># canvas_nest_nomobile:</span></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>个人博客搭建</category>
      </categories>
      <tags>
        <tag>Blog</tag>
        <tag>Hexo</tag>
        <tag>NexT</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo之NexT主题中设置symbols_count_time统计单词</title>
    <url>/2020/03/06/Hexo%E4%B9%8BNexT%E4%B8%BB%E9%A2%98%E4%B8%AD%E8%AE%BE%E7%BD%AEsymbols-count-time%E7%BB%9F%E8%AE%A1%E5%8D%95%E8%AF%8D/</url>
    <content><![CDATA[<hr>
<hr>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul>
<li><strong>Hexo版本</strong>：4.2.0</li>
<li><strong>NexT版本</strong>：7.7.1</li>
<li><strong>symbols_count_time</strong>能够统计页面或者站点的单词以及阅读所需时间</li>
<li>自NexT 6.0发行版之后第三方插件<strong>hexo-wordcount</strong>就被<strong>symbols_count_time</strong>取缔了，相比之下<strong>symbols_count_time</strong>没有额外的依赖，性能更加强大</li>
<li><strong>GitHub</strong>：<a href="https://github.com/theme-next/hexo-symbols-count-time" target="_blank" rel="noopener">symbols_count_time</a></li>
</ul>
<a id="more"></a>

<hr>
<h2 id="安装部署"><a href="#安装部署" class="headerlink" title="安装部署"></a>安装部署</h2><h3 id="1）安装symbols-count-time插件"><a href="#1）安装symbols-count-time插件" class="headerlink" title="1）安装symbols_count_time插件"></a>1）安装symbols_count_time插件</h3><p><strong>在Hexo的根目录下安装symbols_count_time</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install hexo-symbols-count-time</span><br></pre></td></tr></table></figure>

<p><strong>若npm下载速度太慢可以使用淘宝npm镜像</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install -g cnpm --registry=https://registry.npm.taobao.org</span><br><span class="line">cnpm install hexo-symbols-count-time</span><br></pre></td></tr></table></figure>

<h3 id="2）配置Hexo站点配置文件-config-yml"><a href="#2）配置Hexo站点配置文件-config-yml" class="headerlink" title="2）配置Hexo站点配置文件_config.yml"></a>2）配置Hexo站点配置文件<code>_config.yml</code></h3><p><strong>在合适位置添加以下配置信息</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 设置博客单词统计</span></span><br><span class="line">symbols_count_time:</span><br><span class="line">  <span class="comment"># 文章字数统计</span></span><br><span class="line">  symbols: <span class="literal">true</span></span><br><span class="line">  <span class="comment"># 文章阅读时间统计</span></span><br><span class="line">  time: <span class="literal">true</span></span><br><span class="line">  <span class="comment"># 站点总字数统计</span></span><br><span class="line">  total_symbols: <span class="literal">false</span></span><br><span class="line">  <span class="comment"># 站点总阅读时间统计</span></span><br><span class="line">  total_time: <span class="literal">false</span></span><br><span class="line">  exclude_codeblock: <span class="literal">false</span></span><br></pre></td></tr></table></figure>

<h3 id="3）配置NexT主题配置文件themes-next-config-yml"><a href="#3）配置NexT主题配置文件themes-next-config-yml" class="headerlink" title="3）配置NexT主题配置文件themes/next/_config.yml"></a>3）配置NexT主题配置文件<code>themes/next/_config.yml</code></h3><p><strong>在symbols_count_time选下开启单词统计</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Post wordcount display settings</span></span><br><span class="line"><span class="comment"># Dependencies: https://github.com/theme-next/hexo-symbols-count-time</span></span><br><span class="line"><span class="comment"># 设置博客单词统计</span></span><br><span class="line">symbols_count_time:</span><br><span class="line">  <span class="comment"># 是否另起一行（true的话不和发表时间等同一行）</span></span><br><span class="line">  separated_meta: <span class="literal">true</span></span><br><span class="line">  <span class="comment"># 首页文章统计数量前是否显示文字描述（本文字数、阅读时长）</span></span><br><span class="line">  item_text_post: <span class="literal">true</span></span><br><span class="line">  <span class="comment"># 页面底部统计数量前是否显示文字描述（站点总字数、站点阅读时长）</span></span><br><span class="line">  item_text_total: <span class="literal">false</span></span><br><span class="line">  <span class="comment"># 平均字长</span></span><br><span class="line">  awl: 4</span><br><span class="line">  <span class="comment"># 每分钟阅读字数</span></span><br><span class="line">  wpm: 275</span><br></pre></td></tr></table></figure>

<h3 id="4）重新生成Hexo即可"><a href="#4）重新生成Hexo即可" class="headerlink" title="4）重新生成Hexo即可"></a>4）重新生成Hexo即可</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g</span><br><span class="line">hexo s</span><br></pre></td></tr></table></figure>



<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>个人博客搭建</category>
      </categories>
      <tags>
        <tag>Blog</tag>
        <tag>Hexo</tag>
        <tag>NexT</tag>
      </tags>
  </entry>
  <entry>
    <title>Flume之使用Loadbalancing Sink Processor实现sink负载均衡</title>
    <url>/2020/03/05/Flume%E4%B9%8B%E4%BD%BF%E7%94%A8Loadbalancing-Sink-Processor%E5%AE%9E%E7%8E%B0sink%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/</url>
    <content><![CDATA[<hr>
<a id="more"></a>

<hr>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul>
<li><strong>Load balancing Sink Processor</strong>，顾名思义，即能够对Sink组中的每个Sink实现负载均衡，默认采用的是轮询<strong>round_robin</strong>的方式，还可以使用随机方式<strong>random</strong>，或者用户自己实现AbstractSinkSelector抽象类定义自己的Sink Selector类，并提供FQCN（Full Qualified Class Name）全类名来进行配置，并且Load balancing Sink Processor还提供了指数退避backoff，即当某个Sink挂掉时，将会将其加入到黑名单，一定时间内不再访问此Sink，退避时间呈指数增长并默认最大值为30000ms，可以手动设置</li>
</ul>
<hr>
<h2 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a>使用示例</h2><h3 id="1）flume1-properties"><a href="#1）flume1-properties" class="headerlink" title="1）flume1.properties"></a>1）flume1.properties</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># flume1:此配置用于监控某个端口将其追加内容输出到flume2和flume3中</span></span><br><span class="line"><span class="comment"># 并将两个Sink组成一个sink group,并将Sink Processor设置成load_balance类型</span></span><br><span class="line"><span class="comment"># a1:Netcat Source-&gt; Memory Channel-&gt; Load balancing Sink Processor-&gt; Avro Sink</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sink groups</span></span><br><span class="line">a1.sinkgroups = g1</span><br><span class="line"><span class="comment"># 设置sink group中的sinks</span></span><br><span class="line">a1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line"><span class="comment"># 配置Load balancing Sink Processor(只有sink group才可以使用sink processor)</span></span><br><span class="line">a1.sinkgroups.g1.processor.type = load_balance</span><br><span class="line"><span class="comment"># 设置开启指数避让</span></span><br><span class="line">a1.sinkgroups.g1.processor.backoff = <span class="literal">true</span></span><br><span class="line"><span class="comment"># 设置Processor的selector为轮询round_robin</span></span><br><span class="line">a1.sinkgroups.g1.processor.selector = round_robin</span><br><span class="line"><span class="comment"># 设置最大避让时间(ms)</span></span><br><span class="line">a1.sinkgroups.g1.processor.maxTimeOut = 10000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Sources</span></span><br><span class="line"><span class="comment"># 配置a1.sources.r1的各项属性参数,类型/绑定主机ip/端口号</span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = hadoop101</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line"><span class="comment"># Channels</span></span><br><span class="line"><span class="comment"># 配置a1.channerls.c1的各项属性参数,缓存方式/最多缓存的Event个数/单次传输的Event个数</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sinks</span></span><br><span class="line"><span class="comment"># sinks.k1</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = hadoop102</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line"><span class="comment"># sinks.k2</span></span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = hadoop103</span><br><span class="line">a1.sinks.k2.port = 4141</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind</span></span><br><span class="line"><span class="comment"># 注意:source可以绑定多个channel,但是sink/sink group只能绑定单个channel</span></span><br><span class="line"><span class="comment"># r1-&gt;c1-&gt;g1</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c1</span><br></pre></td></tr></table></figure>



<h3 id="2）flume2-properties"><a href="#2）flume2-properties" class="headerlink" title="2）flume2.properties"></a>2）flume2.properties</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># flume2:此配置用于将来自指定Avro端口的数据输出到控制台</span></span><br><span class="line"><span class="comment"># a2:Avro Source-&gt;Memory Channel-&gt;Logger Sink</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent</span></span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.channels = c1</span><br><span class="line">a2.sinks = k1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sources</span></span><br><span class="line"><span class="comment"># a2.sources.r1</span></span><br><span class="line">a2.sources.r1.type = avro</span><br><span class="line"><span class="comment"># 设置监听本地IP</span></span><br><span class="line">a2.sources.r1.bind = 0.0.0.0</span><br><span class="line"><span class="comment"># 设置监听端口号</span></span><br><span class="line">a2.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line"><span class="comment"># Channels</span></span><br><span class="line"><span class="comment"># a2.channels.c1</span></span><br><span class="line"><span class="comment"># 使用内存作为缓存/最多缓存的Event个数/单次传输的Event个数</span></span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sinks</span></span><br><span class="line"><span class="comment"># 运行时设置参数 -Dflume.root.logger=INFO,console 即输出到控制台实时显示</span></span><br><span class="line">a2.sinks.k1.type = logger</span><br><span class="line"><span class="comment"># 设置Event的Body中写入log的最大字节数(默认值为16)</span></span><br><span class="line">a2.sinks.k1.maxBytesToLog = 256</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind</span></span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>



<h3 id="3）flume3-properties"><a href="#3）flume3-properties" class="headerlink" title="3）flume3.properties"></a>3）flume3.properties</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># flume3:此配置用于将来自指定Avro端口的数据输出到控制台</span></span><br><span class="line"><span class="comment"># a3:Avro Source-&gt;Memory Channel-&gt;Logger Sink</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent</span></span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.channels = c1</span><br><span class="line">a3.sinks = k1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sources</span></span><br><span class="line"><span class="comment"># a3.sources.r1</span></span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line"><span class="comment"># 设置监听本地IP</span></span><br><span class="line">a3.sources.r1.bind = 0.0.0.0</span><br><span class="line"><span class="comment"># 设置监听端口号</span></span><br><span class="line">a3.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line"><span class="comment"># Channels</span></span><br><span class="line"><span class="comment"># a3.channels.c1</span></span><br><span class="line"><span class="comment"># 使用内存作为缓存/最多缓存的Event个数/单次传输的Event个数</span></span><br><span class="line">a3.channels.c1.type = memory</span><br><span class="line">a3.channels.c1.capacity = 1000</span><br><span class="line">a3.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sinks</span></span><br><span class="line"><span class="comment"># 运行时设置参数 -Dflume.root.logger=INFO,console 即输出到控制台实时显示</span></span><br><span class="line">a3.sinks.k1.type = logger</span><br><span class="line"><span class="comment"># 设置Event的Body中写入log的最大字节数(默认值为16)</span></span><br><span class="line">a3.sinks.k1.maxBytesToLog = 256</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind</span></span><br><span class="line">a3.sources.r1.channels = c1</span><br><span class="line">a3.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<h3 id="4）对应功能"><a href="#4）对应功能" class="headerlink" title="4）对应功能"></a>4）对应功能</h3><p><strong>agent a1将指定端口的监听数据采用轮询的方式传输给a2和a3，并分别输出到各自的控制台</strong></p>
<h3 id="5）启动命令"><a href="#5）启动命令" class="headerlink" title="5）启动命令"></a>5）启动命令</h3><p><strong>Flume Agent a1至a3分别运行在主机hadoop101、hadoop102、hadoop103上</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./bin/flume-ng agent -n a1 -c conf -f flume1.properties</span><br><span class="line">./bin/flume-ng agent -n a2 -c conf -f flume2.properties -Dflume.root.logger=INFO,console</span><br><span class="line">./bin/flume-ng agent -n a3 -c conf -f flume3.properties -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>





<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Flume</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>Flume之使用Failover Sink Processor实现sink故障转移</title>
    <url>/2020/03/05/Flume%E4%B9%8B%E4%BD%BF%E7%94%A8Failover-Sink-Processor%E5%AE%9E%E7%8E%B0sink%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB/</url>
    <content><![CDATA[<hr>
<a id="more"></a>

<hr>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul>
<li><strong>Failover Sink Processor</strong> 维护着Sink组中Sinks的优先级表，根据优先级尝试将Event传输给不同的Sink直到Event成功发送。当优先级高的Sink不可用时，会将Event传输给下一优先级Sink，以此来确保每个Event都能被投递。当Sink不可用时，Failover Sink Processor和<strong>Load balancing Sink Processor</strong>一样，也会进行指数回退backoff，并可以设置最大回退时间（即在黑名单中的保存时间），在倒计时结束后会再次尝试访问之前挂掉的Sink</li>
</ul>
<hr>
<h2 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a>使用示例</h2><h3 id="1）flume1-properties"><a href="#1）flume1-properties" class="headerlink" title="1）flume1.properties"></a>1）flume1.properties</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># flume1:此配置用于监控某个窗口将其追加内容输出到flume2和flume3中</span></span><br><span class="line"><span class="comment"># 并将两个Sink组成一个sink group,并将Sink Processor设置成Failover类型</span></span><br><span class="line"><span class="comment"># a1:Netcat Source-&gt;Memory Channel-&gt;Avro Sink</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sink groups</span></span><br><span class="line">a1.sinkgroups = g1</span><br><span class="line"><span class="comment"># 设置sink group中的sinks</span></span><br><span class="line">a1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line"><span class="comment"># 设置Failover sink processor(只有sink group才可以使用sink processor)</span></span><br><span class="line">a1.sinkgroups.g1.processor.type = failover</span><br><span class="line"><span class="comment"># 设置Failover sink processor优先级表</span></span><br><span class="line">a1.sinkgroups.g1.processor.priority.k1 = 5</span><br><span class="line">a1.sinkgroups.g1.processor.priority.k2 = 10</span><br><span class="line"><span class="comment"># 设置最大避让时间(ms)</span></span><br><span class="line">a1.sinkgroups.g1.processor.maxpenalty = 10000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Sources</span></span><br><span class="line"><span class="comment"># 配置a1.sources.r1的各项属性参数,类型/绑定主机ip/端口号</span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = hadoop101</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line"><span class="comment"># Channels</span></span><br><span class="line"><span class="comment"># 配置a1.channerls.c1的各项属性参数,缓存方式/最多缓存的Event个数/单次传输的Event个数</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sinks</span></span><br><span class="line"><span class="comment"># sinks.k1</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = hadoop102</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line"><span class="comment"># sinks.k2</span></span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = hadoop103</span><br><span class="line">a1.sinks.k2.port = 4141</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind</span></span><br><span class="line"><span class="comment"># 注意:source可以绑定多个channel,但是sink/sink group只能绑定单个channel</span></span><br><span class="line"><span class="comment"># r1-&gt;c1-&gt;g1</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c1</span><br></pre></td></tr></table></figure>



<h3 id="2）flume2-properties"><a href="#2）flume2-properties" class="headerlink" title="2）flume2.properties"></a>2）flume2.properties</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># flume2:此配置用于将来自指定Avro端口的数据输出到控制台</span></span><br><span class="line"><span class="comment"># a2:Avro Source-&gt;Memory Channel-&gt;Logger Sink</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent</span></span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.channels = c1</span><br><span class="line">a2.sinks = k1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sources</span></span><br><span class="line"><span class="comment"># a2.sources.r1</span></span><br><span class="line">a2.sources.r1.type = avro</span><br><span class="line"><span class="comment"># 设置监听本地IP</span></span><br><span class="line">a2.sources.r1.bind = 0.0.0.0</span><br><span class="line"><span class="comment"># 设置监听端口号</span></span><br><span class="line">a2.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line"><span class="comment"># Channels</span></span><br><span class="line"><span class="comment"># a2.channels.c1</span></span><br><span class="line"><span class="comment"># 使用内存作为缓存/最多缓存的Event个数/单次传输的Event个数</span></span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sinks</span></span><br><span class="line"><span class="comment"># 运行时设置参数 -Dflume.root.logger=INFO,console 即输出到控制台实时显示</span></span><br><span class="line">a2.sinks.k1.type = logger</span><br><span class="line"><span class="comment"># 设置Event的Body中写入log的最大字节数(默认值为16)</span></span><br><span class="line">a2.sinks.k1.maxBytesToLog = 256</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind</span></span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>



<h3 id="3）flume3-properties"><a href="#3）flume3-properties" class="headerlink" title="3）flume3.properties"></a>3）flume3.properties</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># flume3:此配置用于将来自指定Avro端口的数据输出到控制台</span></span><br><span class="line"><span class="comment"># a3:Avro Source-&gt;Memory Channel-&gt;Logger Sink</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent</span></span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.channels = c1</span><br><span class="line">a3.sinks = k1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sources</span></span><br><span class="line"><span class="comment"># a3.sources.r1</span></span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line"><span class="comment"># 设置监听本地IP</span></span><br><span class="line">a3.sources.r1.bind = 0.0.0.0</span><br><span class="line"><span class="comment"># 设置监听端口号</span></span><br><span class="line">a3.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line"><span class="comment"># Channels</span></span><br><span class="line"><span class="comment"># a3.channels.c1</span></span><br><span class="line"><span class="comment"># 使用内存作为缓存/最多缓存的Event个数/单次传输的Event个数</span></span><br><span class="line">a3.channels.c1.type = memory</span><br><span class="line">a3.channels.c1.capacity = 1000</span><br><span class="line">a3.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sinks</span></span><br><span class="line"><span class="comment"># 运行时设置参数 -Dflume.root.logger=INFO,console 即输出到控制台实时显示</span></span><br><span class="line">a3.sinks.k1.type = logger</span><br><span class="line"><span class="comment"># 设置Event的Body中写入log的最大字节数(默认值为16)</span></span><br><span class="line">a3.sinks.k1.maxBytesToLog = 256</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind</span></span><br><span class="line">a3.sources.r1.channels = c1</span><br><span class="line">a3.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>



<h3 id="4）对应功能"><a href="#4）对应功能" class="headerlink" title="4）对应功能"></a>4）对应功能</h3><ul>
<li><strong>Aent a1将指定端口的监听数据输出到a2或者a3的控制台</strong></li>
<li><strong>当Event从Channel中传输给Sink Group之前，首先会根据配置Failover sink processor优先级表尝试将此Event发送给优先级最高的可用Sink，如果成功则继续处理下一个Event。如果在发送过程中，当前Sink宕机，则将其加入黑名单，一定时间内不再尝试将Event发往此Sink，并且退避时间呈指数增长，直到最大退避时间maxpenalty，以此来实现Sink的故障转移</strong></li>
</ul>
<h3 id="5）启动命令"><a href="#5）启动命令" class="headerlink" title="5）启动命令"></a>5）启动命令</h3><p><strong>Flume Agent a1至a3分别运行在主机hadoop101、hadoop102、hadoop103上</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./bin/flume-ng agent -n a1 -c conf -f flume1.properties</span><br><span class="line">./bin/flume-ng agent -n a2 -c conf -f flume2.properties -Dflume.root.logger=INFO,console</span><br><span class="line">./bin/flume-ng agent -n a3 -c conf -f flume3.properties -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>



<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Flume</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>Flume之Multiplexing Channel Selector使用示例</title>
    <url>/2020/03/05/Flume%E4%B9%8BMultiplexing-Channel-Selector%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B/</url>
    <content><![CDATA[<hr>
<a id="more"></a>

<hr>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul>
<li><strong>Multiplexing Channe Selector</strong> 的作用就是根据 <strong>Event</strong> 的 <strong>Header</strong> 中的某个或几个字段的值将其映射到指定的 <strong>Channel</strong> ，便于之后 <strong>Channel Processor</strong> 将Event发送至对应的Channel中去。在Flume中，Multiplexing Channel Selector一般都与 <strong>Interceptor</strong> 拦截器搭配使用，因为新鲜的Event数据中Header为空，需要Interceptor去填充所需字段</li>
</ul>
<hr>
<h2 id="具体配置"><a href="#具体配置" class="headerlink" title="具体配置"></a>具体配置</h2><h3 id="1）flume1-properties"><a href="#1）flume1-properties" class="headerlink" title="1）flume1.properties"></a>1）flume1.properties</h3><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment"># flume1:此配置用于监控单个或多个指定文件将其追加内容生成的Event先通过自定义的TypeInterceptor</span></span><br><span class="line"><span class="comment"># 根据Body中的内容向其Header中添加type字段,然后使用Multiplexing Channel Selector将不同</span></span><br><span class="line"><span class="comment"># type的Event传输到不同的Channel中,最后分别输出到flume2和flume3的控制台</span></span><br><span class="line"><span class="comment"># a1:TailDir Source-&gt; TypeInterceptor -&gt; Multiplexing Channel Selector -&gt;</span></span><br><span class="line"><span class="comment">#   Memory Channel -&gt; Avro Sink</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent</span></span><br><span class="line"><span class="meta">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="meta">a1.channels</span> = <span class="string">c1 c2</span></span><br><span class="line"><span class="meta">a1.sinks</span> = <span class="string">k1 k2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Sources</span></span><br><span class="line"><span class="comment"># a1.sources.r1</span></span><br><span class="line"><span class="meta">a1.sources.r1.type</span> = <span class="string">TAILDIR</span></span><br><span class="line"><span class="comment"># 设置Json文件存储路径(最好使用绝对路径)</span></span><br><span class="line"><span class="comment"># 用于记录文件inode/文件的绝对路径/每个文件的最后读取位置等信息</span></span><br><span class="line"><span class="meta">a1.sources.r1.positionFile</span> = <span class="string">/opt/module/flume-1.8.0/.position/taildir_position.json</span></span><br><span class="line"><span class="comment"># 指定监控的文件组</span></span><br><span class="line"><span class="meta">a1.sources.r1.filegroups</span> = <span class="string">f1</span></span><br><span class="line"><span class="comment"># 配置文件组中的被监控文件</span></span><br><span class="line"><span class="comment"># 设置f2组的监控文件,注意:使用的是正则表达式,而不是Linux通配符</span></span><br><span class="line"><span class="meta">a1.sources.r1.filegroups.f1</span> = <span class="string">/tmp/logs/^.*log$</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Interceptor</span></span><br><span class="line"><span class="comment"># a1.sources.r1.interceptors</span></span><br><span class="line"><span class="comment"># 配置Interceptor链,Interceptor调用顺序与配置循序相同</span></span><br><span class="line"><span class="meta">a1.sources.r1.interceptors</span> = <span class="string">typeInterceptor</span></span><br><span class="line"><span class="comment"># 指定使用的自定义Interceptor全类名,并使用其中的静态内部类Builder</span></span><br><span class="line"><span class="comment"># 要想使用自定义Interceptor,必须将实现的类打包成jar包放入$FLUME_HOME/lib文件夹中</span></span><br><span class="line"><span class="comment"># flume运行Java程序时会将此路径加入到ClassPath中</span></span><br><span class="line"><span class="meta">a1.sources.r1.interceptors.typeInterceptor.type</span> = <span class="string">com.tomandersen.interceptors.TypeInterceptor$Builder</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Channels</span></span><br><span class="line"><span class="comment"># a1.channels.c1</span></span><br><span class="line"><span class="comment"># 使用内存作为缓存/最多缓存的Event个数/单次传输的Event个数</span></span><br><span class="line"><span class="meta">a1.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="meta">a1.channels.c1.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="meta">a1.channels.c1.transactionCapacity</span> = <span class="string">100</span></span><br><span class="line"><span class="comment"># a1.channels.c2</span></span><br><span class="line"><span class="meta">a1.channels.c2.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="meta">a1.channels.c2.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="meta">a1.channels.c2.transactionCapacity</span> = <span class="string">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Channel Selector</span></span><br><span class="line"><span class="comment"># a1.sources.r1.selector</span></span><br><span class="line"><span class="comment"># 使用Multiple Channel Selector</span></span><br><span class="line"><span class="meta">a1.sources.r1.selector.type</span> = <span class="string">multiplexing</span></span><br><span class="line"><span class="comment"># 设置匹配Header的字段</span></span><br><span class="line"><span class="meta">a1.sources.r1.selector.header</span> = <span class="string">type</span></span><br><span class="line"><span class="comment"># 设置不同字段的值映射至各个Channel,其余的Event默认丢弃</span></span><br><span class="line"><span class="meta">a1.sources.r1.selector.mapping.Startup</span> = <span class="string">c1</span></span><br><span class="line"><span class="meta">a1.sources.r1.selector.mapping.Event</span> = <span class="string">c2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Sinks</span></span><br><span class="line"><span class="comment"># a1.sinks.k1</span></span><br><span class="line"><span class="meta">a1.sinks.k1.type</span> = <span class="string">avro</span></span><br><span class="line"><span class="meta">a1.sinks.k1.hostname</span> = <span class="string">hadoop102</span></span><br><span class="line"><span class="meta">a1.sinks.k1.port</span> = <span class="string">4141</span></span><br><span class="line"><span class="comment"># a1.sinks.k2</span></span><br><span class="line"><span class="meta">a1.sinks.k2.type</span> = <span class="string">avro</span></span><br><span class="line"><span class="meta">a1.sinks.k2.hostname</span> = <span class="string">hadoop103</span></span><br><span class="line"><span class="meta">a1.sinks.k2.port</span> = <span class="string">4141</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind</span></span><br><span class="line"><span class="comment"># r1-&gt;TypeInterceptor-&gt;Multiplexing Channel Selector-&gt;c1-&gt;k1</span></span><br><span class="line"><span class="comment"># r1-&gt;TypeInterceptor-&gt;Multiplexing Channel Selector-&gt;c2-&gt;k2</span></span><br><span class="line"><span class="meta">a1.sources.r1.channels</span> = <span class="string">c1 c2</span></span><br><span class="line"><span class="meta">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br><span class="line"><span class="meta">a1.sinks.k2.channel</span> = <span class="string">c2</span></span><br></pre></td></tr></table></figure>

<h3 id="2）flume2-properties"><a href="#2）flume2-properties" class="headerlink" title="2）flume2.properties"></a>2）flume2.properties</h3><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment"># flume2:此配置用于将来自指定Avro端口的数据输出到控制台中</span></span><br><span class="line"><span class="comment"># a2:Avro Source-&gt;Memory Channel-&gt;Logger Sink</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent</span></span><br><span class="line"><span class="meta">a2.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="meta">a2.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="meta">a2.sinks</span> = <span class="string">k1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Sources</span></span><br><span class="line"><span class="meta">a2.sources.r1.type</span> = <span class="string">avro</span></span><br><span class="line"><span class="meta">a2.sources.r1.bind</span> = <span class="string">0.0.0.0</span></span><br><span class="line"><span class="meta">a2.sources.r1.port</span> = <span class="string">4141</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Channels</span></span><br><span class="line"><span class="meta">a2.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="meta">a2.channels.c1.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="meta">a2.channels.c1.transactionCapacity</span> = <span class="string">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Sinks</span></span><br><span class="line"><span class="comment"># 运行时设置参数 -Dflume.root.logger=INFO,console 即输出到控制台实时显示</span></span><br><span class="line"><span class="meta">a2.sinks.k1.type</span> = <span class="string">logger</span></span><br><span class="line"><span class="comment"># 设置Event的Body中写入log的最大字节数(默认值为16)</span></span><br><span class="line"><span class="meta">a2.sinks.k1.maxBytesToLog</span> = <span class="string">256</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind</span></span><br><span class="line"><span class="attr">r1-&gt;c1-&gt;k1</span></span><br><span class="line"><span class="meta">a2.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="meta">a2.sinks.k1.channel</span> = <span class="string">c1</span></span><br></pre></td></tr></table></figure>

<h3 id="3）flume3-properties"><a href="#3）flume3-properties" class="headerlink" title="3）flume3.properties"></a>3）flume3.properties</h3><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment"># flume3:此配置用于将来自指定Avro端口的数据输出到控制台中</span></span><br><span class="line"><span class="comment"># a3:Avro Source-&gt;Memory Channel-&gt;Logger Sink</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent</span></span><br><span class="line"><span class="meta">a3.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="meta">a3.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="meta">a3.sinks</span> = <span class="string">k1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Sources</span></span><br><span class="line"><span class="meta">a3.sources.r1.type</span> = <span class="string">avro</span></span><br><span class="line"><span class="meta">a3.sources.r1.bind</span> = <span class="string">0.0.0.0</span></span><br><span class="line"><span class="meta">a3.sources.r1.port</span> = <span class="string">4141</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Channels</span></span><br><span class="line"><span class="meta">a3.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="meta">a3.channels.c1.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="meta">a3.channels.c1.transactionCapacity</span> = <span class="string">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Sinks</span></span><br><span class="line"><span class="comment"># 运行时设置参数 -Dflume.root.logger=INFO,console 即输出到控制台实时显示</span></span><br><span class="line"><span class="meta">a3.sinks.k1.type</span> = <span class="string">logger</span></span><br><span class="line"><span class="comment"># 设置Event的Body中写入log的最大字节数(默认值为16)</span></span><br><span class="line"><span class="meta">a3.sinks.k1.maxBytesToLog</span> = <span class="string">256</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind</span></span><br><span class="line"><span class="attr">r1-&gt;c1-&gt;k1</span></span><br><span class="line"><span class="meta">a3.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="meta">a3.sinks.k1.channel</span> = <span class="string">c1</span></span><br></pre></td></tr></table></figure>

<h3 id="4）对应功能"><a href="#4）对应功能" class="headerlink" title="4）对应功能"></a>4）对应功能</h3><p><strong>Agent a1监听本地指定文件,将监听到的数据组装成Event通过自定义的 TypeInterceptor 来根据其Body中的内容向Header中添加不同的type字段键值，然后通过 Multiplexing Channel Selector将不同type的Event发送给不同的Channel，并最终分别在a2和a3的控制台上输出</strong></p>
<h3 id="5）启动命令"><a href="#5）启动命令" class="headerlink" title="5）启动命令"></a>5）启动命令</h3><p><strong>Agent a1至a3分别运行在主机hadoop101、hadoop102、hadoop103上</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./bin/flume-ng agent -n a1 -c conf -f flume1.properties</span><br><span class="line">./bin/flume-ng agent -n a2 -c conf -f flume2.properties -Dflume.root.logger=INFO,console</span><br><span class="line">./bin/flume-ng agent -n a3 -c conf -f flume3.properties -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>





<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Flume</tag>
        <tag>大数据</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title>Flume之实现和使用自定义Interceptor</title>
    <url>/2020/03/05/Flume%E4%B9%8B%E5%AE%9E%E7%8E%B0%E5%92%8C%E4%BD%BF%E7%94%A8%E8%87%AA%E5%AE%9A%E4%B9%89Interceptor/</url>
    <content><![CDATA[<hr>
<a id="more"></a>

<hr>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul>
<li>Flume Interceptor拦截器的作用在于能够在Event从Source传输到Channel过程中，修改或者删除Event的Header。多个拦截器Interceptor组成一个拦截器链，拦截器的执行顺序与配置顺序相同，上一个拦截器Interceptor处理后的Event List会传给下一个Interceptor</li>
<li>在Flume中自定义Interceptor时，需要实现org.apache.flume.interceptor.Interceptor接口，以及创建静态内部类去实现org.apache.flume.interceptor.Interceptor.Builder接口</li>
<li>更多详细内容可以参考《Flume构建高可用、可扩展的海量日志采集系统》</li>
</ul>
<hr>
<h2 id="实现自定义Interceptor拦截器"><a href="#实现自定义Interceptor拦截器" class="headerlink" title="实现自定义Interceptor拦截器"></a>实现自定义Interceptor拦截器</h2><h3 id="1）根据使用场景创建Interceptor类"><a href="#1）根据使用场景创建Interceptor类" class="headerlink" title="1）根据使用场景创建Interceptor类"></a>1）根据使用场景创建Interceptor类</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TypeInterceptor</span> <span class="keyword">implements</span> <span class="title">Interceptor</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化时可以不做操作</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// Do nothing</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 单个Event拦截</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Event <span class="title">intercept</span><span class="params">(<span class="keyword">final</span> Event event)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 1.获取Event中的Header</span></span><br><span class="line">        Map&lt;String, String&gt; headers = event.getHeaders();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.获取Event中的Body,将其转换成字符串String</span></span><br><span class="line">        String body = <span class="keyword">new</span> String(event.getBody());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.根据Body中数据向Header添加键值对,表明日志类型</span></span><br><span class="line">        <span class="keyword">if</span> (body.contains(<span class="string">"cm"</span>)) &#123;</span><br><span class="line">            <span class="comment">// 4.添加Header信息</span></span><br><span class="line">            headers.put(<span class="string">"type"</span>, <span class="string">"Startup"</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// 4.添加Header信息</span></span><br><span class="line">            headers.put(<span class="string">"type"</span>, <span class="string">"Event"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> event;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 批量Event拦截</span></span><br><span class="line">    <span class="comment">// 注意:既可以原Event集合进行修改,也可以创建新的Event集合作为成员变量,将此成员变量返回</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Event&gt; <span class="title">intercept</span><span class="params">(<span class="keyword">final</span> List&lt;Event&gt; events)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (Event event : events) &#123;</span><br><span class="line">            <span class="comment">// 1.对每个Event采用单个Event拦截的方式进行处理,忽略其返回值</span></span><br><span class="line">            intercept(event);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 2.返回处理结果</span></span><br><span class="line">        <span class="keyword">return</span> events;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭时可以不作操作</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// Do nothing</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建静态内部类实现Interceptor.Builder接口</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Builder</span> <span class="keyword">implements</span> <span class="title">Interceptor</span>.<span class="title">Builder</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 定义配置信息</span></span><br><span class="line">        <span class="keyword">private</span> Context context;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 定义Interceptor生成器</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Interceptor <span class="title">build</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> TypeInterceptor();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取配置信息</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.context = context;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="2）将此Interceptor类打包，并将jar包放入flume-lib路径下"><a href="#2）将此Interceptor类打包，并将jar包放入flume-lib路径下" class="headerlink" title="2）将此Interceptor类打包，并将jar包放入flume/lib路径下"></a>2）将此Interceptor类打包，并将jar包放入<code>flume/lib</code>路径下</h3><p><strong>此路径在Flume运行时的ClassPath中，因而可以在flume配置文件中可以通过全类名指定使用的Interceptor.Builder类</strong></p>
<p><strong>Maven项目打包插件配置可以参考：</strong><a href="https://blog.csdn.net/TomAndersen/article/details/104245064" target="_blank" rel="noopener">《IDEA中配置Maven项目打包插件》</a></p>
<hr>
<h2 id="创建flume-Agent配置文件"><a href="#创建flume-Agent配置文件" class="headerlink" title="创建flume Agent配置文件"></a>创建flume Agent配置文件</h2><h3 id="1）flume1-properties"><a href="#1）flume1-properties" class="headerlink" title="1）flume1.properties"></a>1）flume1.properties</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># flume1:此配置用于监控单个或多个指定文件将其追加内容先通过自定义的TypeInterceptor</span></span><br><span class="line"><span class="comment"># 向Header中添加type字段,然后使用Multiplexing Channel Selector将不同type的Event</span></span><br><span class="line"><span class="comment"># 传输到不同的Channel中,最后分别输出到flume2和flume3的控制台</span></span><br><span class="line"><span class="comment"># a1:TailDir Source-&gt; TypeInterceptor -&gt; Multiplexing Channel Selector -&gt;</span></span><br><span class="line"><span class="comment">#   Memory Channel -&gt; Avro Sink</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1 c2</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sources</span></span><br><span class="line"><span class="comment"># a1.sources.r1</span></span><br><span class="line">a1.sources.r1.type = TAILDIR</span><br><span class="line"><span class="comment"># 设置Json文件存储路径(最好使用绝对路径)</span></span><br><span class="line"><span class="comment"># 用于记录文件inode/文件的绝对路径/每个文件的最后读取位置等信息</span></span><br><span class="line">a1.sources.r1.positionFile = /opt/module/flume-1.8.0/.position/taildir_position.json</span><br><span class="line"><span class="comment"># 指定监控的文件组</span></span><br><span class="line">a1.sources.r1.filegroups = f1</span><br><span class="line"><span class="comment"># 配置文件组中的被监控文件</span></span><br><span class="line"><span class="comment"># 设置f2组的监控文件,注意:使用的是正则表达式,而不是Linux通配符</span></span><br><span class="line">a1.sources.r1.filegroups.f1 = /tmp/logs/^.*<span class="built_in">log</span>$</span><br><span class="line"></span><br><span class="line"><span class="comment"># Interceptor</span></span><br><span class="line"><span class="comment"># a1.sources.r1.interceptors</span></span><br><span class="line"><span class="comment"># 配置Interceptor链,Interceptor调用顺序与配置循序相同</span></span><br><span class="line">a1.sources.r1.interceptors = typeInterceptor</span><br><span class="line"><span class="comment"># 指定使用的自定义Interceptor全类名,并使用其中的静态内部类Builder</span></span><br><span class="line"><span class="comment"># 要想使用自定义Interceptor,必须将实现的类打包成jar包放入$FLUME_HOME/lib文件夹中</span></span><br><span class="line"><span class="comment"># flume运行Java程序时会将此路径加入到ClassPath中</span></span><br><span class="line">a1.sources.r1.interceptors.typeInterceptor.type = com.tomandersen.interceptors.TypeInterceptor<span class="variable">$Builder</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Channels</span></span><br><span class="line"><span class="comment"># a1.channels.c1</span></span><br><span class="line"><span class="comment"># 使用内存作为缓存/最多缓存的Event个数/单次传输的Event个数</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="comment"># a1.channels.c2</span></span><br><span class="line">a1.channels.c2.type = memory</span><br><span class="line">a1.channels.c2.capacity = 1000</span><br><span class="line">a1.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="comment"># Channel Selector</span></span><br><span class="line"><span class="comment"># a1.sources.r1.selector</span></span><br><span class="line"><span class="comment"># 使用Multiple Channel Selector</span></span><br><span class="line">a1.sources.r1.selector.type = multiplexing</span><br><span class="line"><span class="comment"># 设置匹配Header的字段</span></span><br><span class="line">a1.sources.r1.selector.header = <span class="built_in">type</span></span><br><span class="line"><span class="comment"># 设置不同字段的值映射至各个Channel,其余的Event默认丢弃</span></span><br><span class="line">a1.sources.r1.selector.mapping.Startup = c1</span><br><span class="line">a1.sources.r1.selector.mapping.Event = c2</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sinks</span></span><br><span class="line"><span class="comment"># a1.sinks.k1</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = hadoop102</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line"><span class="comment"># a1.sinks.k2</span></span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = hadoop103</span><br><span class="line">a1.sinks.k2.port = 4141</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind</span></span><br><span class="line"><span class="comment"># r1-&gt;TypeInterceptor-&gt;Multiplexing Channel Selector-&gt;c1-&gt;k1</span></span><br><span class="line"><span class="comment"># r1-&gt;TypeInterceptor-&gt;Multiplexing Channel Selector-&gt;c2-&gt;k2</span></span><br><span class="line">a1.sources.r1.channels = c1 c2</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c2</span><br></pre></td></tr></table></figure>

<h3 id="2）flume2-properties"><a href="#2）flume2-properties" class="headerlink" title="2）flume2.properties"></a>2）flume2.properties</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># flume2:此配置用于将来自指定Avro端口的数据输出到控制台中</span></span><br><span class="line"><span class="comment"># a2:Avro Source-&gt;Memory Channel-&gt;Logger Sink</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent</span></span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.channels = c1</span><br><span class="line">a2.sinks = k1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sources</span></span><br><span class="line">a2.sources.r1.type = avro</span><br><span class="line">a2.sources.r1.bind = 0.0.0.0</span><br><span class="line">a2.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line"><span class="comment"># Channels</span></span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sinks</span></span><br><span class="line"><span class="comment"># 运行时设置参数 -Dflume.root.logger=INFO,console 即输出到控制台实时显示</span></span><br><span class="line">a2.sinks.k1.type = logger</span><br><span class="line"><span class="comment"># 设置Event的Body中写入log的最大字节数(默认值为16)</span></span><br><span class="line">a2.sinks.k1.maxBytesToLog = 256</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind</span></span><br><span class="line">r1-&gt;c1-&gt;k1</span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<h3 id="3）flume3-properties"><a href="#3）flume3-properties" class="headerlink" title="3）flume3.properties"></a>3）flume3.properties</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># flume3:此配置用于将来自指定Avro端口的数据输出到控制台中</span></span><br><span class="line"><span class="comment"># a3:Avro Source-&gt;Memory Channel-&gt;Logger Sink</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent</span></span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.channels = c1</span><br><span class="line">a3.sinks = k1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sources</span></span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = 0.0.0.0</span><br><span class="line">a3.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line"><span class="comment"># Channels</span></span><br><span class="line">a3.channels.c1.type = memory</span><br><span class="line">a3.channels.c1.capacity = 1000</span><br><span class="line">a3.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sinks</span></span><br><span class="line"><span class="comment"># 运行时设置参数 -Dflume.root.logger=INFO,console 即输出到控制台实时显示</span></span><br><span class="line">a3.sinks.k1.type = logger</span><br><span class="line"><span class="comment"># 设置Event的Body中写入log的最大字节数(默认值为16)</span></span><br><span class="line">a3.sinks.k1.maxBytesToLog = 256</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind</span></span><br><span class="line">r1-&gt;c1-&gt;k1</span><br><span class="line">a3.sources.r1.channels = c1</span><br><span class="line">a3.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>



<h3 id="4）对应功能"><a href="#4）对应功能" class="headerlink" title="4）对应功能"></a>4）对应功能</h3><p><strong>Flume Agent a1监听本地指定文件,将监听到的数据组装成Event通过自定义的 TypeInterceptor 来根据其Body中的内容向Header中添加不同的type字段键值，然后通过 Multiplexing Channel Selector将不同type的Event发送给不同的Channel，并最终分别在Flume Agent a2和a3的控制台上输出</strong></p>
<h3 id="5）启动命令"><a href="#5）启动命令" class="headerlink" title="5）启动命令"></a>5）启动命令</h3><p><strong>Flume Agent a1至a3分别运行在主机hadoop101、hadoop102、hadoop103上</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./bin/flume-ng agent -n a1 -c conf -f flume1.properties</span><br><span class="line">./bin/flume-ng agent -n a2 -c conf -f flume2.properties -Dflume.root.logger=INFO,console</span><br><span class="line">./bin/flume-ng agent -n a3 -c conf -f flume3.properties -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>



<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Flume</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>Maven之子模块pom.xml继承父模块pom.xml配置</title>
    <url>/2020/03/05/Maven%E4%B9%8B%E5%AD%90%E6%A8%A1%E5%9D%97pom-xml%E7%BB%A7%E6%89%BF%E7%88%B6%E6%A8%A1%E5%9D%97pom-xml%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<hr>
<a id="more"></a>

<hr>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Maven中可以通过继承父模块pom，来实现pom.xml配置的继承和传递，便于各种Maven插件以及程序依赖的统一管理。通过将子类模块的公共配置，抽象聚合生成父类模块，能够避免pom.xml的重复配置。由于父类模块本身并不包含除了POM之外的项目文件，也就不需要src/main/java之类的文件夹了。每当需要对多个子模块进行相同的配置时，只需要在父类模块的pom中进行配置，而子类中声明使用此配置即可，当然子类pom中也可以自定义配置，并覆盖父类中的各项配置，和Java中类的继承类似。</p>
<hr>
<h2 id="可继承的POM元素"><a href="#可继承的POM元素" class="headerlink" title="可继承的POM元素"></a>可继承的POM元素</h2><p><strong>1) <code>groupId</code>：项目组ID，项目坐标的核心元素</strong></p>
<p><strong>2) <code>version</code>：项目版本，项目坐标的核心元素</strong></p>
<p><strong>3) <code>description</code>：项目的表述信息</strong></p>
<p><strong>4) <code>organization</code>：项目的组织信息</strong></p>
<p><strong>5) <code>inception Year</code>：项目的创始年份</strong></p>
<p><strong>6) <code>url</code>：项目的URL地址</strong></p>
<p><strong>7) <code>developers</code>：项目的开发者信息</strong></p>
<p><strong>8) <code>contributors</code>：项目的贡献者信息</strong></p>
<p><strong>9) <code>distributionManagement</code>：项目的部署管理</strong></p>
<p><strong>10) <code>issueManagement</code>：项目的缺陷和跟踪系统信息</strong></p>
<p><strong>11) <code>ciManagement</code>：项目的持续集成信息系统</strong></p>
<p><strong>12) <code>scm</code>：项目的版本控制系统信息</strong></p>
<p><strong>13) <code>mailingLists</code>：项目的邮件列表信息</strong></p>
<p><strong>14) <code>properties</code>：自定义的Maven属性</strong></p>
<p><strong>15) <code>dependencies</code>：项目的依赖属性</strong></p>
<p><strong>16) <code>dependencyManagement</code>：项目的依赖管理配置</strong></p>
<p><strong>17) <code>repositories</code>：项目的仓库配置</strong></p>
<p><strong>18) <code>build</code>：包括项目的源码目录配置、输出目录配置、插件配置、插件管理配置等</strong></p>
<p><strong>19) <code>reporting</code>：包括项目的报告输出目录配置、报告插件配置等</strong></p>
<hr>
<h2 id="POM继承中的依赖管理和插件管理"><a href="#POM继承中的依赖管理和插件管理" class="headerlink" title="POM继承中的依赖管理和插件管理"></a>POM继承中的依赖管理和插件管理</h2><p>Maven提供的<code>dependencyManagement</code>和<code>pluginManagement</code>元素用于帮助POM继承过程中的依赖管理和插件管理。在父类POM下，此两个元素中的声明的依赖或配置并不会引入实际的依赖或是造成实际的插件调用行为，不过它们能够约束子类POM中的依赖和插件配置的声明。只有当子类POM中配置了真正的<code>dependency</code>或<code>plugin</code>，并且其<code>groupId</code>和<code>artifactId</code>与父类POM中<code>dependencyManagement</code>和<code>pluginManagement</code>相对应时，才会进行实际的依赖引入或插件调用，当然子类中也能够进行自定义配置去覆盖父类，或是额外声明自己的配置</p>
<hr>
<h2 id="POM继承示例"><a href="#POM继承示例" class="headerlink" title="POM继承示例"></a>POM继承示例</h2><p><strong>1) 在父类POM中使用<code>dependencyManagement</code>和<code>pluginManagement</code>，声明子类POM中可能用到的依赖和插件</strong></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.tomandersen<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>HadoopCustomModules<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">packaging</span>&gt;</span>pom<span class="tag">&lt;/<span class="name">packaging</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">modules</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">module</span>&gt;</span>flume<span class="tag">&lt;/<span class="name">module</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">module</span>&gt;</span>log-collector<span class="tag">&lt;/<span class="name">module</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">modules</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--事先声明版本属性--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">slf4j.version</span>&gt;</span>1.7.20<span class="tag">&lt;/<span class="name">slf4j.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">logback.version</span>&gt;</span>1.0.7<span class="tag">&lt;/<span class="name">logback.version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--在父类Maven中使用dependencyManagement声明依赖便于子类Module继承使用,也便于进行依赖版本控制--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencyManagement</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--阿里巴巴开源json解析框架--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.alibaba<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>fastjson<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.51<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!--日志生成框架--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>ch.qos.logback<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>logback-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;logback.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>ch.qos.logback<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>logback-classic<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;logback.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencyManagement</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--在父类Maven中使用pluginManagement管理插件便于子类Module继承使用,也便于进行依赖版本控制--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">pluginManagement</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!--Maven项目编译器compiler插件--&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.3.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">&lt;!--Maven项目汇编assembly插件--&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="name">descriptorRef</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">archive</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">manifest</span>&gt;</span></span><br><span class="line">                            <span class="comment">&lt;!--子类Maven通过mainClass标签设置成主类的全类名FQCN--&gt;</span></span><br><span class="line">                            <span class="comment">&lt;!--&lt;mainClass&gt;&lt;/mainClass&gt;--&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">manifest</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">archive</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">id</span>&gt;</span>make-assembly<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>single<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">pluginManagement</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><strong>2) 在子类POM中声明父类POM，并配置实际使用的<code>dependency</code>和<code>plugin</code>，只需要通过声明<code>groupId</code>和<code>artifactId</code>就可以避免配置各种依赖和插件的详细配置，当然也可以自己覆盖父类配置信息</strong></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!--声明父类POM--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">parent</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>HadoopCustomModules<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.tomandersen<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">parent</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--子类POM信息--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.tomandersen<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log-collector<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--阿里巴巴开源json解析框架--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.alibaba<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>fastjson<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--日志生成框架--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>ch.qos.logback<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>logback-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>ch.qos.logback<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>logback-classic<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--自定义Maven项目编译器compiler插件相关配置--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!--自定义Maven项目汇编assembly插件相关配置--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">archive</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">manifest</span>&gt;</span></span><br><span class="line">                        <span class="comment">&lt;!--此处设置成主类的全名--&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span>com.tomandersen.appclient.AppMain<span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">manifest</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">archive</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="更多详细内容可以查阅《Maven实战》"><a href="#更多详细内容可以查阅《Maven实战》" class="headerlink" title="更多详细内容可以查阅《Maven实战》"></a>更多详细内容可以查阅《Maven实战》</h2><hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Maven</category>
      </categories>
      <tags>
        <tag>Maven</tag>
        <tag>XML</tag>
      </tags>
  </entry>
  <entry>
    <title>Flume之HDFS-Sink使用案例</title>
    <url>/2020/03/05/Flume%E4%B9%8BHDFS-Sink%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B/</url>
    <content><![CDATA[<hr>
<a id="more"></a>

<hr>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul>
<li>操作系统：CentOS 7</li>
<li>Java版本：1.8.0_221</li>
<li>Flume版本：1.8.0</li>
<li>HDFS版本：2.7.7</li>
<li>Flume agent配置：Netcat TCP Source、Memory Channel、HDFS Sink</li>
</ul>
<hr>
<h2 id="具体步骤"><a href="#具体步骤" class="headerlink" title="具体步骤"></a>具体步骤</h2><h3 id="a-拷贝Hadoop相关jar包至flume-lib-路径下"><a href="#a-拷贝Hadoop相关jar包至flume-lib-路径下" class="headerlink" title="a) 拷贝Hadoop相关jar包至flume/lib/路径下"></a>a) 拷贝Hadoop相关jar包至<code>flume/lib/</code>路径下</h3><h4 id="在hadoop-2-7-7-share-路径下找到以下对应jar包，并将其拷贝至flume-lib-路径下。Flume启动时，会将此路径添加至ClassPath"><a href="#在hadoop-2-7-7-share-路径下找到以下对应jar包，并将其拷贝至flume-lib-路径下。Flume启动时，会将此路径添加至ClassPath" class="headerlink" title="在hadoop-2.7.7/share/路径下找到以下对应jar包，并将其拷贝至flume/lib/路径下。Flume启动时，会将此路径添加至ClassPath"></a>在<code>hadoop-2.7.7/share/</code>路径下找到以下对应jar包，并将其拷贝至<code>flume/lib/</code>路径下。Flume启动时，会将此路径添加至ClassPath</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">commons-configuration-1.6.jar</span><br><span class="line">commons-io-2.4.jar</span><br><span class="line">hadoop-auth-2.7.7.jar</span><br><span class="line">hadoop-common-2.7.7.jar</span><br><span class="line">hadoop-hdfs-2.7.7.jar</span><br><span class="line">htrace-core-3.1.0-incubating.jar</span><br></pre></td></tr></table></figure>



<h3 id="b-根据使用场景配置properties文件"><a href="#b-根据使用场景配置properties文件" class="headerlink" title="b) 根据使用场景配置properties文件"></a>b) 根据使用场景配置properties文件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用于从netcat指定端口收集数据最终输出到HDFS中</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sources</span></span><br><span class="line"><span class="comment"># a1.sources.r1</span></span><br><span class="line"><span class="comment"># 配置source类型/绑定主机ip/端口号</span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = 0.0.0.0</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sinks</span></span><br><span class="line"><span class="comment"># a1.sinks.k1</span></span><br><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line"><span class="comment"># 设置hdfs文件路径,同时并设置了按照日期创建文件夹</span></span><br><span class="line">a1.sinks.k1.hdfs.path = /flume/logs/%Y-%m-%d/%H-%M-%S</span><br><span class="line"><span class="comment"># 设置flume创建的hdfs文件前缀</span></span><br><span class="line">a1.sinks.k1.hdfs.filePrefix = logs_%Y-%m-%d</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以下三组参数的配置用于控制flume在hdfs中生成文件的滚动方式</span></span><br><span class="line"><span class="comment"># 满足以下三者中任何一个条件都会新生成hdfs文件</span></span><br><span class="line"><span class="comment"># 设置文件滚动的时间间隔,单位(second),置0表示关闭</span></span><br><span class="line">a1.sinks.k1.hdfs.rollInterval = 10</span><br><span class="line"><span class="comment"># 设置文件滚动的最大size阈值,由于是hdfs sink故最好设置成Block Size的倍数</span></span><br><span class="line"><span class="comment"># 本次实验的hadoop版本为2.7.7(2.7.3之后默认Block Size为128MB,之前为64MB)</span></span><br><span class="line"><span class="comment"># 单位(bytes),置0表示关闭</span></span><br><span class="line">a1.sinks.k1.hdfs.rollSize = 134217700</span><br><span class="line"><span class="comment"># 设置滚动文件存储的最大Event个数</span></span><br><span class="line"><span class="comment"># 此参数一般设置为0,即关闭,除非有严格生产需求并且知道Event大小能够自主控制</span></span><br><span class="line">a1.sinks.k1.hdfs.rollCount = 0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置flume每批次刷到hdfs中的Event个数(超过一定时长也会进行刷新,并非要等满一批次)</span></span><br><span class="line">a1.sinks.k1.hdfs.batchSize = 100</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置hdfs文件格式,目前只支持(SequenceFile/DataStream/CompressedStream)</span></span><br><span class="line"><span class="comment"># CompressedStream类型需要配合hdfs.codeC参数来指定具体的压缩方式</span></span><br><span class="line"><span class="comment"># SequenceFile表示按照HDFS序列文件SequenceFile的方式进行压缩</span></span><br><span class="line"><span class="comment"># DataStream则表示不进行压缩</span></span><br><span class="line">a1.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以下三组参数的配置配合转义序列(如%y %m %d %H %M %S等)能够自定义时间轮转最小刻度</span></span><br><span class="line"><span class="comment"># 设置hdfs时间向下取整</span></span><br><span class="line"><span class="comment"># 设置向下取整之后文件夹将按照一定时间大小的刻度进行创建文件夹</span></span><br><span class="line"><span class="comment"># 否则都是按照之前设置每分钟进行文件夹的创建</span></span><br><span class="line">a1.sinks.k1.hdfs.round = <span class="literal">true</span></span><br><span class="line"><span class="comment"># 设置hdfs时间向下取整的最小单元倍数</span></span><br><span class="line">a1.sinks.k1.hdfs.roundValue = 30</span><br><span class="line"><span class="comment"># 设置hdfs时间向下取整的最小单位</span></span><br><span class="line">a1.sinks.k1.hdfs.roundUnit = second</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设定是否使用本地时间戳,默认为false(即使用Event的Header中的时间戳)</span></span><br><span class="line"><span class="comment"># 本次实验中Event的Header为空,需要使用本地时间戳</span></span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Channels</span></span><br><span class="line"><span class="comment"># 定义a2的channerls.c1的类型为memory,即使用内存作为缓存/最多缓存的Event个数/单次传输的Event个数</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind</span></span><br><span class="line"><span class="comment"># 注意:source可以绑定多个channel,但是sink只能绑定单个channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>



<h3 id="c-使用此配置文件启动agent"><a href="#c-使用此配置文件启动agent" class="headerlink" title="c) 使用此配置文件启动agent"></a>c) 使用此配置文件启动agent</h3><h4 id="启动脚本前保证HDFS集群正常运行"><a href="#启动脚本前保证HDFS集群正常运行" class="headerlink" title="启动脚本前保证HDFS集群正常运行"></a>启动脚本前保证HDFS集群正常运行</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop101 flume-1.8.0]$ call-cluster.sh jps</span><br><span class="line">----------hadoop103----------</span><br><span class="line">18272 Jps</span><br><span class="line">17794 DataNode</span><br><span class="line">17987 NodeManager</span><br><span class="line">18105 JobHistoryServer</span><br><span class="line">17868 SecondaryNameNode</span><br><span class="line">----------hadoop102----------</span><br><span class="line">17826 DataNode</span><br><span class="line">18457 Jps</span><br><span class="line">17950 ResourceManager</span><br><span class="line">18079 NodeManager</span><br><span class="line">----------hadoop101----------</span><br><span class="line">10321 DataNode</span><br><span class="line">10785 Jps</span><br><span class="line">10619 NodeManager</span><br><span class="line">10205 NameNode</span><br><span class="line"></span><br><span class="line">----------execute <span class="string">"jps"</span> <span class="keyword">in</span> cluster takes 6 seconds----------</span><br><span class="line"></span><br><span class="line">[tomandersen@hadoop101 flume-1.8.0]$</span><br></pre></td></tr></table></figure>

<h4 id="在Flume安装路径下通过bin-flume-ng脚本启动agent"><a href="#在Flume安装路径下通过bin-flume-ng脚本启动agent" class="headerlink" title="在Flume安装路径下通过bin/flume-ng脚本启动agent"></a>在Flume安装路径下通过<code>bin/flume-ng</code>脚本启动agent</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./bin/flume-ng agent -n a1 -c conf/ -f job/netcat-memory-hdfs.properties</span><br></pre></td></tr></table></figure>



<h3 id="d-发送测试数据并检查HDFS中是否成功上传对应数据"><a href="#d-发送测试数据并检查HDFS中是否成功上传对应数据" class="headerlink" title="d) 发送测试数据并检查HDFS中是否成功上传对应数据"></a>d) 发送测试数据并检查HDFS中是否成功上传对应数据</h3><h4 id="发送测试数据"><a href="#发送测试数据" class="headerlink" title="发送测试数据"></a>发送测试数据</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop101 ~]$ <span class="built_in">echo</span> Hello World! | nc localhost 44444</span><br><span class="line">OK</span><br><span class="line">[tomandersen@hadoop101 ~]$ </span><br><span class="line">[tomandersen@hadoop101 ~]$</span><br></pre></td></tr></table></figure>

<h4 id="进入NameNode-Web-UI页面查看HDFS文件"><a href="#进入NameNode-Web-UI页面查看HDFS文件" class="headerlink" title="进入NameNode Web UI页面查看HDFS文件"></a>进入NameNode Web UI页面查看HDFS文件</h4><p><img src="https://img-blog.csdnimg.cn/20200305113842376.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="查看HDFS文件"></p>
<h4 id="下载并查看查看HDFS文件内容"><a href="#下载并查看查看HDFS文件内容" class="headerlink" title="下载并查看查看HDFS文件内容"></a>下载并查看查看HDFS文件内容</h4><p><img src="https://img-blog.csdnimg.cn/20200305113859854.png" alt="查看HDFS文件"></p>
<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Flume</tag>
        <tag>大数据</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title>Flume之入门级安装部署</title>
    <url>/2020/03/05/Flume%E4%B9%8B%E5%85%A5%E9%97%A8%E7%BA%A7%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<hr>
<a id="more"></a>

<hr>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul>
<li>操作系统：CentOS 7</li>
<li>Java版本：1.8.0_221</li>
<li>Flume版本：1.8.0</li>
</ul>
<hr>
<h2 id="具体步骤"><a href="#具体步骤" class="headerlink" title="具体步骤"></a>具体步骤</h2><h3 id="a-下载flume"><a href="#a-下载flume" class="headerlink" title="a) 下载flume"></a>a) 下载flume</h3><ul>
<li>flume官网：<a href="https://flume.apache.org/" target="_blank" rel="noopener">https://flume.apache.org/</a></li>
<li>flume最新版：<a href="https://flume.apache.org/download.html" target="_blank" rel="noopener">https://flume.apache.org/download.html</a></li>
<li>flume各个历史版本：<a href="http://archive.apache.org/dist/flume/" target="_blank" rel="noopener">http://archive.apache.org/dist/flume/</a></li>
</ul>
<h3 id="b-安装flume"><a href="#b-安装flume" class="headerlink" title="b) 安装flume"></a>b) 安装flume</h3><h4 id="将flume解压到指定路径下"><a href="#将flume解压到指定路径下" class="headerlink" title="将flume解压到指定路径下"></a>将flume解压到指定路径下</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar -xzvf apache-flume-1.8.0-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>

<h4 id="修改flume默认文件夹名（可选）"><a href="#修改flume默认文件夹名（可选）" class="headerlink" title="修改flume默认文件夹名（可选）"></a>修改flume默认文件夹名（可选）</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mv apache-flume-1.8.0-bin flume-1.8.0</span><br></pre></td></tr></table></figure>

<h3 id="c-配置flume-env-sh文件"><a href="#c-配置flume-env-sh文件" class="headerlink" title="c) 配置flume-env.sh文件"></a>c) 配置flume-env.sh文件</h3><h4 id="拷贝flume-1-8-0-conf-flume-env-sh-template，创建flume-env-sh文件"><a href="#拷贝flume-1-8-0-conf-flume-env-sh-template，创建flume-env-sh文件" class="headerlink" title="拷贝flume-1.8.0/conf/flume-env.sh.template，创建flume-env.sh文件"></a>拷贝<code>flume-1.8.0/conf/flume-env.sh.template</code>，创建<code>flume-env.sh</code>文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cp flume-env.sh.template flume-env.sh</span><br></pre></td></tr></table></figure>

<h4 id="或者直接重命名"><a href="#或者直接重命名" class="headerlink" title="或者直接重命名"></a>或者直接重命名</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mv flume-env.sh.template flume-env.sh</span><br></pre></td></tr></table></figure>

<h4 id="修改flume-env-sh中JAVA-HOME，将其设置成Java安装绝对路径"><a href="#修改flume-env-sh中JAVA-HOME，将其设置成Java安装绝对路径" class="headerlink" title="修改flume-env.sh中JAVA_HOME，将其设置成Java安装绝对路径"></a>修改<code>flume-env.sh</code>中<code>JAVA_HOME</code>，将其设置成Java安装绝对路径</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim flume-env.sh</span><br><span class="line"><span class="comment"># 修改前</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/java-8-oracle</span><br><span class="line"><span class="comment"># 修改后</span></span><br><span class="line"><span class="comment"># export JAVA_HOME=/usr/lib/jvm/java-8-oracle</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_221</span><br></pre></td></tr></table></figure>

<h4 id="至此flume就已经配置完成了，flume运行日志的默认数据追加路径为-logs-flume-log"><a href="#至此flume就已经配置完成了，flume运行日志的默认数据追加路径为-logs-flume-log" class="headerlink" title="至此flume就已经配置完成了，flume运行日志的默认数据追加路径为./logs/flume.log"></a>至此flume就已经配置完成了，flume运行日志的默认数据追加路径为<code>./logs/flume.log</code></h4><h4 id="若要修改flume运行日志输出路径，修改并配置flume-1-8-0-conf-log4j-properties文件即可"><a href="#若要修改flume运行日志输出路径，修改并配置flume-1-8-0-conf-log4j-properties文件即可" class="headerlink" title="若要修改flume运行日志输出路径，修改并配置flume-1.8.0/conf/log4j.properties文件即可"></a>若要修改flume运行日志输出路径，修改并配置<code>flume-1.8.0/conf/log4j.properties</code>文件即可</h4><hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Flume</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>GitHub Page绑定至个人域名</title>
    <url>/2020/03/04/Github-Page%E7%BB%91%E5%AE%9A%E8%87%B3%E4%B8%AA%E4%BA%BA%E5%9F%9F%E5%90%8D/</url>
    <content><![CDATA[<hr>
<a id="more"></a>

<hr>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul>
<li><strong>此教程主要用于将个人域名映射至Github Page，同时实现Github Page在个人域名的发布</strong></li>
<li><strong>本次所使用的个人域名是在阿里云上申请注册所得</strong></li>
<li><strong>注意：如果想要取消Github Page发布，删除CNAME文件即可。然后清除浏览器缓存，就又可以使用原来的Github Page网址对其进行访问</strong></li>
</ul>
<hr>
<h2 id="具体步骤："><a href="#具体步骤：" class="headerlink" title="具体步骤："></a>具体步骤：</h2><h3 id="1-注册个人域名"><a href="#1-注册个人域名" class="headerlink" title="1. 注册个人域名"></a>1. 注册个人域名</h3><ul>
<li><strong>本人是在阿里云平台申请的个人域名，首年价格是19元，注册步骤就不再赘述了</strong></li>
<li><strong>实名认证到注册审批本人实测大概3个小时左右完成</strong></li>
</ul>
<h3 id="2-添加域名解析"><a href="#2-添加域名解析" class="headerlink" title="2. 添加域名解析"></a>2. 添加域名解析</h3><h4 id="a）进入阿里云控制台—域名列表，然后找到需要添加映射的个人域名"><a href="#a）进入阿里云控制台—域名列表，然后找到需要添加映射的个人域名" class="headerlink" title="a）进入阿里云控制台—域名列表，然后找到需要添加映射的个人域名"></a>a）进入阿里云控制台—域名列表，然后找到需要添加映射的个人域名</h4><p><img src="https://img-blog.csdnimg.cn/20200304231152820.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="添加域名解析"></p>
<h4 id="b）然后点击添加域名解析记录"><a href="#b）然后点击添加域名解析记录" class="headerlink" title="b）然后点击添加域名解析记录"></a>b）然后点击添加域名解析记录</h4><p><img src="https://img-blog.csdnimg.cn/20200304231210309.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="添加域名解析"></p>
<h4 id="c）配置域名映射"><a href="#c）配置域名映射" class="headerlink" title="c）配置域名映射"></a>c）配置域名映射</h4><ul>
<li><p><strong>“记录类型”设置成<code>CNAME</code></strong></p>
</li>
<li><p><strong>“主机记录”可以设置成<code>@</code>即直接使用主域名作映射，或者<code>www</code>即使用<code>www.主域名</code>的形式进行映射，因为本人是使用此域名映射个人博客，所以设置成<code>blog</code>即使用blog作为子域名且用于映射</strong></p>
</li>
<li><p><strong>“记录值”则设置成Github Page的网址即可</strong></p>
</li>
<li><p><strong>等待几分钟后生效</strong></p>
</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/20200304231230144.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="添加域名解析"></p>
<ul>
<li><strong>至此就建立了个人域名至Github Page的映射，但此时还不能使建立映射的域名直接访问Github Page，因为还需要在Github上发布自己的Github Page，直接访问的话会出现Github Pages 404</strong></li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/20200304231251195.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="添加域名解析"></p>
<h3 id="3-发布Github-Page"><a href="#3-发布Github-Page" class="headerlink" title="3. 发布Github Page"></a>3. 发布Github Page</h3><h4 id="a）进入Github-Page仓库页面，点击Setting"><a href="#a）进入Github-Page仓库页面，点击Setting" class="headerlink" title="a）进入Github Page仓库页面，点击Setting"></a>a）进入Github Page仓库页面，点击Setting</h4><p><img src="https://img-blog.csdnimg.cn/20200304231309815.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="Setting"></p>
<h4 id="b）在其中找到Github-Page，通过编辑Custom-domain并保存来实现Github-Page发布"><a href="#b）在其中找到Github-Page，通过编辑Custom-domain并保存来实现Github-Page发布" class="headerlink" title="b）在其中找到Github Page，通过编辑Custom domain并保存来实现Github Page发布"></a>b）在其中找到Github Page，通过编辑<code>Custom domain</code>并保存来实现Github Page发布</h4><p><img src="https://img-blog.csdnimg.cn/20200304231323491.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="发布GithubPage"></p>
<h4 id="同时会发现在仓库中多出了CNAME文件，后续如果想要取消发布直接删除此文件即可，同时要清除浏览器缓存，否则会一直访问之前设置的页面"><a href="#同时会发现在仓库中多出了CNAME文件，后续如果想要取消发布直接删除此文件即可，同时要清除浏览器缓存，否则会一直访问之前设置的页面" class="headerlink" title="同时会发现在仓库中多出了CNAME文件，后续如果想要取消发布直接删除此文件即可，同时要清除浏览器缓存，否则会一直访问之前设置的页面"></a>同时会发现在仓库中多出了CNAME文件，后续如果想要取消发布直接删除此文件即可，同时要清除浏览器缓存，否则会一直访问之前设置的页面</h4><p><img src="https://img-blog.csdnimg.cn/20200304231342835.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="CNAME文件"></p>
<h4 id="至此就实现了个人域名和映射和Github-Page的发布，通过之前设置的个人域名就能直接访问Github-Page"><a href="#至此就实现了个人域名和映射和Github-Page的发布，通过之前设置的个人域名就能直接访问Github-Page" class="headerlink" title="至此就实现了个人域名和映射和Github Page的发布，通过之前设置的个人域名就能直接访问Github Page"></a>至此就实现了个人域名和映射和Github Page的发布，通过之前设置的个人域名就能直接访问Github Page</h4><h3 id="4-在Hexo-source路径下创建CNAME文件"><a href="#4-在Hexo-source路径下创建CNAME文件" class="headerlink" title="4. 在Hexo/source路径下创建CNAME文件"></a>4. 在<code>Hexo/source</code>路径下创建CNAME文件</h3><p><strong>由于每次Deploy本地Hexo仓库时，都会清除GitHub上仓库中的CNAME文件，所以需要在Hexo根目录的source文件夹下创建CNAME文件，文件内容为个人域名，如：<code>blog.tomandersen.cn</code></strong></p>
<p><strong>这样每次部署时都会将CNMAE文件同时打包至远端Repository中，避免每次都要设置GitHub Page发布域名</strong></p>
<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>个人博客搭建</category>
      </categories>
      <tags>
        <tag>GitHub</tag>
        <tag>Blog</tag>
      </tags>
  </entry>
  <entry>
    <title>Shell中冒号的特殊用法</title>
    <url>/2020/02/27/Shell%E4%B8%AD%E5%86%92%E5%8F%B7%E7%9A%84%E7%89%B9%E6%AE%8A%E7%94%A8%E6%B3%95/</url>
    <content><![CDATA[<hr>
<a id="more"></a>

<hr>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul>
<li>本文主要是参考了鸟哥写的《鸟哥的Linux私房菜》。因为总是在各种脚本中见到<code>:-</code>的用法而只是了解其大概含义，所以翻阅了相关资料，编写此博文以作记录</li>
</ul>
<hr>
<h2 id="Shell中冒号在变量赋值时的各种特殊用法"><a href="#Shell中冒号在变量赋值时的各种特殊用法" class="headerlink" title="Shell中冒号在变量赋值时的各种特殊用法"></a>Shell中冒号在变量赋值时的各种特殊用法</h2><table>
<thead>
<tr>
<th align="center">变量设定方式</th>
<th align="center">str变量没有设定时</th>
<th align="center">str为空字符串时</th>
<th align="center">str已经设定为非空字符串时</th>
</tr>
</thead>
<tbody><tr>
<td align="center">var=${str-expr}</td>
<td align="center">var=expr</td>
<td align="center">var=””</td>
<td align="center">var=$str</td>
</tr>
<tr>
<td align="center">var=${str:-expr}</td>
<td align="center">var=expr</td>
<td align="center">var=expr</td>
<td align="center">var=$str</td>
</tr>
<tr>
<td align="center">var=${str+expr}</td>
<td align="center">var=””</td>
<td align="center">var=expr</td>
<td align="center">var=expr</td>
</tr>
<tr>
<td align="center">var=${str:+expr}</td>
<td align="center">var=””</td>
<td align="center">var=””</td>
<td align="center">var=expr</td>
</tr>
<tr>
<td align="center">var=${str=expr}</td>
<td align="center">str=expr<br/>var=expr</td>
<td align="center">str 不变<br/>var=””</td>
<td align="center">str 不变<br/>var=$str</td>
</tr>
<tr>
<td align="center">var=${str:=expr}</td>
<td align="center">str=expr<br/>var=expr</td>
<td align="center">str=expr<br/>var=expr</td>
<td align="center">str 不变<br/>var=$str</td>
</tr>
<tr>
<td align="center">var=${str?expr}</td>
<td align="center">expr 输出至 stderr</td>
<td align="center">var=””</td>
<td align="center">var=$str</td>
</tr>
<tr>
<td align="center">var=${str:?expr}</td>
<td align="center">expr 输出至 stderr</td>
<td align="center">expr 输出至 stderr</td>
<td align="center">var=$str</td>
</tr>
</tbody></table>
<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>CentOS7</tag>
        <tag>Linux</tag>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title>Shell中逻辑与 &amp;&amp; 与逻辑或 || 的使用</title>
    <url>/2020/02/26/Shell%E4%B8%AD%E9%80%BB%E8%BE%91%E4%B8%8E-%E4%B8%8E%E9%80%BB%E8%BE%91%E6%88%96-%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<hr>
<a id="more"></a>

<hr>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul>
<li>本文主要是参考了鸟哥写的《鸟哥的Linux私房菜》。由于看到别人脚本中有相关运用，因此翻阅了相关资料，编写此博文以作记录</li>
</ul>
<hr>
<h2 id="Shell中-amp-amp-与-的运行规则"><a href="#Shell中-amp-amp-与-的运行规则" class="headerlink" title="Shell中&amp;&amp;与||的运行规则"></a>Shell中&amp;&amp;与||的运行规则</h2><table>
<thead>
<tr>
<th align="center">指令</th>
<th>执行说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><strong>command1 &amp;&amp; command2</strong></td>
<td>若command1正确执行，即退出码为0（$?==0），则执行command2，整体退出码以command2执行结果为准；若command1执行错误，即退出码非0（$?!=0），则不执行command2，整体退出码为0</td>
</tr>
<tr>
<td align="center"><strong>command1 || command2</strong></td>
<td>若command1正确执行，即退出码为0（$?==0），则不执行command2，整体退出码为0；若command1执行错误，即退出码非0（$?!=0），则执行command2，整体退出码以command2执行结果为准</td>
</tr>
</tbody></table>
<hr>
<h2 id="推广"><a href="#推广" class="headerlink" title="推广"></a>推广</h2><table>
<thead>
<tr>
<th>指令</th>
<th>执行说明</th>
</tr>
</thead>
<tbody><tr>
<td><strong>command1 &amp;&amp; command2 || command3</strong></td>
<td>等价于(command1 &amp;&amp; command2 )|| command3，前面括号中的命令为一个整体，具体执行规则参考上表</td>
</tr>
<tr>
<td><strong>command1 || command2 &amp;&amp; command3</strong></td>
<td>等价于(command1 || command2) &amp;&amp; command3，前面括号中的命令为一个整体，具体执行规则参考上表</td>
</tr>
</tbody></table>
<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>CentOS7</tag>
        <tag>Linux</tag>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop之配置历史服务器并开启日志聚集</title>
    <url>/2020/02/26/Hadoop%E4%B9%8B%E9%85%8D%E7%BD%AE%E5%8E%86%E5%8F%B2%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%B9%B6%E5%BC%80%E5%90%AF%E6%97%A5%E5%BF%97%E8%81%9A%E9%9B%86/</url>
    <content><![CDATA[<hr>
<a id="more"></a>

<hr>
<h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h2><p><strong>Hadoop中设置任务历史服务器并开启日志聚集的必要性</strong>：一般情况下在YARN的Web UI中只能查看本次YARN运行期间执行的Application的首个Container的运行日志，即ApplicationMaster的运行日志（MR任务一般是分成多个task通过多个Container分别执行，这些Container分布在集群的任意主机上，首个Container用于运行ApplicationMaster，而AppMaster则负责分发调度task，以及启动其他Container），既不能查看历史job的日志，也不能查看程序输出的Counter统计信息（即命令行提交jar程序的最终执行输出，如job执行过程中的读写数据量、Map/Reduce task数量、各个阶段执行时间等一系列参数），所以需要开启jobhistory-server以及yarn-log-aggregation，即开启任务历史服务器以及日志聚集功能，来实现历史任务日志和作业运行日志，开启jobhistory-server之后，任务结束后运行日志会聚集到HDFS指定目录中，而不再保存本地文件系统中。</p>
<hr>
<h2 id="2-集群规划"><a href="#2-集群规划" class="headerlink" title="2. 集群规划"></a>2. 集群规划</h2><table>
<thead>
<tr>
<th align="center"></th>
<th align="center">hadoop101</th>
<th align="center">hadoop102</th>
<th align="center">hadoop103</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><strong>NameNode</strong></td>
<td align="center"><strong>√</strong></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center"><strong>DataNode</strong></td>
<td align="center"><strong>√</strong></td>
<td align="center"><strong>√</strong></td>
<td align="center"><strong>√</strong></td>
</tr>
<tr>
<td align="center"><strong>SecondaryNameNode</strong></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"><strong>√</strong></td>
</tr>
<tr>
<td align="center"><strong>ResourceManager</strong></td>
<td align="center"></td>
<td align="center"><strong>√</strong></td>
<td align="center"></td>
</tr>
<tr>
<td align="center"><strong>NodeManager</strong></td>
<td align="center"><strong>√</strong></td>
<td align="center"><strong>√</strong></td>
<td align="center"><strong>√</strong></td>
</tr>
<tr>
<td align="center"><strong>JobHistoryServer</strong></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"><strong>√</strong></td>
</tr>
</tbody></table>
<hr>
<h2 id="3-具体配置"><a href="#3-具体配置" class="headerlink" title="3. 具体配置"></a>3. 具体配置</h2><h3 id="a-core-site-xml"><a href="#a-core-site-xml" class="headerlink" title="a) core-site.xml"></a>a) core-site.xml</h3><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定HDFS文件系统访问地址,将其设置为NameNode的地址--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;hdfs:/</span><span class="regexp">/hadoop101:9000&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--指定Hadoop运行时产生文件的存储目录--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;property&gt;</span></span><br><span class="line"><span class="regexp">        &lt;name&gt;hadoop.tmp.dir&lt;/</span>name&gt;</span><br><span class="line">        &lt;value&gt;<span class="regexp">/opt/m</span>odule/hadoop<span class="number">-2.7</span><span class="number">.7</span>/tmp&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">&lt;<span class="regexp">/configuration&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="b-hdfs-site-xml"><a href="#b-hdfs-site-xml" class="headerlink" title="b) hdfs-site.xml"></a>b) hdfs-site.xml</h3><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定HDFS副本因子数--&gt;</span><br><span class="line">    &lt;!--由于实验主机磁盘空间不足,本次实验中设置为<span class="number">1</span>,一般需要设置为<span class="number">3</span>--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;1&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--以下是NameNode配置--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;!--指定NameNode节点的Web UI地址--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;property&gt;</span></span><br><span class="line"><span class="regexp">        &lt;name&gt;dfs.namenode.http-address&lt;/</span>name&gt;</span><br><span class="line">        &lt;value&gt;hadoop101:<span class="number">50070</span>&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定NameNode节点上存储name table(fsimage)文件的本地路径--&gt;</span><br><span class="line">    &lt;!--默认值:file:<span class="comment">//$&#123;hadoop.tmp.dir&#125;/dfs/name--&gt;</span></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.name.dir&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;file:/</span><span class="regexp">/$&#123;hadoop.tmp.dir&#125;/</span>dfs/namenode/fsimage&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定NameNode节点上存储transaction(edits)文件的本地路径--&gt;</span><br><span class="line">    &lt;!--默认值:$&#123;dfs.namenode.name.dir&#125;--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.edits.dir&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;file:/</span><span class="regexp">/$&#123;hadoop.tmp.dir&#125;/</span>dfs/namenode/edits&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定DataNode节点上存储Blocks文件的本地路径,此处为修改--&gt;</span><br><span class="line">    &lt;!--默认值:file:<span class="comment">//$&#123;hadoop.tmp.dir&#125;/dfs/data--&gt;</span></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.data.dir&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;file:/</span><span class="regexp">/$&#123;hadoop.tmp.dir&#125;/</span>dfs/datanode/data&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--以下是SecondaryNameNode配置--&gt;</span><br><span class="line">    &lt;!--指定NameNode辅助名称节点SecondaryNameNode的Web UI地址--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.secondary.http-address&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;hadoop103:50090&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--指定SecondaryNameNode节点上存储temporary images文件的本地路径--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;!--默认值:file:/</span><span class="regexp">/$&#123;hadoop.tmp.dir&#125;/</span>dfs/namesecondary--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.checkpoint.dir&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;file:/</span><span class="regexp">/$&#123;hadoop.tmp.dir&#125;/</span>dfs/namesecondary/fsimage&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定SecondaryNameNode节点上存储temporary edits文件的本地路径--&gt;</span><br><span class="line">    &lt;!--默认值:$&#123;dfs.namenode.checkpoint.dir&#125;--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.checkpoint.edits.dir&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;file:/</span><span class="regexp">/$&#123;hadoop.tmp.dir&#125;/</span>dfs/namesecondary/edits&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">&lt;<span class="regexp">/configuration&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="c-mapred-site-xml"><a href="#c-mapred-site-xml" class="headerlink" title="c) mapred-site.xml"></a>c) mapred-site.xml</h3><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定MR程序运行框架,设置为YARN上运行,默认是在本地运行--&gt;</span><br><span class="line">    &lt;!--默认值:local--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;yarn&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--指定历史服务器JobHistoryServer进程间通信IPC地址--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;property&gt;</span></span><br><span class="line"><span class="regexp">        &lt;name&gt;mapreduce.jobhistory.address&lt;/</span>name&gt;</span><br><span class="line">        &lt;value&gt;hadoop103:<span class="number">10020</span>&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定历史服务器JobHistoryServer的Web UI地址--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;hadoop103:19888&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp">    </span></span><br><span class="line"><span class="regexp">&lt;/</span>configuration&gt;</span><br></pre></td></tr></table></figure>

<h3 id="d-yarn-site-xml"><a href="#d-yarn-site-xml" class="headerlink" title="d) yarn-site.xml"></a>d) yarn-site.xml</h3><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--Site specific YARN configuration properties--&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--设置Reducer获取数据的方式--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;mapreduce_shuffle&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--指定YARN中ResourceManager的ip地址--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;property&gt;</span></span><br><span class="line"><span class="regexp">        &lt;name&gt;yarn.resourcemanager.hostname&lt;/</span>name&gt;</span><br><span class="line">        &lt;value&gt;hadoop102&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--此参数指的是nodemanager的可用内存大小,单位为Mb,设置为主机内存大小--&gt;</span><br><span class="line">    &lt;!--本次实验主机内存大小为<span class="number">2</span>GB,此参数根据各机器分配的物理内存大小设置,若大于物理内存值会影响程序运行效率--&gt;</span><br><span class="line">    &lt;!--默认值:<span class="number">8192</span>--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;2048&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--开启日志聚集功能--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;property&gt;</span></span><br><span class="line"><span class="regexp">        &lt;name&gt;yarn.log-aggregation-enable&lt;/</span>name&gt;</span><br><span class="line">        &lt;value&gt;<span class="literal">true</span>&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--日志聚集位置,默认为HDFS文件系统的/tmp/logs路径下,默认格式为/tmp/logs/$&#123;user&#125;/logs--&gt;</span><br><span class="line">    &lt;!--默认值:<span class="regexp">/tmp/</span>logs--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.remote-app-log-dir&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;/</span>tmp/logs&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--聚集日志保留时间设置<span class="number">7</span>天--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;604800&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">&lt;/</span>configuration&gt;</span><br></pre></td></tr></table></figure>



<hr>
<h2 id="4-启动HDFS和YARN集群"><a href="#4-启动HDFS和YARN集群" class="headerlink" title="4. 启动HDFS和YARN集群"></a>4. 启动HDFS和YARN集群</h2><h3 id="在hadoop101上启动HDFS集群："><a href="#在hadoop101上启动HDFS集群：" class="headerlink" title="在hadoop101上启动HDFS集群："></a>在hadoop101上启动HDFS集群：</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./sbin/start-dfs.sh</span><br></pre></td></tr></table></figure>

<h3 id="在hadoop102上启动YARN集群："><a href="#在hadoop102上启动YARN集群：" class="headerlink" title="在hadoop102上启动YARN集群："></a>在hadoop102上启动YARN集群：</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./sbin/start-yarn.sh</span><br></pre></td></tr></table></figure>

<h3 id="在hadoop103上启动JobHistoryServer："><a href="#在hadoop103上启动JobHistoryServer：" class="headerlink" title="在hadoop103上启动JobHistoryServer："></a>在hadoop103上启动JobHistoryServer：</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./sbin/mr-jobhistory-daemon.sh start historyserver</span><br></pre></td></tr></table></figure>

<h3 id="启动脚本："><a href="#启动脚本：" class="headerlink" title="启动脚本："></a>启动脚本：</h3><ul>
<li>最好是自己自定义启动脚本，这里提供个参考脚本<code>hadoop-ctl.sh</code>，操作参数为<code>start</code>和<code>stop</code></li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment"># 用于启动Hadoop集群,包括HDFS/YARN/JobHistoryServer</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 判断参数个数</span></span><br><span class="line"><span class="keyword">if</span> ((<span class="variable">$#</span> != 1)); <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> -e <span class="string">"\nWorng Parameter!"</span></span><br><span class="line">    <span class="built_in">exit</span> 1</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取当前时间(相对时间)</span></span><br><span class="line">start_time=$(date +%s)</span><br><span class="line"><span class="comment"># 获取操作方式</span></span><br><span class="line">operate=<span class="variable">$1</span></span><br><span class="line"><span class="comment"># 设定HDFS客户端,即NameNode节点地址</span></span><br><span class="line">HDFS_Client=<span class="string">"hadoop101"</span></span><br><span class="line"><span class="comment"># 设定YARN客户端,即ResourceManager节点地址</span></span><br><span class="line">YARN_Client=<span class="string">"hadoop102"</span></span><br><span class="line"><span class="comment"># 设置任务历史服务器客户端,即JobHistoryServer节点地址</span></span><br><span class="line">JobHistoryServer=<span class="string">"hadoop103"</span></span><br><span class="line"><span class="comment"># 指定启动用户</span></span><br><span class="line">user=<span class="string">"tomandersen"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$operate</span> <span class="keyword">in</span></span><br><span class="line">start)</span><br><span class="line">    <span class="comment"># 启动HDFS集群</span></span><br><span class="line">    <span class="built_in">echo</span> -e <span class="string">"\n----------Starting HDFS cluster----------"</span></span><br><span class="line">    ssh <span class="variable">$user</span>@<span class="variable">$HDFS_Client</span> <span class="string">"source /etc/profile;</span></span><br><span class="line"><span class="string">    HADOOP_HOME=<span class="variable">$&#123;HADOOP_HOME:-/opt/module/hadoop-2.7.7&#125;</span>;</span></span><br><span class="line"><span class="string">    <span class="variable">$HADOOP_HOME</span>/sbin/start-dfs.sh"</span></span><br><span class="line">    <span class="comment"># 启动YARN集群</span></span><br><span class="line">    <span class="built_in">echo</span> -e <span class="string">"\n----------Starting YARN cluster----------"</span></span><br><span class="line">    ssh <span class="variable">$user</span>@<span class="variable">$YARN_Client</span> <span class="string">"source /etc/profile;</span></span><br><span class="line"><span class="string">    HADOOP_HOME=<span class="variable">$&#123;HADOOP_HOME:-/opt/module/hadoop-2.7.7&#125;</span>;</span></span><br><span class="line"><span class="string">    <span class="variable">$HADOOP_HOME</span>/sbin/start-yarn.sh"</span></span><br><span class="line">    <span class="comment"># 启动历史服务器</span></span><br><span class="line">    <span class="built_in">echo</span> -e <span class="string">"\n----------Starting JobHistoryServer----------"</span></span><br><span class="line">    ssh <span class="variable">$user</span>@<span class="variable">$JobHistoryServer</span> <span class="string">"source /etc/profile;</span></span><br><span class="line"><span class="string">    HADOOP_HOME=<span class="variable">$&#123;HADOOP_HOME:-/opt/module/hadoop-2.7.7&#125;</span>;</span></span><br><span class="line"><span class="string">    <span class="variable">$HADOOP_HOME</span>/sbin/mr-jobhistory-daemon.sh start historyserver"</span></span><br><span class="line">    ;;</span><br><span class="line">stop)</span><br><span class="line">    <span class="comment"># 关闭HDFS集群</span></span><br><span class="line">    <span class="built_in">echo</span> -e <span class="string">"\n----------Stopping HDFS cluster----------"</span></span><br><span class="line">    ssh <span class="variable">$user</span>@<span class="variable">$HDFS_Client</span> <span class="string">"source /etc/profile;</span></span><br><span class="line"><span class="string">    HADOOP_HOME=<span class="variable">$&#123;HADOOP_HOME:-/opt/module/hadoop-2.7.7&#125;</span>;</span></span><br><span class="line"><span class="string">    <span class="variable">$HADOOP_HOME</span>/sbin/stop-dfs.sh"</span></span><br><span class="line">    <span class="comment"># 关闭YARN集群</span></span><br><span class="line">    <span class="built_in">echo</span> -e <span class="string">"\n----------Stopping YARN cluster----------"</span></span><br><span class="line">    ssh <span class="variable">$user</span>@<span class="variable">$YARN_Client</span> <span class="string">"source /etc/profile;</span></span><br><span class="line"><span class="string">    HADOOP_HOME=<span class="variable">$&#123;HADOOP_HOME:-/opt/module/hadoop-2.7.7&#125;</span>;</span></span><br><span class="line"><span class="string">    <span class="variable">$HADOOP_HOME</span>/sbin/stop-yarn.sh"</span></span><br><span class="line">    <span class="comment"># 关闭历史服务器</span></span><br><span class="line">    <span class="built_in">echo</span> -e <span class="string">"\n----------Stopping JobHistoryServer----------"</span></span><br><span class="line">    ssh <span class="variable">$user</span>@<span class="variable">$JobHistoryServer</span> <span class="string">"source /etc/profile;</span></span><br><span class="line"><span class="string">    HADOOP_HOME=<span class="variable">$&#123;HADOOP_HOME:-/opt/module/hadoop-2.7.7&#125;</span>;</span></span><br><span class="line"><span class="string">    <span class="variable">$HADOOP_HOME</span>/sbin/mr-jobhistory-daemon.sh stop historyserver"</span></span><br><span class="line">    ;;</span><br><span class="line">*)</span><br><span class="line">    <span class="built_in">echo</span> -e <span class="string">"\nWorng Parameter!"</span></span><br><span class="line">    <span class="built_in">exit</span> 1</span><br><span class="line">    ;;</span><br><span class="line"><span class="keyword">esac</span></span><br><span class="line"></span><br><span class="line">end_time=$(date +%s)</span><br><span class="line">execution_time=$((<span class="variable">$&#123;end_time&#125;</span> - <span class="variable">$&#123;start_time&#125;</span>))</span><br><span class="line"><span class="built_in">echo</span> -e <span class="string">"\n----------<span class="variable">$operate</span> Hadoop cluster takes <span class="variable">$&#123;execution_time&#125;</span> seconds----------\n"</span></span><br></pre></td></tr></table></figure>



<hr>
<h2 id="5-jps检查进程"><a href="#5-jps检查进程" class="headerlink" title="5. jps检查进程"></a>5. jps检查进程</h2><ul>
<li>本次配置的hadoop集群，启动后各主机进程情况如下所示：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop101 hadoop]$ call-cluster.sh jps</span><br><span class="line">----------hadoop103----------</span><br><span class="line">38738 SecondaryNameNode</span><br><span class="line">39077 Jps</span><br><span class="line">38825 NodeManager</span><br><span class="line">38622 DataNode</span><br><span class="line">38958 JobHistoryServer</span><br><span class="line">----------hadoop102----------</span><br><span class="line">44720 ResourceManager</span><br><span class="line">45175 Jps</span><br><span class="line">44570 DataNode</span><br><span class="line">44862 NodeManager</span><br><span class="line">----------hadoop101----------</span><br><span class="line">48291 NameNode</span><br><span class="line">48438 DataNode</span><br><span class="line">48694 NodeManager</span><br><span class="line">48856 Jps</span><br><span class="line"></span><br><span class="line">----------execute <span class="string">"jps"</span> <span class="keyword">in</span> cluster takes 3 seconds----------</span><br><span class="line"></span><br><span class="line">[tomandersen@hadoop101 hadoop]$ </span><br><span class="line">[tomandersen@hadoop101 hadoop]$</span><br></pre></td></tr></table></figure>



<h2 id="6-通过JobHistoryServer查看任务运行情况"><a href="#6-通过JobHistoryServer查看任务运行情况" class="headerlink" title="6. 通过JobHistoryServer查看任务运行情况"></a>6. 通过JobHistoryServer查看任务运行情况</h2><h3 id="a-查看JobHistoryServer的Web-UI："><a href="#a-查看JobHistoryServer的Web-UI：" class="headerlink" title="a) 查看JobHistoryServer的Web UI："></a>a) 查看JobHistoryServer的Web UI：</h3><p><img src="https://img-blog.csdnimg.cn/20200226222121569.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="查看JobHistoryServer的Web UI"></p>
<h3 id="b-查看Job运行情况："><a href="#b-查看Job运行情况：" class="headerlink" title="b) 查看Job运行情况："></a>b) 查看Job运行情况：</h3><p><img src="https://img-blog.csdnimg.cn/20200226222143202.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="查看Job运行情况"></p>
<p><img src="https://img-blog.csdnimg.cn/20200226222210552.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="查看Job运行情况"></p>
<h3 id="c-查看Job的运行日志"><a href="#c-查看Job的运行日志" class="headerlink" title="c) 查看Job的运行日志"></a>c) 查看Job的运行日志</h3><p><img src="https://img-blog.csdnimg.cn/20200226222232809.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="查看Job的运行日志"></p>
<p><img src="https://img-blog.csdnimg.cn/2020022622225166.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="查看Job的运行日志"></p>
<h3 id="d-查看Job的Counter统计信息"><a href="#d-查看Job的Counter统计信息" class="headerlink" title="d) 查看Job的Counter统计信息"></a>d) 查看Job的Counter统计信息</h3><p><img src="https://img-blog.csdnimg.cn/20200226222335229.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="查看Job的Counter统计信息"></p>
<p><img src="https://img-blog.csdnimg.cn/20200226222349289.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="查看Job的Counter统计信息"></p>
<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>CentOS7</tag>
        <tag>大数据</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop中HA模式配置（HDFS HA &amp; YARN HA）</title>
    <url>/2020/02/26/Hadoop%E4%B8%ADHA%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE%EF%BC%88HDFS-HA-YARN-HA%EF%BC%89/</url>
    <content><![CDATA[<hr>
<a id="more"></a>

<hr>
<h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1.前言"></a>1.前言</h2><ul>
<li>操作系统：CentOS 7</li>
<li>Hadoop版本：2.7.7</li>
<li>Zookeeper版本：3.4.14</li>
<li>Java版本：1.8.0_221</li>
<li>Hadoop HA模式分为HDFS HA（NameNode HA）和YARN HA（ResourceManager HA）两个部分</li>
<li>在本次配置中同时配置了HDFS HA和YARN HA下的自动故障转移Automatic Failover，以及历史服务器JobHistoryServer</li>
<li>所有配置均在同一用户下操作</li>
</ul>
<hr>
<h2 id="2-集群规划"><a href="#2-集群规划" class="headerlink" title="2.集群规划"></a>2.集群规划</h2><table>
<thead>
<tr>
<th align="left"></th>
<th align="center">hadoop101</th>
<th align="center">hadoop102</th>
<th align="center">hadoop103</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>NameNode</strong></td>
<td align="center"><strong>√</strong></td>
<td align="center"><strong>√</strong></td>
<td align="center"></td>
</tr>
<tr>
<td align="left"><strong>DataNode</strong></td>
<td align="center"><strong>√</strong></td>
<td align="center"><strong>√</strong></td>
<td align="center"><strong>√</strong></td>
</tr>
<tr>
<td align="left"><strong>ResourceManager</strong></td>
<td align="center"><strong>√</strong></td>
<td align="center"><strong>√</strong></td>
<td align="center"></td>
</tr>
<tr>
<td align="left"><strong>NodeManager</strong></td>
<td align="center"><strong>√</strong></td>
<td align="center"><strong>√</strong></td>
<td align="center"><strong>√</strong></td>
</tr>
<tr>
<td align="left"><strong>JobHistoryServer</strong></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"><strong>√</strong></td>
</tr>
<tr>
<td align="left"><strong>JournalNode</strong></td>
<td align="center"><strong>√</strong></td>
<td align="center"><strong>√</strong></td>
<td align="center"><strong>√</strong></td>
</tr>
<tr>
<td align="left"><strong>DFSZKFailoverController</strong></td>
<td align="center"><strong>√</strong></td>
<td align="center"><strong>√</strong></td>
<td align="center"></td>
</tr>
<tr>
<td align="left"><strong>Zookeeper</strong></td>
<td align="center"><strong>√</strong></td>
<td align="center"><strong>√</strong></td>
<td align="center"><strong>√</strong></td>
</tr>
</tbody></table>
<hr>
<h2 id="3-具体配置"><a href="#3-具体配置" class="headerlink" title="3.具体配置"></a>3.具体配置</h2><h3 id="a-core-site-xml"><a href="#a-core-site-xml" class="headerlink" title="a) core-site.xml"></a>a) core-site.xml</h3><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定HDFS文件系统访问地址,将其设置为NameNode的地址--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;hdfs:/</span><span class="regexp">/dfsHAcluster&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--指定Hadoop运行时产生文件的存储目录--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;property&gt;</span></span><br><span class="line"><span class="regexp">        &lt;name&gt;hadoop.tmp.dir&lt;/</span>name&gt;</span><br><span class="line">        &lt;value&gt;<span class="regexp">/opt/m</span>odule/HA/hadoop<span class="number">-2.7</span><span class="number">.7</span>/tmp&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定JournalNode在本地存储edits文件的绝对路径--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.journalnode.edits.dir&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;$&#123;hadoop.tmp.dir&#125;/</span>dfs/journalnode/localdata&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--为HDFS(NameNode) HA模式实现自动故障转移,设置Zookeeper服务器--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;ha.zookeeper.quorum&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;zkServer1:2181,zkServer2:2181,zkServer3:2181&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;    </span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">&lt;/</span>configuration&gt;</span><br></pre></td></tr></table></figure>



<h3 id="b-hdfs-site-xml"><a href="#b-hdfs-site-xml" class="headerlink" title="b) hdfs-site.xml"></a>b) hdfs-site.xml</h3><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定HDFS副本因子数--&gt;</span><br><span class="line">    &lt;!--由于实验主机磁盘空间不足,本次实验中设置为<span class="number">1</span>,一般需要设置为<span class="number">3</span>--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;1&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--以下是关于HDFS(NameNode) HA模式的配置--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;!--设置集群的nameservice name--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;property&gt;</span></span><br><span class="line"><span class="regexp">        &lt;name&gt;dfs.nameservices&lt;/</span>name&gt;</span><br><span class="line">        &lt;value&gt;dfsHAcluster&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--设置HDFS HA Cluster的节点--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.ha.namenodes.dfsHAcluster&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;nn1,nn2&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--关于HDFS HA集群nn1节点的配置--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;!--设置HDFS HA集群nn1节点的RPC地址--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;property&gt;</span></span><br><span class="line"><span class="regexp">        &lt;name&gt;dfs.namenode.rpc-address.dfsHAcluster.nn1&lt;/</span>name&gt;</span><br><span class="line">        &lt;value&gt;hadoop101:<span class="number">8020</span>&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--设置HDFS HA集群nn1节点的Web UI地址--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.http-address.dfsHAcluster.nn1&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;hadoop101:50070&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">        &lt;!--指定HDFS HA集群dfsHAcluster中nn1节点上存储name table(fsimage)文件的本地路径--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;property&gt;</span></span><br><span class="line"><span class="regexp">        &lt;name&gt;dfs.namenode.name.dir.dfsHAcluster.nn1&lt;/</span>name&gt;</span><br><span class="line">        &lt;value&gt;$&#123;hadoop.tmp.dir&#125;/dfs/dfsHAcluster/nn1/fsimage&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定HDFS HA集群dfsHAcluster中nn1节点上存储transaction(edits)文件的本地路径--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.edits.dir.dfsHAcluster.nn1&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;$&#123;hadoop.tmp.dir&#125;/</span>dfs/dfsHAcluster/nn1/edits&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定HDFS HA集群dfsHAcluster中nn2节点上存储Blocks文件的本地路径--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.data.dir.dfsHAcluster.nn1&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;$&#123;hadoop.tmp.dir&#125;/</span>dfs/dfsHAcluster/nn1/data&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--关于HDFS HA集群nn2节点的配置--&gt;</span><br><span class="line">    &lt;!--设置HDFS HA集群nn2节点的RPC地址--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.rpc-address.dfsHAcluster.nn2&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;hadoop102:8020&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--设置HDFS HA集群nn2节点的Web UI地址--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;property&gt;</span></span><br><span class="line"><span class="regexp">        &lt;name&gt;dfs.namenode.http-address.dfsHAcluster.nn2&lt;/</span>name&gt;</span><br><span class="line">        &lt;value&gt;hadoop102:<span class="number">50070</span>&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定HDFS HA集群dfsHAcluster中nn2节点上存储name table(fsimage)文件的本地路径--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.name.dir.dfsHAcluster.nn2&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;$&#123;hadoop.tmp.dir&#125;/</span>dfs/dfsHAcluster/nn2/fsimage&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定HDFS HA集群dfsHAcluster中nn2节点上存储transaction(edits)文件的本地路径--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.edits.dir.dfsHAcluster.nn2&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;$&#123;hadoop.tmp.dir&#125;/</span>dfs/dfsHAcluster/nn2/edits&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定HDFS HA集群dfsHAcluster中nn2节点上存储Blocks文件的本地路径--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.data.dir.dfsHAcluster.nn2&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;$&#123;hadoop.tmp.dir&#125;/</span>dfs/dfsHAcluster/nn2/data&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &lt;!--设置Active NameNode向StandBy NameNode共享edits文件的URI--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.shared.edits.dir&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;qjournal:/</span><span class="regexp">/zkServer1:8485;zkServer2:8485;zkServer3:8485/</span>dfsHAcluster&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--设置HDFS用于联络Active NameNode的Java <span class="class"><span class="keyword">class</span>--&gt;</span></span><br><span class="line"><span class="class">    &lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="class">        &lt;<span class="title">name</span>&gt;<span class="title">dfs</span>.<span class="title">client</span>.<span class="title">failover</span>.<span class="title">proxy</span>.<span class="title">provider</span>.<span class="title">dfsHAcluster</span>&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="class">        &lt;<span class="title">value</span>&gt;<span class="title">org</span>.<span class="title">apache</span>.<span class="title">hadoop</span>.<span class="title">hdfs</span>.<span class="title">server</span>.<span class="title">namenode</span>.<span class="title">ha</span>.<span class="title">ConfiguredFailoverProxyProvider</span>&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="class">    &lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    &lt;!--设置当进行故障转移<span class="title">failover</span>时通过何种方式隔离<span class="title">Active</span> <span class="title">NameNode</span>--&gt;</span></span><br><span class="line"><span class="class">    &lt;!--本次设置成使用<span class="title">ssh</span>隔离--&gt;</span></span><br><span class="line"><span class="class">    &lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="class">        &lt;<span class="title">name</span>&gt;<span class="title">dfs</span>.<span class="title">ha</span>.<span class="title">fencing</span>.<span class="title">methods</span>&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="class">        &lt;<span class="title">value</span>&gt;<span class="title">sshfence</span>&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="class">    &lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    &lt;!--设置<span class="title">ssh</span>隔离就必须设置成当前用户<span class="title">ssh</span>对其他<span class="title">NameNode</span>免密登录,</span></span><br><span class="line"><span class="class">    同时需要在此提供私钥路径--&gt;</span></span><br><span class="line"><span class="class">    &lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="class">        &lt;<span class="title">name</span>&gt;<span class="title">dfs</span>.<span class="title">ha</span>.<span class="title">fencing</span>.<span class="title">ssh</span>.<span class="title">private</span>-<span class="title">key</span>-<span class="title">files</span>&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="class">        &lt;<span class="title">value</span>&gt;/<span class="title">home</span>/<span class="title">TomAndersen</span>/.<span class="title">ssh</span>/<span class="title">id_rsa</span>&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="class">    &lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    &lt;!--设置<span class="title">ssh</span>连接超时时间--&gt;</span></span><br><span class="line"><span class="class">    &lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="class">        &lt;<span class="title">name</span>&gt;<span class="title">dfs</span>.<span class="title">ha</span>.<span class="title">fencing</span>.<span class="title">ssh</span>.<span class="title">connect</span>-<span class="title">timeout</span>&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="class">        &lt;<span class="title">value</span>&gt;30000&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="class">    &lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    &lt;!--以下通过<span class="title">Zookeeper</span>设置自动故障转移<span class="title">automatic</span> <span class="title">failover</span>--&gt;</span></span><br><span class="line"><span class="class">    &lt;!--设置开启故障自动转移--&gt;</span></span><br><span class="line"><span class="class">    &lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="class">        &lt;<span class="title">name</span>&gt;<span class="title">dfs</span>.<span class="title">ha</span>.<span class="title">automatic</span>-<span class="title">failover</span>.<span class="title">enabled</span>&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="class">        &lt;<span class="title">value</span>&gt;<span class="title">true</span>&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="class">    &lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">&lt;/<span class="title">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>



<h3 id="c-mapred-site-xml"><a href="#c-mapred-site-xml" class="headerlink" title="c) mapred-site.xml"></a>c) mapred-site.xml</h3><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定MR程序运行框架,设置为YARN上运行,默认是在本地运行--&gt;</span><br><span class="line">    &lt;!--默认值:local--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;yarn&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--指定历史服务器JobHistoryServer进程间通信IPC地址--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;property&gt;</span></span><br><span class="line"><span class="regexp">        &lt;name&gt;mapreduce.jobhistory.address&lt;/</span>name&gt;</span><br><span class="line">        &lt;value&gt;hadoop103:<span class="number">10020</span>&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定历史服务器JobHistoryServer的Web UI地址--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;hadoop103:19888&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp">    </span></span><br><span class="line"><span class="regexp">&lt;/</span>configuration&gt;</span><br></pre></td></tr></table></figure>



<h3 id="d-yarn-site-xml"><a href="#d-yarn-site-xml" class="headerlink" title="d) yarn-site.xml"></a>d) yarn-site.xml</h3><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--Site specific YARN configuration properties--&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--设置Reducer获取数据的方式--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;mapreduce_shuffle&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--此参数指的是nodemanager的可用内存大小,单位为Mb,设置为主机内存大小--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;!--本次实验主机内存大小为2GB,此参数根据各机器分配的物理内存大小设置,若大于物理内存值会影响程序运行效率--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;!--默认值:8192--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;property&gt;</span></span><br><span class="line"><span class="regexp">        &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/</span>name&gt;</span><br><span class="line">        &lt;value&gt;<span class="number">2048</span>&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--开启日志聚集功能--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.log-aggregation-enable&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;true&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--日志聚集位置,默认为HDFS文件系统的/</span>tmp/logs路径下,默认格式为/tmp/logs/$&#123;user&#125;/logs--&gt;</span><br><span class="line">    &lt;!--默认值:<span class="regexp">/tmp/</span>logs--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.remote-app-log-dir&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;/</span>tmp/logs&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--聚集日志保留时间设置<span class="number">7</span>天--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;604800&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--以下是关于ResourceMangaer HA模式的配置--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;!--设置开启YARN HA模式--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;property&gt;</span></span><br><span class="line"><span class="regexp">        &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/</span>name&gt;</span><br><span class="line">        &lt;value&gt;<span class="literal">true</span>&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--设置ResourceManager Cluster ID即RM集群名--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.cluster-id&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;RMcluster&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--设置RM集群中的RM节点ID--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;property&gt;</span></span><br><span class="line"><span class="regexp">        &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/</span>name&gt;</span><br><span class="line">        &lt;value&gt;rm1,rm2&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--以下是关于RM集群中rm1节点的配置--&gt;</span><br><span class="line">    &lt;!--指定RM集群中rm1节点的ip地址--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;hadoop102&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--指定RM集群中rm1节点的Web UI地址--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;property&gt;</span></span><br><span class="line"><span class="regexp">        &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/</span>name&gt;</span><br><span class="line">        &lt;value&gt;hadoop102:<span class="number">8088</span>&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--以下是关于RM集群中rm2节点的配置--&gt;</span><br><span class="line">    &lt;!--指定RM集群中rm2节点的ip地址--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;hadoop101&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--指定RM集群中rm2节点的Web UI地址--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;property&gt;</span></span><br><span class="line"><span class="regexp">        &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/</span>name&gt;</span><br><span class="line">        &lt;value&gt;hadoop101:<span class="number">8088</span>&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定RM集群使用的Zookeeper集群所提供的Client端口--&gt;</span><br><span class="line">    &lt;!--注意与Zookeeper集群中设置的客户端端口一致--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.zk-address&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;zkServer1:2181,zkServer2:2181,zkServer3:2181&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--启用RM自动恢复--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;!--需同时设置yarn.resourcemanager.store.class--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;property&gt;</span></span><br><span class="line"><span class="regexp">        &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/</span>name&gt;</span><br><span class="line">        &lt;value&gt;<span class="literal">true</span>&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定resourcemanager的状态信息存储在zookeeper集群的工具类--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.store.class&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">&lt;/</span>configuration&gt;</span><br></pre></td></tr></table></figure>





<hr>
<h2 id="4-启动HA集群"><a href="#4-启动HA集群" class="headerlink" title="4.启动HA集群"></a>4.启动HA集群</h2><h3 id="a-首次启动集群"><a href="#a-首次启动集群" class="headerlink" title="a) 首次启动集群"></a>a) 首次启动集群</h3><h4 id="注意："><a href="#注意：" class="headerlink" title="注意："></a>注意：</h4><ul>
<li><p><strong>首次启动一个新的Hadoop HA集群时需要对NameNodes进行格式化，若已经格式化则直接正常启动即可</strong></p>
</li>
<li><p><strong>若想重新格式化NameNodes，需要将所有NameNodes节点上hadoop中的<code>tmp</code>和<code>logs</code>文件夹都删除，同时最好也删除Zookeeper集群上的所有hadoop-ha相关节点</strong></p>
</li>
</ul>
<h4 id="格式化NameNodes步骤："><a href="#格式化NameNodes步骤：" class="headerlink" title="格式化NameNodes步骤："></a>格式化NameNodes步骤：</h4><h5 id="1-启动Zookeeper集群"><a href="#1-启动Zookeeper集群" class="headerlink" title="(1) 启动Zookeeper集群"></a>(1) 启动<code>Zookeeper</code>集群</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./bin/zkServer.sh</span><br></pre></td></tr></table></figure>

<h5 id="2-在HDFS集群中所有节点上启动JournalNode"><a href="#2-在HDFS集群中所有节点上启动JournalNode" class="headerlink" title="(2) 在HDFS集群中所有节点上启动JournalNode"></a>(2) 在HDFS集群中所有节点上启动<code>JournalNode</code></h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./sbin/hadoop-daemon.sh start journalnode</span><br></pre></td></tr></table></figure>

<h5 id="3-在首个NameNode节点上格式化NameNode（dfsHAcluster-nn1）"><a href="#3-在首个NameNode节点上格式化NameNode（dfsHAcluster-nn1）" class="headerlink" title="(3) 在首个NameNode节点上格式化NameNode（dfsHAcluster.nn1）"></a>(3) 在首个NameNode节点上格式化<code>NameNode</code>（dfsHAcluster.nn1）</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./bin/hdfs namenode -format</span><br></pre></td></tr></table></figure>

<h5 id="4-在此节点上格式化zkNode即创建zkNode节点（dfsHAcluster-nn1）"><a href="#4-在此节点上格式化zkNode即创建zkNode节点（dfsHAcluster-nn1）" class="headerlink" title="(4) 在此节点上格式化zkNode即创建zkNode节点（dfsHAcluster.nn1）"></a>(4) 在此节点上格式化zkNode即创建zkNode节点（dfsHAcluster.nn1）</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./bin/hdfs zkfc -formatZK</span><br></pre></td></tr></table></figure>

<h5 id="5-在此节点上启动首个NameNode进程（dfsHAcluster-nn1）"><a href="#5-在此节点上启动首个NameNode进程（dfsHAcluster-nn1）" class="headerlink" title="(5) 在此节点上启动首个NameNode进程（dfsHAcluster.nn1）"></a>(5) 在此节点上启动首个<code>NameNode</code>进程（dfsHAcluster.nn1）</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./sbin/hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure>

<h5 id="6-同时启动DFSZKFailoverController、DataNode进程（dfsHAcluster-nn1）"><a href="#6-同时启动DFSZKFailoverController、DataNode进程（dfsHAcluster-nn1）" class="headerlink" title="(6) 同时启动DFSZKFailoverController、DataNode进程（dfsHAcluster.nn1）"></a>(6) 同时启动<code>DFSZKFailoverController、DataNode</code>进程（dfsHAcluster.nn1）</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./sbin/hadoop-daemon.sh start zkfc</span><br><span class="line">./sbin/hadoop-daemon.sh start datanode</span><br></pre></td></tr></table></figure>

<h5 id="7-在其他所有NameNode节点上进行同步格式化"><a href="#7-在其他所有NameNode节点上进行同步格式化" class="headerlink" title="(7) 在其他所有NameNode节点上进行同步格式化"></a>(7) 在其他所有NameNode节点上进行同步格式化</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./bin/hdfs namenode -bootstrapStandby</span><br></pre></td></tr></table></figure>

<h5 id="8-同时在其他所有NameNode节点上启动NameNode、DFSZKFailoverController、DataNode进程"><a href="#8-同时在其他所有NameNode节点上启动NameNode、DFSZKFailoverController、DataNode进程" class="headerlink" title="(8) 同时在其他所有NameNode节点上启动NameNode、DFSZKFailoverController、DataNode进程"></a>(8) 同时在其他所有NameNode节点上启动<code>NameNode、DFSZKFailoverController、DataNode</code>进程</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./sbin/hadoop-daemon.sh start namenode</span><br><span class="line">./sbin/hadoop-daemon.sh start zkfc</span><br><span class="line">./sbin/hadoop-daemon.sh start datanode</span><br></pre></td></tr></table></figure>

<h5 id="9-启动RM集群：在YARN集群中所有RM节点上启动RM进程，第二个启动的RM为Active（可以不启动）"><a href="#9-启动RM集群：在YARN集群中所有RM节点上启动RM进程，第二个启动的RM为Active（可以不启动）" class="headerlink" title="(9) 启动RM集群：在YARN集群中所有RM节点上启动RM进程，第二个启动的RM为Active（可以不启动）"></a>(9) 启动RM集群：在YARN集群中所有RM节点上启动RM进程，第二个启动的RM为Active（可以不启动）</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./sbin/yarn-daemon.sh start resourcemanager</span><br></pre></td></tr></table></figure>

<h5 id="10-启动NM集群：在YARN集群中所有节点上启动NM进程（可以不启动）"><a href="#10-启动NM集群：在YARN集群中所有节点上启动NM进程（可以不启动）" class="headerlink" title="(10) 启动NM集群：在YARN集群中所有节点上启动NM进程（可以不启动）"></a>(10) 启动NM集群：在YARN集群中所有节点上启动NM进程（可以不启动）</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./sbin/yarn-daemon.sh start nodemanager</span><br></pre></td></tr></table></figure>

<h5 id="11-在历史服务器节点上，启动JobHistoryServer进程（可以不启动）"><a href="#11-在历史服务器节点上，启动JobHistoryServer进程（可以不启动）" class="headerlink" title="(11) 在历史服务器节点上，启动JobHistoryServer进程（可以不启动）"></a>(11) 在历史服务器节点上，启动<code>JobHistoryServer</code>进程（可以不启动）</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./sbin/mr-jobhistory-daemon.sh</span><br></pre></td></tr></table></figure>



<h4 id="正常情况下此时集群已经正常运行，各节点具体进程情况如下所示："><a href="#正常情况下此时集群已经正常运行，各节点具体进程情况如下所示：" class="headerlink" title="正常情况下此时集群已经正常运行，各节点具体进程情况如下所示："></a>正常情况下此时集群已经正常运行，各节点具体进程情况如下所示：</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop101 hadoop-2.7.7]$ call-cluster.sh jps</span><br><span class="line">----------hadoop103----------</span><br><span class="line">32688 Jps</span><br><span class="line">23553 QuorumPeerMain</span><br><span class="line">32418 JobHistoryServer</span><br><span class="line">32291 JournalNode</span><br><span class="line">32520 NodeManager</span><br><span class="line">32186 DataNode</span><br><span class="line">----------hadoop102----------</span><br><span class="line">36963 JournalNode</span><br><span class="line">37669 Jps</span><br><span class="line">36775 NameNode</span><br><span class="line">37209 ResourceManager</span><br><span class="line">36858 DataNode</span><br><span class="line">25612 QuorumPeerMain</span><br><span class="line">37308 NodeManager</span><br><span class="line">37102 DFSZKFailoverController</span><br><span class="line">----------hadoop101----------</span><br><span class="line">40129 NameNode</span><br><span class="line">40451 JournalNode</span><br><span class="line">41075 Jps</span><br><span class="line">40788 ResourceManager</span><br><span class="line">40246 DataNode</span><br><span class="line">40891 NodeManager</span><br><span class="line">40653 DFSZKFailoverController</span><br><span class="line">28798 QuorumPeerMain</span><br><span class="line"></span><br><span class="line">----------execute <span class="string">"jps"</span> <span class="keyword">in</span> cluster takes 4 seconds----------</span><br><span class="line"></span><br><span class="line">[tomandersen@hadoop101 hadoop-2.7.7]$ </span><br><span class="line">[tomandersen@hadoop101 hadoop-2.7.7]$</span><br></pre></td></tr></table></figure>



<h4 id="可以通过haadmin和rmadmin命令查看和更改HA节点状态，集群中只有一个NN和RM处于Active，其余皆为StandBy："><a href="#可以通过haadmin和rmadmin命令查看和更改HA节点状态，集群中只有一个NN和RM处于Active，其余皆为StandBy：" class="headerlink" title="可以通过haadmin和rmadmin命令查看和更改HA节点状态，集群中只有一个NN和RM处于Active，其余皆为StandBy："></a>可以通过<code>haadmin</code>和<code>rmadmin</code>命令查看和更改HA节点状态，集群中只有一个NN和RM处于Active，其余皆为StandBy：</h4><h5 id="1-haadmin-获取HDFS-HA模式中指定NN节点的状态，如："><a href="#1-haadmin-获取HDFS-HA模式中指定NN节点的状态，如：" class="headerlink" title="(1) haadmin 获取HDFS HA模式中指定NN节点的状态，如："></a>(1) haadmin 获取HDFS HA模式中指定NN节点的状态，如：</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/hdfs haadmin -getServiceState nn1</span><br></pre></td></tr></table></figure>

<h5 id="2-haadmin-强制修改HDFS-HA模式中指定NN节点的状态，如："><a href="#2-haadmin-强制修改HDFS-HA模式中指定NN节点的状态，如：" class="headerlink" title="(2) haadmin 强制修改HDFS HA模式中指定NN节点的状态，如："></a>(2) haadmin 强制修改HDFS HA模式中指定NN节点的状态，如：</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/hdfs haadmin -transitionToActive --forcemanual nn1</span><br></pre></td></tr></table></figure>

<h5 id="3-rmadmin-获取YARN-HA模式中指定节点的状态，如："><a href="#3-rmadmin-获取YARN-HA模式中指定节点的状态，如：" class="headerlink" title="(3) rmadmin 获取YARN HA模式中指定节点的状态，如："></a>(3) rmadmin 获取YARN HA模式中指定节点的状态，如：</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/yarn rmadmin -getServiceState rm1</span><br></pre></td></tr></table></figure>

<h5 id="4-rmadmin-强制修改YARN-HA模式中指定节点的状态，如："><a href="#4-rmadmin-强制修改YARN-HA模式中指定节点的状态，如：" class="headerlink" title="(4) rmadmin 强制修改YARN HA模式中指定节点的状态，如："></a>(4) rmadmin 强制修改YARN HA模式中指定节点的状态，如：</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/yarn rmadmin -transitionToStandby --forcemanual rm2</span><br></pre></td></tr></table></figure>



<h4 id="若要关闭HDFS集群，直接使用对应命令关闭即可："><a href="#若要关闭HDFS集群，直接使用对应命令关闭即可：" class="headerlink" title="若要关闭HDFS集群，直接使用对应命令关闭即可："></a>若要关闭HDFS集群，直接使用对应命令关闭即可：</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./sbin/stop-dfs.sh</span><br></pre></td></tr></table></figure>



<h4 id="若要关闭YARN集群，除了在其中的一个RM节点上执行关闭YARN命令之外还需要在其他节点上手动关闭RM进程："><a href="#若要关闭YARN集群，除了在其中的一个RM节点上执行关闭YARN命令之外还需要在其他节点上手动关闭RM进程：" class="headerlink" title="若要关闭YARN集群，除了在其中的一个RM节点上执行关闭YARN命令之外还需要在其他节点上手动关闭RM进程："></a>若要关闭YARN集群，除了在其中的一个RM节点上执行关闭YARN命令之外还需要在其他节点上手动关闭RM进程：</h4><h5 id="关闭YARN集群："><a href="#关闭YARN集群：" class="headerlink" title="关闭YARN集群："></a>关闭YARN集群：</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./sbin/stop-yarn.sh</span><br></pre></td></tr></table></figure>

<h5 id="在其他节点上关闭RM："><a href="#在其他节点上关闭RM：" class="headerlink" title="在其他节点上关闭RM："></a>在其他节点上关闭RM：</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./sbin/yarn-daemon.sh stop resourcemanager</span><br></pre></td></tr></table></figure>



<h3 id="b-正常启动集群"><a href="#b-正常启动集群" class="headerlink" title="b) 正常启动集群"></a>b) 正常启动集群</h3><h4 id="启动HDFS-HA集群："><a href="#启动HDFS-HA集群：" class="headerlink" title="启动HDFS HA集群："></a>启动HDFS HA集群：</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./sbin/start-dfs.sh</span><br></pre></td></tr></table></figure>

<h4 id="启动YARN-HA集群："><a href="#启动YARN-HA集群：" class="headerlink" title="启动YARN HA集群："></a>启动YARN HA集群：</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./sbin/start-yarn.sh</span><br></pre></td></tr></table></figure>

<h4 id="同时并在其他RM节点上手动启动RM进程进而开启RM-HA，第二个启动的RM为Active："><a href="#同时并在其他RM节点上手动启动RM进程进而开启RM-HA，第二个启动的RM为Active：" class="headerlink" title="同时并在其他RM节点上手动启动RM进程进而开启RM HA，第二个启动的RM为Active："></a>同时并在其他RM节点上手动启动RM进程进而开启RM HA，第二个启动的RM为Active：</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./sbin/yarn-daemon.sh start resourcemanager</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>CentOS7</tag>
        <tag>大数据</tag>
        <tag>Hadoop</tag>
        <tag>HA</tag>
      </tags>
  </entry>
  <entry>
    <title>Zookeeper使用ssh远程启动脚本失败的解决方案</title>
    <url>/2020/02/20/Zookeeper%E4%BD%BF%E7%94%A8ssh%E8%BF%9C%E7%A8%8B%E5%90%AF%E5%8A%A8%E8%84%9A%E6%9C%AC%E5%A4%B1%E8%B4%A5%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/</url>
    <content><![CDATA[<hr>
<a id="more"></a>

<hr>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul>
<li><strong>本文主要记录一次解决问题的经历</strong></li>
</ul>
<hr>
<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><ul>
<li>在编写zookeeper群起脚本时，想要基于ssh命令来启动集群中所有zookeeper服务器节点。但是在使用ssh远程执行远端脚本时，控制台输出显示远端脚本已经正常运行结束，远端zookeeper进程实际上却未能运行。使用的ssh命令如下：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop101 bin]$ ssh hadoop102 <span class="string">"<span class="variable">$ZOOKEEPER_HOME</span>/bin/zkServer.sh start"</span></span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.4.14/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="解决过程"><a href="#解决过程" class="headerlink" title="解决过程"></a>解决过程</h2><h3 id="1）查看远端主机的运行日志zookeeper-out："><a href="#1）查看远端主机的运行日志zookeeper-out：" class="headerlink" title="1）查看远端主机的运行日志zookeeper.out："></a>1）查看远端主机的运行日志zookeeper.out：</h3><ul>
<li>zookeeper.out默认输出在启动zkServer.sh脚本的当前路径下，查看其中内容，如下所示：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop102 runtime]$ cat zookeeper.out </span><br><span class="line">nohup: 无法运行命令<span class="string">"java"</span>: 没有那个文件或目录</span><br></pre></td></tr></table></figure>

<ul>
<li>发现脚本中无法找到<code>java</code>命令（猜测可能是环境变量的问题）</li>
</ul>
<h3 id="2）尝试在脚本zkServer-sh中定位java命令使用位置："><a href="#2）尝试在脚本zkServer-sh中定位java命令使用位置：" class="headerlink" title="2）尝试在脚本zkServer.sh中定位java命令使用位置："></a>2）尝试在脚本zkServer.sh中定位java命令使用位置：</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop102 bin]$ cat zkServer.sh | grep java -n</span><br><span class="line">20:<span class="comment"># it should be linked to and not copied. Things like java jar files are found</span></span><br><span class="line">39:<span class="comment"># http://java.sun.com/javase/6/docs/technotes/guides/management/agent.html</span></span><br></pre></td></tr></table></figure>

<ul>
<li>但发现其中都是注释，并没有显示调用java命令。然后查看zkServer.sh中的内容，发现此脚本在开头还使用<code>. zkEnv.sh</code>的方式运行了zkEnv.sh脚本，因此我们再去此脚本中定位java命令</li>
</ul>
<h3 id="3）尝试在脚本zkEnv-sh中定位java命令使用位置："><a href="#3）尝试在脚本zkEnv-sh中定位java命令使用位置：" class="headerlink" title="3）尝试在脚本zkEnv.sh中定位java命令使用位置："></a>3）尝试在脚本zkEnv.sh中定位java命令使用位置：</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop102 bin]$ cat zkEnv.sh | grep java -n</span><br><span class="line">49:<span class="keyword">if</span> [ -f <span class="string">"<span class="variable">$ZOOCFGDIR</span>/java.env"</span> ]</span><br><span class="line">51:    . <span class="string">"<span class="variable">$ZOOCFGDIR</span>/java.env"</span></span><br><span class="line">69:  JAVA=<span class="string">"<span class="variable">$JAVA_HOME</span>/bin/java"</span></span><br><span class="line">71:  JAVA=java</span><br></pre></td></tr></table></figure>

<ul>
<li>查看具体程序段：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">68 <span class="keyword">if</span> [ <span class="string">"<span class="variable">$JAVA_HOME</span>"</span> != <span class="string">""</span> ]; <span class="keyword">then</span></span><br><span class="line">69   JAVA=<span class="string">"<span class="variable">$JAVA_HOME</span>/bin/java"</span></span><br><span class="line">70 <span class="keyword">else</span></span><br><span class="line">71   JAVA=java</span><br><span class="line">72 <span class="keyword">fi</span></span><br></pre></td></tr></table></figure>

<ul>
<li>结果发现此段程序并没有什么异常，于是尝试验证之前的推测，可能是环境变量导致此次错误</li>
</ul>
<h3 id="4）查看远端环境变量主机设置："><a href="#4）查看远端环境变量主机设置：" class="headerlink" title="4）查看远端环境变量主机设置："></a>4）查看远端环境变量主机设置：</h3><ul>
<li>使用ssh登录远端主机后，查看Java环境变量：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop102 bin]$ <span class="built_in">echo</span> <span class="variable">$JAVA_HOME</span></span><br><span class="line">/opt/module/jdk1.8.0_221</span><br><span class="line">[tomandersen@hadoop102 bin]$ <span class="built_in">which</span> java</span><br><span class="line">/opt/module/jdk1.8.0_221/bin/java</span><br><span class="line">[tomandersen@hadoop102 bin]$ <span class="built_in">echo</span> <span class="variable">$PATH</span> | grep <span class="variable">$JAVA_HOME</span></span><br><span class="line">/usr/<span class="built_in">local</span>/bin:/usr/bin:/usr/<span class="built_in">local</span>/sbin:/usr/sbin:/opt/module/jdk1.8.0_221/bin:/opt/module/hadoop-2.7.7/bin:/opt/module/hadoop-2.7.7/sbin:/opt/module/zookeeper-3.4.14/bin:/home/TomAndersen/.<span class="built_in">local</span>/bin:/home/TomAndersen/bin</span><br></pre></td></tr></table></figure>

<ul>
<li>结果发现环境变量配置正常，然后尝试使用ssh命令远程调用Java环境变量，查看是否能正常输出：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop101 bin]$ ssh hadoop102 <span class="string">"echo <span class="variable">$JAVA_HOME</span>"</span></span><br><span class="line">/opt/module/jdk1.8.0_221</span><br><span class="line">[tomandersen@hadoop101 bin]$ ssh hadoop102 <span class="string">"which java"</span></span><br><span class="line"><span class="built_in">which</span>: no java <span class="keyword">in</span> (/usr/<span class="built_in">local</span>/bin:/usr/bin)</span><br><span class="line">[tomandersen@hadoop101 bin]$ ssh hadoop102 <span class="string">"echo <span class="variable">$PATH</span>"</span></span><br><span class="line">/usr/<span class="built_in">local</span>/bin:/usr/bin:/usr/<span class="built_in">local</span>/sbin:/usr/sbin:/opt/module/jdk1.8.0_221/bin:/opt/module/hadoop-2.7.7/bin:/opt/module/hadoop-2.7.7/sbin:/opt/module/zookeeper-3.4.14/bin:/home/TomAndersen/.<span class="built_in">local</span>/bin:/home/TomAndersen/bin</span><br></pre></td></tr></table></figure>

<ul>
<li>显然使用ssh远程执行Shell命令能够读取环境变量，且远端主机各个环境变量设置正常，但是却无法使用java命令</li>
<li>于是猜测是不是使用ssh工具执行远端脚本时，没有加载环境变量。</li>
</ul>
<h3 id="5）验证猜想"><a href="#5）验证猜想" class="headerlink" title="5）验证猜想"></a>5）验证猜想</h3><ul>
<li>接下来在远端主机上创建测试脚本<code>test.sh</code>，用于测试是否使用ssh工具远端执行此脚本时无法读取环境变量，其中测试脚本内容如下：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">1 <span class="built_in">echo</span> <span class="variable">$PATH</span></span><br><span class="line">2 <span class="built_in">echo</span> <span class="variable">$JAVA_HOME</span></span><br><span class="line">3 <span class="built_in">which</span> java</span><br></pre></td></tr></table></figure>

<ul>
<li>使用ssh远程执行此脚本：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop101 ~]$ ssh hadoop102 <span class="string">"~/test.sh"</span></span><br><span class="line">/usr/<span class="built_in">local</span>/bin:/usr/bin</span><br><span class="line"></span><br><span class="line"><span class="built_in">which</span>: no java <span class="keyword">in</span> (/usr/<span class="built_in">local</span>/bin:/usr/bin)</span><br></pre></td></tr></table></figure>

<ul>
<li>至此我们可以得出结果，使用ssh执行远端脚本时，在脚本内是没有加载环境变量的，即没有加载<code>/etc/profile</code>文件，因此我们修改原始ssh远程执行命令，在前面加上<code>source /etc/profile</code>，即主动加载系统环境变量，然后发现脚本能够正常调用环境变量：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop101 ~]$ ssh hadoop102 <span class="string">"source /etc/profile;~/test.sh"</span></span><br><span class="line">/usr/<span class="built_in">local</span>/bin:/usr/bin:/usr/<span class="built_in">local</span>/sbin:/usr/sbin:/opt/module/jdk1.8.0_221/bin:/opt/module/hadoop-2.7.7/bin:/opt/module/hadoop-2.7.7/sbin:/opt/module/zookeeper-3.4.14/bin</span><br><span class="line">/opt/module/jdk1.8.0_221</span><br><span class="line">/opt/module/jdk1.8.0_221/bin/java</span><br></pre></td></tr></table></figure>

<ul>
<li>至此问题解决</li>
</ul>
<hr>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><h3 id="①-在ssh远程执行的正式命令之前加上source-etc-profile-，即主动加载环境变量"><a href="#①-在ssh远程执行的正式命令之前加上source-etc-profile-，即主动加载环境变量" class="headerlink" title="① 在ssh远程执行的正式命令之前加上source /etc/profile;，即主动加载环境变量"></a>① 在ssh远程执行的正式命令之前加上<code>source /etc/profile;</code>，即主动加载环境变量</h3><h3 id="②-同理可以在对应用户的-bashrc文件末尾加入source-etc-profile，也是同样的效果"><a href="#②-同理可以在对应用户的-bashrc文件末尾加入source-etc-profile，也是同样的效果" class="headerlink" title="② 同理可以在对应用户的.bashrc文件末尾加入source /etc/profile，也是同样的效果"></a>② 同理可以在对应用户的.bashrc文件末尾加入<code>source /etc/profile</code>，也是同样的效果</h3><hr>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="http://feihu.me/blog/2014/env-problem-when-ssh-executing-command-on-remote/" target="_blank" rel="noopener">ssh连接远程主机执行脚本的环境变量问题</a></p>
<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>CentOS7</tag>
        <tag>大数据</tag>
        <tag>ssh</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title>Zookeeper分布式安装配置过程</title>
    <url>/2020/02/20/Zookeeper%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%E8%BF%87%E7%A8%8B/</url>
    <content><![CDATA[<hr>
<a id="more"></a>

<hr>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul>
<li>集群主机：hadoop101、hadoop102、hadoop103</li>
<li>操作系统：CentOS 7</li>
<li>Zookeeper版本：3.4.14</li>
<li>JDK版本：1.8.0_221</li>
<li>所有配置操作都必须在同一用户下</li>
</ul>
<hr>
<h2 id="1-下载Zookeeper"><a href="#1-下载Zookeeper" class="headerlink" title="1. 下载Zookeeper"></a>1. 下载Zookeeper</h2><h3 id="在Zookeeper官网提供的镜像网站上下载合适版本，本次下载的版本是3-4-14"><a href="#在Zookeeper官网提供的镜像网站上下载合适版本，本次下载的版本是3-4-14" class="headerlink" title="在Zookeeper官网提供的镜像网站上下载合适版本，本次下载的版本是3.4.14"></a>在<a href="http://zookeeper.apache.org/" target="_blank" rel="noopener">Zookeeper官网</a>提供的<a href="http://archive.apache.org/dist/zookeeper/" target="_blank" rel="noopener">镜像网站</a>上下载合适版本，本次下载的版本是3.4.14</h3><ul>
<li><img src="https://img-blog.csdnimg.cn/2020022016535789.png" alt="下载Zookeeper"></li>
</ul>
<ul>
<li><p><img src="https://img-blog.csdnimg.cn/20200220165429871.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="下载Zookeeper"></p>
</li>
<li><p><img src="https://img-blog.csdnimg.cn/20200220165542460.png" alt="下载Zookeeper"></p>
</li>
</ul>
<hr>
<h2 id="2-安装Zookeeper"><a href="#2-安装Zookeeper" class="headerlink" title="2. 安装Zookeeper"></a>2. 安装Zookeeper</h2><h3 id="直接使用普通用户将压缩包解压到目标路径下即可，本次解压路径为-opt-module-，具体命令如下："><a href="#直接使用普通用户将压缩包解压到目标路径下即可，本次解压路径为-opt-module-，具体命令如下：" class="headerlink" title="直接使用普通用户将压缩包解压到目标路径下即可，本次解压路径为/opt/module/，具体命令如下："></a>直接使用普通用户将压缩包解压到目标路径下即可，本次解压路径为<code>/opt/module/</code>，具体命令如下：</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop101 software]$ tar -xzvf zookeeper-3.4.14.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p><strong>注意：本次实验中此路径已经使用root用户将所有权给了用户tomandersen，默认是root用户才能访问，如果要使用相同路径，记得先排查确保文件所属用户，避免后续因为权限而出现混乱。为了避免麻烦，建议使用自定义路径</strong></p>
<hr>
<h2 id="3-配置conf-zoo-cfg"><a href="#3-配置conf-zoo-cfg" class="headerlink" title="3. 配置conf/zoo.cfg"></a>3. 配置conf/zoo.cfg</h2><h3 id="1）将conf文件夹下的zoo-sample-cfg文件更名为zoo-cfg："><a href="#1）将conf文件夹下的zoo-sample-cfg文件更名为zoo-cfg：" class="headerlink" title="1）将conf文件夹下的zoo_sample.cfg文件更名为zoo.cfg："></a>1）将<code>conf</code>文件夹下的<code>zoo_sample.cfg</code>文件更名为<code>zoo.cfg</code>：</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop101 software]$ mv zoo_sample.cfg zoo.cfg</span><br></pre></td></tr></table></figure>

<h3 id="2）然后编辑zoo-cfg，修改其中参数，具体参数可参考如下："><a href="#2）然后编辑zoo-cfg，修改其中参数，具体参数可参考如下：" class="headerlink" title="2）然后编辑zoo.cfg，修改其中参数，具体参数可参考如下："></a>2）然后编辑<code>zoo.cfg</code>，修改其中参数，具体参数可参考如下：</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tickTime=2000</span><br><span class="line">initLimit=10</span><br><span class="line">syncLimit=5</span><br><span class="line">dataDir=/opt/module/zookeeper-3.4.14/zkData</span><br><span class="line">dataLogDir=/opt/module/zookeeper-3.4.14/logs/transaction</span><br><span class="line">clientPort=2181</span><br><span class="line">autopurge.snapRetainCount=3</span><br><span class="line">autopurge.purgeInterval=1</span><br><span class="line"><span class="comment"># The zookeeper cluster setting</span></span><br><span class="line">server.1=hadoop101:2888:3888</span><br><span class="line">server.2=hadoop102:2888:3888</span><br><span class="line">server.3=hadoop103:2888:3888</span><br></pre></td></tr></table></figure>

<h3 id="其中需要特别说明的参数："><a href="#其中需要特别说明的参数：" class="headerlink" title="其中需要特别说明的参数："></a>其中需要特别说明的参数：</h3><ul>
<li><strong>dataLogDir</strong>：在默认的配置中并没有此参数的相关描述，但在官方文档中描述到，此参数用于设置 <strong>事务日志（transaction log）</strong> 存放路径，将<code>dataLogDir</code>和<code>dataDir</code>设置于不同存储设备对于Zookeeper的吞吐率有较大影响。所以这里即便是没有设置多余的设备用于存储日志，也建议将其分开存储</li>
<li><strong>autopurge.snapRetainCount</strong>和<strong>autopurge.purgeInterval</strong>：这两个参数配合起来就是在每间隔1个小时清洗一次快照snapshot和对应的事务日志transaction log，只保存最新的三个版本的快照和其对应的事务日志</li>
<li><strong>server.&lt;myid&gt;=&lt;host&gt;:2888:3888</strong>：此参数专门用于设置Zookeeper集群，其中<code>&lt;myid&gt;</code>指的是zookeeper服务器ID，主要通过<code>dataDir</code>路径下的<code>myid</code>文件来确定，此文件需要自己手动创建，文件中内容只能有对应服务器ID，不能有任何多余内容。<code>&lt;host&gt;</code>则指的是zookeeper服务器主机ip，由于此前已经在<code>/etc/hosts</code>中设置了映射，所有此处直接使用设置的主机名<code>hadoop101 hadoop102 hadoop103</code>，其中<code>2888</code>和<code>3888</code>分别指的是zookeeper集群仲裁模式（完全分布式）下<code>Leader</code>和<code>Follower</code>通信端口，以及zookeeper集群选取Leader时各个服务器之间的通信端口</li>
</ul>
<hr>
<h2 id="4-设置myid"><a href="#4-设置myid" class="headerlink" title="4. 设置myid"></a>4. 设置myid</h2><h3 id="官方文档原文："><a href="#官方文档原文：" class="headerlink" title="官方文档原文："></a>官方文档原文：</h3><ul>
<li>You attribute the server id to each machine by creating a file named <strong>myid</strong>, one for each server, which resides in that server’s data directory, as specified by the configuration file parameter <strong>dataDir</strong>.</li>
<li>The <strong>myid</strong> file consists of a single line containing only the text of that machine’s id. So <strong>myid of server 1 would contain the text “1” and nothing else</strong>. The id must be unique within the ensemble and should have a value <strong>between 1 and 255</strong>. <strong>IMPORTANT:</strong> if you enable extended features such as TTL Nodes (see below) the id must be between 1 and 254 due to internal limitations.</li>
</ul>
<h3 id="总而言之："><a href="#总而言之：" class="headerlink" title="总而言之："></a>总而言之：</h3><ul>
<li>文件<code>myid</code>必须在<code>dataDir</code>路径下，其中内容只能是单个整数，且<code>myid</code>的值必须与<code>zoo.cfg</code>中的配置相对应，<code>myid</code>的值必须在1~255之间，且唯一</li>
</ul>
<h3 id="1）在dataDir路径下创建myid："><a href="#1）在dataDir路径下创建myid：" class="headerlink" title="1）在dataDir路径下创建myid："></a>1）在dataDir路径下创建myid：</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop101 zookeeper-3.4.14]$ <span class="built_in">echo</span> 1 &gt; zkData/myid</span><br></pre></td></tr></table></figure>

<h3 id="2）将zookeeper文件夹分发给集群主机："><a href="#2）将zookeeper文件夹分发给集群主机：" class="headerlink" title="2）将zookeeper文件夹分发给集群主机："></a>2）将zookeeper文件夹分发给集群主机：</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop101 module]$ scp -r zookeeper-3.4.14/ hadoop102:/opt/module/</span><br><span class="line">[tomandersen@hadoop101 module]$ scp -r zookeeper-3.4.14/ hadoop103:/opt/module/</span><br></pre></td></tr></table></figure>

<p><strong>这里可以写个分发脚本，因为本次实验集群数量较少，所以直接手敲了</strong></p>
<h3 id="3）分别设置集群各个主机的myid："><a href="#3）分别设置集群各个主机的myid：" class="headerlink" title="3）分别设置集群各个主机的myid："></a>3）分别设置集群各个主机的myid：</h3><p><strong>这里就不再赘述了，设置方式和规则与前述相同</strong></p>
<hr>
<h2 id="5-配置群起脚本"><a href="#5-配置群起脚本" class="headerlink" title="5. 配置群起脚本"></a>5. 配置群起脚本</h2><h3 id="1）Zookeeper中没有设置群起脚本，需要自行编写。本次使用ssh工具来实现远程调用zkServer脚本启动Zookeeper集群。具体可参考如下："><a href="#1）Zookeeper中没有设置群起脚本，需要自行编写。本次使用ssh工具来实现远程调用zkServer脚本启动Zookeeper集群。具体可参考如下：" class="headerlink" title="1）Zookeeper中没有设置群起脚本，需要自行编写。本次使用ssh工具来实现远程调用zkServer脚本启动Zookeeper集群。具体可参考如下："></a>1）Zookeeper中没有设置群起脚本，需要自行编写。本次使用ssh工具来实现远程调用zkServer脚本启动Zookeeper集群。具体可参考如下：</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"> 1 <span class="comment">#!/bin/bash</span></span><br><span class="line"> 2 <span class="comment"># 此脚本用于启动zookeeper集群，使用方式和zkServer.sh相同</span></span><br><span class="line"> 3 </span><br><span class="line"> 4 <span class="comment"># 判断输入参数个数</span></span><br><span class="line"> 5 <span class="keyword">if</span> ((<span class="variable">$#</span> != 1)); <span class="keyword">then</span></span><br><span class="line"> 6     <span class="built_in">echo</span> <span class="string">"Wrong parameters!"</span></span><br><span class="line"> 7     <span class="built_in">exit</span> 1</span><br><span class="line"> 8 <span class="keyword">fi</span></span><br><span class="line"> 9 <span class="comment"># 获取当前用户</span></span><br><span class="line">10 user=$(whoami)</span><br><span class="line">11 <span class="comment"># 集群的ip地址</span></span><br><span class="line">12 cluster=<span class="string">"hadoop101 hadoop102 hadoop103"</span></span><br><span class="line">13 </span><br><span class="line">14 <span class="comment"># 根据输入参数调用对应功能</span></span><br><span class="line">15 <span class="keyword">case</span> <span class="variable">$1</span> <span class="keyword">in</span></span><br><span class="line">16 <span class="string">"start"</span>)</span><br><span class="line">17     <span class="keyword">for</span> host <span class="keyword">in</span> <span class="variable">$cluster</span>; <span class="keyword">do</span></span><br><span class="line">18         ssh <span class="variable">$user</span>@<span class="variable">$host</span> <span class="string">"source /etc/profile;<span class="variable">$ZOOKEEPER_HOME</span>/bin/zkServer.sh start"</span></span><br><span class="line">19     <span class="keyword">done</span></span><br><span class="line">20     ;;</span><br><span class="line">21 <span class="string">"status"</span>)</span><br><span class="line">22     <span class="keyword">for</span> host <span class="keyword">in</span> <span class="variable">$cluster</span>; <span class="keyword">do</span></span><br><span class="line">23         ssh <span class="variable">$user</span>@<span class="variable">$host</span> <span class="string">"source /etc/profile;<span class="variable">$ZOOKEEPER_HOME</span>/bin/zkServer.sh status"</span></span><br><span class="line">24     <span class="keyword">done</span></span><br><span class="line">25     ;;</span><br><span class="line">26 <span class="string">"stop"</span>)</span><br><span class="line">27     <span class="keyword">for</span> host <span class="keyword">in</span> <span class="variable">$cluster</span>; <span class="keyword">do</span></span><br><span class="line">28         ssh <span class="variable">$user</span>@<span class="variable">$host</span> <span class="string">"source /etc/profile;<span class="variable">$ZOOKEEPER_HOME</span>/bin/zkServer.sh stop"</span></span><br><span class="line">29     <span class="keyword">done</span></span><br><span class="line">30     ;;</span><br><span class="line">31 *)</span><br><span class="line">32     <span class="built_in">echo</span> <span class="string">"Worong parameter!"</span></span><br><span class="line">33     <span class="built_in">exit</span> 1</span><br><span class="line">34     ;;</span><br><span class="line">35 <span class="keyword">esac</span></span><br></pre></td></tr></table></figure>

<p><strong>注意：其中环境变量<code>$ZOOKEEPER_HOME</code>已经在<code>/etc/profile</code>中配置为zookeeper对应安装路径</strong></p>
<h3 id="2）运行群起脚本，测试集群是否正常运行"><a href="#2）运行群起脚本，测试集群是否正常运行" class="headerlink" title="2）运行群起脚本，测试集群是否正常运行"></a>2）运行群起脚本，测试集群是否正常运行</h3><ul>
<li><strong>启动集群</strong></li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop101 bin]$ zkCluster-server.sh start</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>查看集群状态</strong></li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop101 bin]$ zkCluster-server.sh status</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>关闭集群</strong></li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop101 bin]$ zkCluster-server.sh stop</span><br></pre></td></tr></table></figure>



<p><strong>如果启动失败，可以查看启动脚本的当前路径下的zookeeper.out文件，其中记录有错误原因。此文件为Zookeeper的运行日志。相关zookeeper日志介绍可以参考：<a href="https://blog.csdn.net/TomAndersen/article/details/104406465" target="_blank" rel="noopener">Zookeeper中日志文件种类</a></strong></p>
<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>CentOS7</tag>
        <tag>大数据</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title>Zookeeper中stat结构体参数介绍</title>
    <url>/2020/02/20/Zookeeper%E4%B8%ADstat%E7%BB%93%E6%9E%84%E4%BD%93%E5%8F%82%E6%95%B0%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<hr>
<a id="more"></a>

<hr>
<h2 id="stat结构体示例"><a href="#stat结构体示例" class="headerlink" title="stat结构体示例"></a>stat结构体示例</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] <span class="built_in">stat</span> /</span><br><span class="line">cZxid = 0x0</span><br><span class="line">ctime = Thu Jan 01 08:00:00 CST 1970</span><br><span class="line">mZxid = 0x0</span><br><span class="line">mtime = Thu Jan 01 08:00:00 CST 1970</span><br><span class="line">pZxid = 0x300000002</span><br><span class="line">cversion = 0</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 0</span><br><span class="line">numChildren = 2</span><br></pre></td></tr></table></figure>



<hr>
<h2 id="参数介绍"><a href="#参数介绍" class="headerlink" title="参数介绍"></a>参数介绍</h2><h3 id="1-cZxid"><a href="#1-cZxid" class="headerlink" title="1.  cZxid"></a>1.  cZxid</h3><ul>
<li><p>创建节点的事务zxid</p>
</li>
<li><p>每次修改zookeeper状态都会收到一个zxid形式的时间戳,也就是zookeeper事务ID。事务ID是zookeeper中所有修改总的次序。每个修改都有唯一的zxid,如果zxid1小于zxid,那么zxid在zxid2之前发生。</p>
</li>
</ul>
<h3 id="2-ctime"><a href="#2-ctime" class="headerlink" title="2.  ctime"></a>2.  ctime</h3><ul>
<li>znode被创建的毫秒数(从1970年开始)</li>
</ul>
<h3 id="3-mZxid"><a href="#3-mZxid" class="headerlink" title="3.  mZxid"></a>3.  mZxid</h3><ul>
<li>znode最后更新的事务zxid</li>
</ul>
<h3 id="4-mtime"><a href="#4-mtime" class="headerlink" title="4.  mtime"></a>4.  mtime</h3><ul>
<li>znode最后修改的毫秒数(从1970年开始)</li>
</ul>
<h3 id="5-pZxid"><a href="#5-pZxid" class="headerlink" title="5.  pZxid"></a>5.  pZxid</h3><ul>
<li>最后更新的子节点zxid</li>
</ul>
<h3 id="6-cversion"><a href="#6-cversion" class="headerlink" title="6.  cversion"></a>6.  cversion</h3><ul>
<li>znode子节点变化号,znode子节点修改次数</li>
</ul>
<h3 id="7-dataversion"><a href="#7-dataversion" class="headerlink" title="7.  dataversion"></a>7.  dataversion</h3><ul>
<li>znode数据变化号</li>
</ul>
<h3 id="8-aclVersion"><a href="#8-aclVersion" class="headerlink" title="8.  aclVersion"></a>8.  aclVersion</h3><ul>
<li>znode访问控制列表的变化号</li>
</ul>
<h3 id="9-ephemeralOwner"><a href="#9-ephemeralOwner" class="headerlink" title="9.  ephemeralOwner"></a>9.  ephemeralOwner</h3><ul>
<li>如果是临时节点,这个是znode拥有session id。如果不是临时节点则是0。</li>
</ul>
<h3 id="10-dataLength"><a href="#10-dataLength" class="headerlink" title="10.  dataLength"></a>10.  dataLength</h3><ul>
<li>znode的数据长度</li>
</ul>
<h3 id="11-numChildren"><a href="#11-numChildren" class="headerlink" title="11.  numChildren"></a>11.  numChildren</h3><ul>
<li>znode子节点数量</li>
</ul>
<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>CentOS7</tag>
        <tag>大数据</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title>Zookeeper中日志文件种类</title>
    <url>/2020/02/20/Zookeeper%E4%B8%AD%E6%97%A5%E5%BF%97%E6%96%87%E4%BB%B6%E7%A7%8D%E7%B1%BB/</url>
    <content><![CDATA[<hr>
<a id="more"></a>

<hr>
<h2 id="日志种类"><a href="#日志种类" class="headerlink" title="日志种类"></a>日志种类</h2><ul>
<li>Zookeeper在运行期间主要输出三类文件：<strong>快照（snapshot）</strong>、<strong>事务日志（transaction log）</strong>、<strong>运行日志</strong>。其中snapshot存放于<code>dataDir</code>中，事务日志在未设置<code>dataLogDir</code>参数时默认与snapshot存放路径相同，运行日志zookeeper.out默认存放在启动脚本的当前路径下</li>
<li>snapshot是zookeeper节点存储的数据的永久备份，而事务日志transaction log则是用于存储Znode的变化信息。当事务日志文件变得较大时，Zookeeper会将当前所有znode节点的最新状态生成快照snapshot存储到dataDir中，同时生成新的事务日志，用于接收最新的znodes变化。在生成新快照期间，也许会有新的事务被追加到旧的事务日志中，因此有些比快照更新的事务也许会存放在上一个版本的事务日志中</li>
<li>在Zookeeper的默认设置中，默认将snapshot和事务日志放置在同一个文件夹中，且默认会永久保存这两种文件。在Zookeeper 3.4.0之后可以通过设置<code>conf/zoo.cfg</code>中的<code>autopurge.snapRetainCount</code>和<code>autopurge.purgeInterval</code>参数来开启定时清理，每次保留固定数量的快照与其对应的事务日志</li>
<li>官方文档中建议将快照snapshot和事务日志transaction log存放于不同存储设备中，即单独设置<code>dataLogDir</code>存放事务日志，并且不与<code>dataDir</code>在同一设备下，因为是否设置独立存储设备用于存储日志，将会对Zookeeper吞吐量有较大影响</li>
<li>zookeeper.out文件在官方文档中并没有被称为运行日志，在此只是一种通俗的便于理解的称呼，zookeeper的默认存放路径在启动zkServer.sh脚本的当前路径下，这就导致每次启动zkServer.sh时都很难记住生成的zookeeper文件存放位置，因此建议修改其生成路径，便于每次出错后能快速定位错误</li>
<li>Zookeeper使用<code>SLF4J(Simple Logging Facade for Java)</code>作为其日志的基本框架（接口），为了向后兼容，Zookeeper绑定使用的是<code>LOG4J(Log for Java)</code>作为其具体的日志解决方案，log4j是Apache的开源项目之一。当然也可以在<code>SLF4J</code>的基础上使用其他的日志框架，这取决于你的具体应用</li>
</ul>
<hr>
<h2 id="独立存储事务日志"><a href="#独立存储事务日志" class="headerlink" title="独立存储事务日志"></a>独立存储事务日志</h2><ul>
<li><code>conf/zoo.cfg</code>的默认参数设置中没有<code>dataLogDir</code>参数，也没有相关介绍，但是在官方文档的Advanced Configuration部分有具体描述。具体设置可参考：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">15 <span class="comment"># 自定义Zookeeper集群数据本地存储路径</span></span><br><span class="line">16 dataDir=/opt/module/zookeeper-3.4.14/zkData</span><br><span class="line">17 <span class="comment"># 设置事务日志transaction log的存储路径,默认是dateDir,但是理应存储在不同设备下,避免对吞吐量造成负面影响</span></span><br><span class="line">18 dataLogDir=/opt/module/zookeeper-3.4.14/logs/transaction</span><br></pre></td></tr></table></figure>



<hr>
<h2 id="快照和事务日志定期清理"><a href="#快照和事务日志定期清理" class="headerlink" title="快照和事务日志定期清理"></a>快照和事务日志定期清理</h2><ul>
<li>在Zookeeper 3.4.0之后可以通过设置<code>conf/zoo.cfg</code>中的<code>autopurge.snapRetainCount</code>和<code>autopurge.purgeInterval</code>参数来开启定时清理，每次保留固定数量的快照与其对应的事务日志。具体设置可参考：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">34 <span class="comment"># The number of snapshots to retain in dataDir</span></span><br><span class="line">35 autopurge.snapRetainCount=3</span><br><span class="line">36 <span class="comment"># Purge task interval in hours</span></span><br><span class="line">37 <span class="comment"># Set to "0" to disable auto purge feature</span></span><br><span class="line">38 autopurge.purgeInterval=1</span><br></pre></td></tr></table></figure>



<hr>
<h2 id="事务日志可视化"><a href="#事务日志可视化" class="headerlink" title="事务日志可视化"></a>事务日志可视化</h2><ul>
<li>事务日志transaction log默认都是二进制文件，关于日志格式化输出，官方文档中也有相应介绍，基本都是基于zookeeper和slf4j提供的API实现二进制日志文件格式转换，命令使用格式为<code>java -cp zookeeper-3.4.14.jar:lib/slf4j-api-1.7.25.jar org.apache.zookeeper.server.LogFormatter  &lt;事务日志文件路径&gt;</code>，注意：其中所使用的包的版本需要根据自己的zookeeper调整，使用的LogFormatter类应该都是固定的。具体使用可参考：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop101 zookeeper-3.4.14]$ </span><br><span class="line">[tomandersen@hadoop101 zookeeper-3.4.14]$ </span><br><span class="line">[tomandersen@hadoop101 zookeeper-3.4.14]$ java -cp zookeeper-3.4.14.jar:lib/slf4j-api-1.7.25.jar  org.apache.zookeeper.server.LogFormatter   logs/transaction/version-2/log.300000001</span><br><span class="line">SLF4J: Failed to load class <span class="string">"org.slf4j.impl.StaticLoggerBinder"</span>.</span><br><span class="line">SLF4J: Defaulting to no-operation (NOP) logger implementation</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html<span class="comment">#StaticLoggerBinder for further details.</span></span><br><span class="line">ZooKeeper Transactional Log File with dbid 0 txnlog format version 2</span><br><span class="line">20-2-19 上午10时02分46秒 session 0x1000034fab40000 cxid 0x0 zxid 0x300000001 createSession 30000</span><br><span class="line"></span><br><span class="line">20-2-19 上午10时03分38秒 session 0x1000034fab40000 cxid 0x2 zxid 0x300000002 create <span class="string">'/TomAndersen,#5468697320697320546f6d416e64657273656e277320706572736f6e616c20736974652e,v&#123;s&#123;31,s&#123;'</span>world,<span class="string">'anyone&#125;&#125;&#125;,F,1</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">20-2-19 上午10时03分44秒 session 0x1000034fab40000 cxid 0x4 zxid 0x300000003 closeSession null</span></span><br><span class="line"><span class="string">EOF reached after 3 txns.</span></span><br><span class="line"><span class="string">[tomandersen@hadoop101 zookeeper-3.4.14]$ </span></span><br><span class="line"><span class="string">[tomandersen@hadoop101 zookeeper-3.4.14]$ </span></span><br><span class="line"><span class="string">[tomandersen@hadoop101 zookeeper-3.4.14]$</span></span><br></pre></td></tr></table></figure>



<hr>
<h2 id="修改运行日志输出路径"><a href="#修改运行日志输出路径" class="headerlink" title="修改运行日志输出路径"></a>修改运行日志输出路径</h2><ul>
<li>具体修改zookeeper.out的输出路径，可以参考：<a href="https://blog.csdn.net/TomAndersen/article/details/104405017" target="_blank" rel="noopener">Zookeeper修改运行日志zookeeper.out输出路径</a></li>
</ul>
<p><strong>以上是个人结合各种资料的理解，如果存在错误恳请各位留言指正，感谢！</strong></p>
<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>CentOS7</tag>
        <tag>大数据</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title>Zookeeper修改运行日志zookeeper.out输出路径</title>
    <url>/2020/02/20/Zookeeper%E4%BF%AE%E6%94%B9%E8%BF%90%E8%A1%8C%E6%97%A5%E5%BF%97zookeeper-out%E8%BE%93%E5%87%BA%E8%B7%AF%E5%BE%84/</url>
    <content><![CDATA[<hr>
<a id="more"></a>

<hr>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul>
<li>Zookeeper中运行日志 <strong>zookeeper.out</strong> 文件的输出路径默认为启动脚本的当前路径，导致Zookeeper集群启动失败时总是不记得输出日志在哪儿，不便于查找错误原因，因此很有必要设置固定路径来保存运行日志</li>
<li>在本次实验之前已经将 <strong>dataDir</strong> 和 <strong>dataLogDir</strong> 分别设置为<code>$ZOOKEEPER_HOME/zkData</code>和<code>$ZOOKEEPER_HOME/logs/transaction</code>，都是在<code>$ZOOKEEPER_HOME/conf/zoo.cfg</code>中配置。值得注意的是在此<code>zoo.cfg</code>文件中配置必须使用<strong>绝对路径</strong>，不能使用环境变量<code>$ZOOKEEPER_HOME</code>，此处是为了描述方便才使用此变量名</li>
<li>本次实验将把运行日志文件 <strong>zookeeper.out</strong> 输出路径指定为<code>$ZOOKEEPER_HOME/logs/runtime/</code>路径下</li>
</ul>
<hr>
<h2 id="简单配置"><a href="#简单配置" class="headerlink" title="简单配置"></a>简单配置</h2><h3 id="修改-ZOOKEEPER-HOME-bin-zkEnv-sh"><a href="#修改-ZOOKEEPER-HOME-bin-zkEnv-sh" class="headerlink" title="修改$ZOOKEEPER_HOME/bin/zkEnv.sh"></a>修改<code>$ZOOKEEPER_HOME/bin/zkEnv.sh</code></h3><ul>
<li>将<code>ZOO_LOG_DIR</code>设置成自定义路径，本次设置为<code>$ZOOBINDIR/../logs/runtime</code>，其中<code>ZOOBINDIR</code>变量是此脚本开头获取的Zookeeper的bin路径，我们直接以此来定位自己的日志路径即可。具体如下：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 修改前</span></span><br><span class="line"> 54 <span class="keyword">if</span> [ <span class="string">"x<span class="variable">$&#123;ZOO_LOG_DIR&#125;</span>"</span> = <span class="string">"x"</span> ]</span><br><span class="line"> 55 <span class="keyword">then</span></span><br><span class="line"> 56     ZOO_LOG_DIR=<span class="string">"."</span></span><br><span class="line"> 57 <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改后</span></span><br><span class="line"> 54 <span class="keyword">if</span> [ <span class="string">"x<span class="variable">$&#123;ZOO_LOG_DIR&#125;</span>"</span> = <span class="string">"x"</span> ]</span><br><span class="line"> 55 <span class="keyword">then</span></span><br><span class="line"> 56     <span class="comment">#ZOO_LOG_DIR="."</span></span><br><span class="line"> 57     <span class="comment">#自定义运行日志文件输出路径</span></span><br><span class="line"> 58     ZOO_LOG_DIR=<span class="string">"<span class="variable">$ZOOBINDIR</span>/../logs/runtime"</span></span><br><span class="line"> 59 <span class="keyword">fi</span></span><br></pre></td></tr></table></figure>



<ul>
<li>这样在每次使用<code>zkSever</code>的时候，都能将运行日志 <strong>zookeeper.out</strong> 输出到指定路径下，但这样配置有个问题，就是每次运行Zookeeper时，此日志都会被覆盖，而不是append到文件中，故每次运行结束后只会保存有本次运行日志，若单次运行时间很长也会导致日志文件也很大。</li>
</ul>
<hr>
<h2 id="进阶配置"><a href="#进阶配置" class="headerlink" title="进阶配置"></a>进阶配置</h2><ul>
<li>在之前的配置中，我们只是实现了保存本次Zookeeper运行日志在指定路径下，这次我们通过配置<code>$ZOOKEEPER_HOME/conf/log4j.properties</code>来使用log4j日志框架将Zookeeper每次的运行日志都保存到指定路径下</li>
</ul>
<h3 id="配置-ZOOKEEPER-HOME-conf-log4j-properties文件"><a href="#配置-ZOOKEEPER-HOME-conf-log4j-properties文件" class="headerlink" title="配置$ZOOKEEPER_HOME/conf/log4j.properties文件"></a>配置<code>$ZOOKEEPER_HOME/conf/log4j.properties</code>文件</h3><ul>
<li>修改其中的<code>zookeeper.root.logger zookeeper.log.dir log4j.appender.ROLLINGFILE.MaxBackupIndex</code>共三个参数。具体如下：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 修改前</span></span><br><span class="line">zookeeper.root.logger=INFO, CONSOLE</span><br><span class="line">...</span><br><span class="line">zookeeper.log.dir=.</span><br><span class="line">...</span><br><span class="line"><span class="comment">#log4j.appender.ROLLINGFILE.MaxBackupIndex=10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改后</span></span><br><span class="line">zookeeper.root.logger=INFO, CONSOLE, ROLLINGFILE</span><br><span class="line">zookeeper.log.dir=/opt/module/zookeeper-3.4.14/logs/runtime</span><br><span class="line">log4j.appender.ROLLINGFILE.MaxBackupIndex=5</span><br></pre></td></tr></table></figure>

<ul>
<li>其中参数<code>zookeeper.root.logger</code>是设置日志优先级和打印方式，默认为控制台打印<code>CONSOLE</code>，而在<code>zkServer.sh</code>中会启动后台命令，将控制台输出的日志输出重定向到 <strong>zookeeper.out</strong> 文件中。修改后添加了滚动产生文件输出方式<code>ROLLINGFILE</code>；通过参数<code>zookeeper.log.dir</code>设置日志文件 <strong>zookeeper.log</strong> 的存储路径，这里直接采用绝对路径，相对路径可能不识别；通过参数<code>log4j.appender.ROLLINGFILE.MaxBackupIndex</code>设置最大日志数量，每个日志文件大小最大默认为10MB，以此文件大小进行分割，默认日志文件名为 <strong>zookeeper.log</strong>，其中所有参数都可以自定义修改。其余参数不再赘述。</li>
</ul>
<h3 id="修改-ZOOKEEPER-HOME-bin-zkEnv-sh-1"><a href="#修改-ZOOKEEPER-HOME-bin-zkEnv-sh-1" class="headerlink" title="修改$ZOOKEEPER_HOME/bin/zkEnv.sh"></a>修改<code>$ZOOKEEPER_HOME/bin/zkEnv.sh</code></h3><ul>
<li>将其中的<code>ZOO_LOG4J_PROP</code>设置成与log4j配置文件中相同，避免参数覆盖。具体如下：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 修改前</span></span><br><span class="line"> 61 <span class="keyword">if</span> [ <span class="string">"x<span class="variable">$&#123;ZOO_LOG4J_PROP&#125;</span>"</span> = <span class="string">"x"</span> ]</span><br><span class="line"> 62 <span class="keyword">then</span></span><br><span class="line"> 63     ZOO_LOG4J_PROP=<span class="string">"INFO,CONSOLE"</span></span><br><span class="line"> 64 <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改后</span></span><br><span class="line"> 61 <span class="keyword">if</span> [ <span class="string">"x<span class="variable">$&#123;ZOO_LOG4J_PROP&#125;</span>"</span> = <span class="string">"x"</span> ]</span><br><span class="line"> 62 <span class="keyword">then</span></span><br><span class="line"> 63     <span class="comment">#ZOO_LOG4J_PROP="INFO,CONSOLE"</span></span><br><span class="line"> 64     <span class="comment">#自定义运行日志信息输出方式,增加了滚动输出的方式,初始只有控制台输出</span></span><br><span class="line"> 65     ZOO_LOG4J_PROP=<span class="string">"INFO,CONSOLE,ROLLINGFILE"</span></span><br><span class="line"> 66 <span class="keyword">fi</span></span><br></pre></td></tr></table></figure>

<h3 id="修改-ZOOKEEPER-HOME-bin-zkServer-sh"><a href="#修改-ZOOKEEPER-HOME-bin-zkServer-sh" class="headerlink" title="修改$ZOOKEEPER_HOME/bin/zkServer.sh"></a>修改<code>$ZOOKEEPER_HOME/bin/zkServer.sh</code></h3><ul>
<li>修改原始输出逻辑，不再将本次运行日志信息所有输出到 <strong>zookeeper.out</strong> 中，而是使用log4j框架输出到 <strong>zookeeper.log</strong> 中，便于管理。而 <strong>zookeeper.out</strong> 只用于输出标准错误。具体如下所示：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 修改前</span></span><br><span class="line">141     nohup <span class="string">"<span class="variable">$JAVA</span>"</span> <span class="string">"-Dzookeeper.log.dir=<span class="variable">$&#123;ZOO_LOG_DIR&#125;</span>"</span> <span class="string">"-Dzookeeper.root.logger=<span class="variable">$&#123;ZOO_LOG4J_PROP&#125;</span>"</span> \</span><br><span class="line">142     -cp <span class="string">"<span class="variable">$CLASSPATH</span>"</span> <span class="variable">$JVMFLAGS</span> <span class="variable">$ZOOMAIN</span> <span class="string">"<span class="variable">$ZOOCFG</span>"</span> &gt; <span class="string">"<span class="variable">$_ZOO_DAEMON_OUT</span>"</span> 2&gt;&amp;1 &lt; /dev/null &amp;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将原始指令注释,设置新的输出逻辑,只讲标准错误输出到zookeeper.out中.修改后:</span></span><br><span class="line">141 <span class="comment">#    nohup "$JAVA" "-Dzookeeper.log.dir=$&#123;ZOO_LOG_DIR&#125;" "-Dzookeeper.root.logger=$&#123;ZOO_LOG4J_PROP&#125;" \</span></span><br><span class="line">142 <span class="comment">#    -cp "$CLASSPATH" $JVMFLAGS $ZOOMAIN "$ZOOCFG" &gt; "$_ZOO_DAEMON_OUT" 2&gt;&amp;1 &lt; /dev/null &amp;</span></span><br><span class="line">143     nohup <span class="string">"<span class="variable">$JAVA</span>"</span> <span class="string">"-Dzookeeper.log.dir=<span class="variable">$&#123;ZOO_LOG_DIR&#125;</span>"</span> <span class="string">"-Dzookeeper.root.logger=<span class="variable">$&#123;ZOO_LOG4J_PROP&#125;</span>"</span> \</span><br><span class="line">144     -cp <span class="string">"<span class="variable">$CLASSPATH</span>"</span> <span class="variable">$JVMFLAGS</span> <span class="variable">$ZOOMAIN</span> <span class="string">"<span class="variable">$ZOOCFG</span>"</span> 2&gt; <span class="string">"<span class="variable">$_ZOO_DAEMON_OUT</span>"</span> 1&gt; /dev/null &amp;</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>这样就能保证历史运行日志都能输出到指定文件夹中，并且不会因为运行日志文件堆积造成负载点爆炸。而标准错误都会输出到 <strong>zookeeper.out</strong> 文件中，和运行日志放置在同一文件夹中。当需要查看脚本命令错误时，可以查看 <strong>zookeeper.out</strong> 文件，当需要查看程序错误时就查看 <strong>zookeeper.log</strong> 运行日志文件</li>
</ul>
<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>CentOS7</tag>
        <tag>大数据</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux中硬链接和软链接的区别和联系</title>
    <url>/2020/02/19/Linux%E4%B8%AD%E7%A1%AC%E9%93%BE%E6%8E%A5%E5%92%8C%E8%BD%AF%E9%93%BE%E6%8E%A5%E7%9A%84%E5%8C%BA%E5%88%AB%E5%92%8C%E8%81%94%E7%B3%BB/</url>
    <content><![CDATA[<hr>
<a id="more"></a>

<hr>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul>
<li><p>我们知道文件都有文件名与数据，这在 Linux 上被分成两个部分：<strong>用户数据 (user data)</strong> 与<strong>元数据 (metadata)</strong>。用户数据，即<strong>文件数据块 (data block)</strong>，数据块是记录文件真实内容的地方；而元数据则是文件的附加属性，如文件大小、创建时间、所有者等信息。</p>
</li>
<li><p>在 Linux 中，元数据中的 <strong>inode 号</strong>（inode 是文件元数据的一部分但其并不包含文件名，inode 号即索引节点号）才是文件的唯一标识而非文件名。文件名仅是为了方便人们的记忆和使用，系统或程序通过 inode 号寻找正确的文件数据块。在 Linux 系统中查看 inode 号可使用命令 <code>stat</code> 或<code>ls -i</code>（若是 AIX 系统，则使用命令<code>istat</code>）。当移动或者重命名文件时，并不影响文件的用户数据及 inode 号。</p>
</li>
<li><p>为解决文件的共享使用，Linux 系统引入了两种链接：<strong>硬链接 (hard link)</strong> 与<strong>软链接（又称符号链接，即 soft link 或 symbolic link）</strong>。链接为 Linux 系统解决了文件的共享使用，还带来了隐藏文件路径、增加权限安全及节省存储等好处。、</p>
</li>
<li><p>若一个 inode 号对应多个文件名，则称这些文件为硬链接。换言之，硬链接就是同一个文件使用了多个别名。而软链接与硬链接不同，若文件用户数据块中存放的内容是另一文件的路径名的指向，则该文件就是软连接。软链接就是一个普通文件，只是数据块内容有点特殊。软链接有着自己的 inode 号以及用户数据块。</p>
</li>
</ul>
<hr>
<h2 id="硬链接特性"><a href="#硬链接特性" class="headerlink" title="硬链接特性"></a>硬链接特性</h2><p>由于硬链接是有着相同 inode 号仅文件名不同的文件，因此硬链接存在以下几点特性：</p>
<ul>
<li>文件有相同的 inode 及 data block</li>
<li>只能对已存在的文件进行创建</li>
<li>不能交叉文件系统进行硬链接的创建</li>
<li>不能对目录进行创建，只可对文件创建</li>
<li>删除一个硬链接文件并不影响其他有相同 inode 号的文件</li>
</ul>
<hr>
<h2 id="软链接特性"><a href="#软链接特性" class="headerlink" title="软链接特性"></a>软链接特性</h2><p>软链接的创建与使用没有类似硬链接的诸多限制：</p>
<ul>
<li>软链接有自己的文件属性及权限等</li>
<li>可对不存在的文件或目录创建软链接</li>
<li>软链接可交叉文件系统</li>
<li>软链接可对文件或目录创建</li>
<li>创建软链接时，链接计数不会增加</li>
<li>删除软链接并不影响被指向的文件，但若被指向的原文件被删除，则相关软连接被称为死链接（即 dangling link，若被指向路径文件被重新创建，死链接可恢复为正常的软链接）</li>
</ul>
<p><strong>以上内容均截取自：<a href="https://www.ibm.com/developerworks/cn/linux/l-cn-hardandsymb-links/index.html" target="_blank" rel="noopener">IBM Developer : 理解 Linux 的硬链接与软链接</a>，想要获取详细介绍建议访问此网站</strong></p>
<hr>
<h2 id="硬链接基本操作命令"><a href="#硬链接基本操作命令" class="headerlink" title="硬链接基本操作命令"></a>硬链接基本操作命令</h2><ul>
<li><strong>创建硬链接</strong>，命令格式为<code>ln &lt;目标&gt; &lt;链接名&gt;</code>，或者使用<code>link &lt;目标&gt; &lt;链接名&gt;</code>，如：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop101 ~]$ ln currentTime current</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>修改硬链接目标</strong>，命令格式为<code>ln -f &lt;新目标&gt; &lt;链接名&gt;</code>，实际过程是删除同名链接，然后重新创建新链接，如：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop101 ~]$ ln -f currentDate current</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>修改硬链接名称</strong>，和修改普通文件文件名相同：<code>mv &lt;old filename&gt; &lt;new filename&gt;</code></li>
<li><strong>删除硬链接</strong>，删除硬链接就和删除普通文件一样，命令格式为<code>rm &lt;链接名&gt;</code></li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop101 ~]$ rm current</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>查看文件inode号</strong>，命令格式为<code>stat &lt;文件名&gt;</code>，如：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop101 ~]$ <span class="built_in">stat</span> current</span><br><span class="line">  文件：<span class="string">"current"</span></span><br><span class="line">  大小：344       	块：8          IO 块：4096   普通文件</span><br><span class="line">设备：fd00h/64768d	Inode：9884384     硬链接：2</span><br><span class="line">权限：(0644/-rw-r--r--)  Uid：( 1000/tomandersen)   Gid：(    0/    root)</span><br><span class="line">环境：unconfined_u:object_r:user_home_t:s0</span><br><span class="line">最近访问：2020-02-19 11:12:51.937818178 +0800</span><br><span class="line">最近更改：2020-02-19 10:56:24.351489790 +0800</span><br><span class="line">最近改动：2020-02-19 11:17:12.107748775 +0800</span><br><span class="line">创建时间：-</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>根据Inode号查找硬链接</strong>，命令格式为<code>find &lt;查找路径&gt; -inum &lt;Inode号&gt;</code>，如：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop101 ~]$ sudo find ~/ -inum 9884384</span><br><span class="line">/home/TomAndersen/currentTime</span><br><span class="line">/home/TomAndersen/current</span><br></pre></td></tr></table></figure>



<hr>
<h2 id="软链接基本操作命令"><a href="#软链接基本操作命令" class="headerlink" title="软链接基本操作命令"></a>软链接基本操作命令</h2><ul>
<li><strong>创建软连接</strong>，命令格式为<code>ln -s &lt;目标&gt; &lt;链接名&gt;</code>，如：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop101 ~]$ ln -s <span class="variable">$JAVA_HOME</span> java_home</span><br><span class="line">[tomandersen@hadoop101 ~]$ ll</span><br><span class="line">总用量 40</span><br><span class="line">drwxrwxr-x. 2 tomandersen tomandersen    53 2月  19 09:37 bin</span><br><span class="line">-rw-r--r--. 1 tomandersen tomandersen  8686 2月   9 21:47 currentDate</span><br><span class="line">-rw-r--r--. 1 tomandersen root          344 2月  19 10:56 currentTime</span><br><span class="line">lrwxrwxrwx. 1 tomandersen tomandersen    24 2月  19 11:46 java_home -&gt; /opt/module/jdk1.8.0_221</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>修改软链接名称</strong>，和修改普通文件文件名相同：<code>mv &lt;old filename&gt; &lt;new filename&gt;</code>，如：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop101 ~]$ mv java_home hadoop_home</span><br><span class="line">[tomandersen@hadoop101 ~]$ ll</span><br><span class="line">总用量 40</span><br><span class="line">drwxrwxr-x. 2 tomandersen tomandersen    53 2月  19 09:37 bin</span><br><span class="line">-rw-r--r--. 1 tomandersen tomandersen  8686 2月   9 21:47 currentDate</span><br><span class="line">-rw-r--r--. 1 tomandersen root          344 2月  19 10:56 currentTime</span><br><span class="line">lrwxrwxrwx. 1 tomandersen tomandersen    24 2月  19 11:46 hadoop_home -&gt; /opt/module/jdk1.8.0_221</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>修改软链接目标</strong>，命令格式为<code>ln -snf &lt;新目标&gt; &lt;链接名&gt;</code>，如：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop101 ~]$ ln -snf <span class="variable">$HADOOP_HOME</span> hadoop_home</span><br><span class="line">[tomandersen@hadoop101 ~]$ ll</span><br><span class="line">总用量 40</span><br><span class="line">drwxrwxr-x. 2 tomandersen tomandersen    53 2月  19 09:37 bin</span><br><span class="line">-rw-r--r--. 1 tomandersen tomandersen  8686 2月   9 21:47 currentDate</span><br><span class="line">-rw-r--r--. 1 tomandersen root          344 2月  19 10:56 currentTime</span><br><span class="line">lrwxrwxrwx. 1 tomandersen tomandersen    24 2月  19 11:57 hadoop_home -&gt; /opt/module/hadoop-2.7.7</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>删除软链接</strong>，和删除普通文件命令格式相同<code>rm &lt;链接名&gt;</code>，注意后面千万不要加上斜杠 <code>/</code> ，否则就是删除软链接目标文件夹中内容，如</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop101 ~]$ rm hadoop_home</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>根据链接目标查找对应软链接</strong>，命令格式为<code>find &lt;查找路径&gt; -lname &lt;链接目标&gt;</code>，如：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop101 ~]$ sudo find ~/ -lname <span class="variable">$HADOOP_HOME</span> </span><br><span class="line">/home/TomAndersen/hadoop_home</span><br></pre></td></tr></table></figure>



<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CentOS7</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Shell中四种执行脚本方式的对比</title>
    <url>/2020/02/17/Shell%E4%B8%AD%E5%9B%9B%E7%A7%8D%E6%89%A7%E8%A1%8C%E8%84%9A%E6%9C%AC%E6%96%B9%E5%BC%8F%E7%9A%84%E5%AF%B9%E6%AF%94/</url>
    <content><![CDATA[<hr>
<a id="more"></a>

<hr>
<h2 id="测试脚本"><a href="#测试脚本" class="headerlink" title="测试脚本"></a>测试脚本</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment"># test7:用于对比各种脚本程序执行方式的区别</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"The variable var1 is"</span> <span class="variable">$var1</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"The environment variable env1 is"</span> <span class="variable">$env1</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"The current shell PID is $$"</span></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="1-使用绝对路径执行脚本"><a href="#1-使用绝对路径执行脚本" class="headerlink" title="1. 使用绝对路径执行脚本"></a>1. 使用绝对路径执行脚本</h2><ul>
<li><strong>测试过程</strong></li>
</ul>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">root@Dell-Tom:~# var1=1</span><br><span class="line">root@Dell-Tom:~# export env1=1</span><br><span class="line">root@Dell-Tom:~# echo "The current shell PID is" $$</span><br><span class="line">The current shell PID is <span class="number">564</span></span><br><span class="line">root@Dell-Tom:~#</span><br><span class="line">root@Dell-Tom:~#</span><br><span class="line">root@Dell-Tom:~# /root/script_test/test7</span><br><span class="line">The variable var1 is</span><br><span class="line">The environment variable env1 is <span class="number">1</span></span><br><span class="line">The current shell PID is <span class="number">587</span></span><br></pre></td></tr></table></figure>

<ul>
<li><strong>小结：使用绝对路径执行脚本，实际上是新建一个子Shell执行脚本中的命令，并且新建脚本会继承当前Shell的环境变量，但是不继承当前Shell的其他变量</strong></li>
</ul>
<hr>
<h2 id="2-直接使用脚本名执行脚本"><a href="#2-直接使用脚本名执行脚本" class="headerlink" title="2. 直接使用脚本名执行脚本"></a>2. 直接使用脚本名执行脚本</h2><ul>
<li><strong>注意：前提是环境变量PATH中含有脚本路径</strong></li>
<li><strong>测试过程</strong></li>
</ul>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">root@Dell-Tom:~# var1=1</span><br><span class="line">root@Dell-Tom:~# export env1=1</span><br><span class="line">root@Dell-Tom:~# echo "The current shell PID is" $$</span><br><span class="line">The current shell PID is <span class="number">564</span></span><br><span class="line">root@Dell-Tom:~#</span><br><span class="line">root@Dell-Tom:~#</span><br><span class="line">root@Dell-Tom:~# test7</span><br><span class="line">The variable var1 is</span><br><span class="line">The environment variable env1 is <span class="number">1</span></span><br><span class="line">The current shell PID is <span class="number">640</span></span><br></pre></td></tr></table></figure>

<ul>
<li><strong>小结：直接使用脚本名作为命令执行脚本，实际上也是新建一个子Shell执行脚本中的命令，并且新建脚本会继承当前Shell的环境变量，但是不继承当前Shell的其他变量</strong></li>
</ul>
<hr>
<h2 id="3-使用“-test-sh”格式命令执行脚本"><a href="#3-使用“-test-sh”格式命令执行脚本" class="headerlink" title="3. 使用“. test.sh”格式命令执行脚本"></a>3. 使用“. test.sh”格式命令执行脚本</h2><ul>
<li><strong>注意：直接使用<code>.</code>格式执行脚本前提是脚本在指定路径下，或者环境变量PATH中能够找到脚本。一般情况下<code>.</code>后面追加空格以及<code>绝对路径</code>或<code>相对路径</code>都可以，不添加路径的话就是在<code>PATH</code>中定义的路径下寻找</strong></li>
<li><strong>测试过程</strong></li>
</ul>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">root@Dell-Tom:~# var1=1</span><br><span class="line">root@Dell-Tom:~# export env1=1</span><br><span class="line">root@Dell-Tom:~# echo "The current shell PID is" $$</span><br><span class="line">The current shell PID is <span class="number">564</span></span><br><span class="line">root@Dell-Tom:~#</span><br><span class="line">root@Dell-Tom:~#</span><br><span class="line">root@Dell-Tom:~# . script_test/test7</span><br><span class="line">The variable var1 is <span class="number">1</span></span><br><span class="line">The environment variable env1 is <span class="number">1</span></span><br><span class="line">The current shell PID is <span class="number">564</span></span><br></pre></td></tr></table></figure>

<ul>
<li><strong>小结：使用<code>. test.sh</code>格式命令执行脚本，相当于将脚本中的所有命令取出在当前Shell中顺序执行，所以有相同的环境变量、局部变量、状态变量</strong></li>
</ul>
<hr>
<h2 id="4-使用“-test-sh”格式命令执行脚本"><a href="#4-使用“-test-sh”格式命令执行脚本" class="headerlink" title="4. 使用“./test.sh”格式命令执行脚本"></a>4. 使用“./test.sh”格式命令执行脚本</h2><ul>
<li><strong>注意：此格式命令实际上是<code>.</code>加上脚本的相对路径</strong></li>
<li><strong>测试过程</strong></li>
</ul>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">root@Dell-Tom:~# var1=1</span><br><span class="line">root@Dell-Tom:~# export env1=1</span><br><span class="line">root@Dell-Tom:~#</span><br><span class="line">root@Dell-Tom:~# echo "The current shell PID is" $$</span><br><span class="line">The current shell PID is <span class="number">564</span></span><br><span class="line">root@Dell-Tom:~#</span><br><span class="line">root@Dell-Tom:~# ./script_test/test7</span><br><span class="line">The variable var1 is</span><br><span class="line">The environment variable env1 is <span class="number">1</span></span><br><span class="line">The current shell PID is <span class="number">648</span></span><br></pre></td></tr></table></figure>

<ul>
<li><strong>小结：使用<code>.</code>加上脚本相对路径的方式执行脚本，实际上也是新建一个子Shell执行脚本中的命令，并且新建脚本会继承当前Shell的环境变量，但是不继承当前Shell的其他变量</strong></li>
</ul>
<hr>
<h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h2><p><strong>①使用绝对路径执行脚本<code>/root/script_test/test.sh</code>，实际上是新建一个子Shell执行脚本中的命令，并且新建脚本会继承当前Shell的环境变量，但是不继承当前Shell的其他变量</strong></p>
<p><strong>②直接使用脚本名<code>test.sh</code>作为命令执行脚本，实际上也是新建一个子Shell执行脚本中的命令，并且新建脚本会继承当前Shell的环境变量，但是不继承当前Shell的其他变量。前提是环境变量PATH中含有脚本路径</strong></p>
<p><strong>③使用<code>. test.sh</code>格式命令执行脚本，相当于将脚本中的所有命令取出在当前Shell中顺序执行，所以有相同的环境变量、局部变量、状态变量等。前提是脚本在当前路径下，或者环境变量PATH中能够找到脚本</strong></p>
<p><strong>④使用<code>.</code>加上脚本相对路径的方式<code>./test.sh</code>执行脚本，实际上也是新建一个子Shell执行脚本中的命令，并且新建脚本会继承当前Shell的环境变量，但是不继承当前Shell的其他变量</strong></p>
<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>CentOS7</tag>
        <tag>Linux</tag>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title>Shell中特殊参数变量和特殊状态变量汇总</title>
    <url>/2020/02/17/Shell%E4%B8%AD%E7%89%B9%E6%AE%8A%E5%8F%82%E6%95%B0%E5%8F%98%E9%87%8F%E5%92%8C%E7%89%B9%E6%AE%8A%E7%8A%B6%E6%80%81%E5%8F%98%E9%87%8F%E6%B1%87%E6%80%BB/</url>
    <content><![CDATA[<hr>
<a id="more"></a>

<hr>
<h2 id="特殊参数变量："><a href="#特殊参数变量：" class="headerlink" title="特殊参数变量："></a>特殊参数变量：</h2><p><strong><code>$#</code>：传给Shell脚本的参数个数</strong></p>
<p><strong><code>$0</code>：当前Shell脚本名</strong></p>
<p><strong><code>$1</code>：传递给Shell脚本的第1个参数，若没有则为空</strong></p>
<p><strong><code>$2</code>：传递给Shell脚本的第2个参数，若没有则为空</strong></p>
<p><strong><code>$3</code>：传递给Shell脚本的第3个参数，后续以此类推</strong></p>
<p><strong><code>${10}</code>：传递给Shell脚本的第10个参数，读取的参数位置大于等于10之后需要使用花括号</strong></p>
<p><strong><code>$@</code>：传递给Shell脚本的所有参数组成的列表，即参数列表</strong></p>
<p><strong><code>$*</code>：传递给Shell脚本的的所有参数组成的单个字符串，不同参数之间空格依旧存在</strong></p>
<hr>
<h2 id="特殊状态变量："><a href="#特殊状态变量：" class="headerlink" title="特殊状态变量："></a>特殊状态变量：</h2><p><strong><code>$?</code>：上一指令的退出码（既可以是执行脚本程序指令也可以是普通命令），0表示正常退出，其余表示异常</strong></p>
<p><strong><code>$$</code>：当前Shell的PID</strong></p>
<p><strong><code>$!</code>：获取上一个后台执行的Shell的PID（若不在运行，则返回空）</strong></p>
<p><strong><code>$_</code>：获取上一个执行指令，如果执行指令有参数，则返回指令的最后一个参数（如执行脚本）</strong></p>
<hr>
<h2 id="测试脚本："><a href="#测试脚本：" class="headerlink" title="测试脚本："></a>测试脚本：</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment"># test8:用于练习特殊参数变量</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"The number of parameters is <span class="variable">$#</span>"</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"The current script is <span class="variable">$0</span>"</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"The first parameter is <span class="variable">$1</span>"</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"The parameter-10 is <span class="variable">$&#123;10&#125;</span>"</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"The parameters string is $*"</span></span><br><span class="line"><span class="comment"># 使用shift命令可以将参数列表前移指定次数，每次移动一次则覆盖一次$1，参数个数减1</span></span><br><span class="line"><span class="built_in">shift</span> 1</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"The parameters list is <span class="variable">$@</span>"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 用于练习特殊状态变量</span></span><br><span class="line">date </span><br><span class="line"><span class="built_in">echo</span> <span class="string">"The last command or parameter is <span class="variable">$_</span>"</span></span><br><span class="line">date +%Y-%m-%d</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"The last command or parameter is <span class="variable">$_</span>"</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"123"</span>&gt; tmp &amp;</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"The last command exit code is $?"</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"The last background shell PID is $!"</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"The current shell PID is $$"</span></span><br></pre></td></tr></table></figure>
<hr>
<h2 id="测试脚本输出："><a href="#测试脚本输出：" class="headerlink" title="测试脚本输出："></a>测试脚本输出：</h2><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">root@localhost:~/script_test# ./test8 <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span></span><br><span class="line">The number of parameters is <span class="number">6</span></span><br><span class="line">The current script is ./test8</span><br><span class="line">The first parameter is <span class="number">1</span></span><br><span class="line">The parameter<span class="number">-10</span> is</span><br><span class="line">The parameters <span class="built_in">string</span> is <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span></span><br><span class="line">The parameters <span class="built_in">list</span> is <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span></span><br><span class="line">Mon Feb <span class="number">17</span> <span class="number">10</span>:<span class="number">57</span>:<span class="number">17</span> CST <span class="number">2020</span></span><br><span class="line">The last command <span class="keyword">or</span> parameter is date</span><br><span class="line"><span class="number">2020</span><span class="number">-02</span><span class="number">-17</span></span><br><span class="line">The last command <span class="keyword">or</span> parameter is +%Y-%m-%d</span><br><span class="line">The last command <span class="built_in">exit</span> code is <span class="number">0</span></span><br><span class="line">The last <span class="built_in">background</span> shell PID is <span class="number">460</span></span><br><span class="line">The current shell PID is <span class="number">457</span></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>CentOS7</tag>
        <tag>Linux</tag>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title>Java中实例初始化方法&lt;init&gt;()原理解析</title>
    <url>/2020/02/16/Java%E4%B8%AD%E5%AE%9E%E4%BE%8B%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E6%B3%95-init-%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<hr>
<a id="more"></a>

<hr>
<h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h2><ul>
<li><strong>本文主要从字节码角度解析使用<code>new</code>关键字创建对象时，实例初始化方法<code>&lt;init&gt;()</code>的执行过程和原理</strong></li>
</ul>
<hr>
<h2 id="2-测试代码"><a href="#2-测试代码" class="headerlink" title="2. 测试代码"></a>2. 测试代码</h2><ul>
<li><strong>本次实验所举的例子并不具备实际意义，只是为了实验方便而取名为Anima和Dog</strong></li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Animal</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> number = <span class="number">10</span>;</span><br><span class="line">	Animal()&#123;</span><br><span class="line">		number++;</span><br><span class="line">		System.out.println(number);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dog</span> <span class="keyword">extends</span> <span class="title">Animal</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> number = getDefaultNumber();</span><br><span class="line">	<span class="keyword">final</span> <span class="keyword">int</span> defaultNumber = <span class="number">20</span>;</span><br><span class="line">	&#123;</span><br><span class="line">		System.out.println(<span class="string">"This nonstatic block"</span>);</span><br><span class="line">	&#125;</span><br><span class="line">	Dog()&#123;</span><br><span class="line">		<span class="keyword">this</span>(<span class="number">20</span>);</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	Dog(<span class="keyword">int</span> num)&#123;</span><br><span class="line">		number = num;</span><br><span class="line">		System.out.println(number);</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="function"><span class="keyword">int</span> <span class="title">getDefaultNumber</span><span class="params">()</span></span>&#123;</span><br><span class="line">		<span class="keyword">return</span> defaultNumber;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">InheritTest</span></span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">		<span class="keyword">new</span> Dog();</span><br><span class="line">        <span class="comment">// Output:</span></span><br><span class="line">        <span class="comment">// 11</span></span><br><span class="line">        <span class="comment">// This nonstatic block</span></span><br><span class="line">        <span class="comment">// 20</span></span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="3-反编译class文件"><a href="#3-反编译class文件" class="headerlink" title="3. 反编译class文件"></a>3. 反编译class文件</h2><ul>
<li><p><strong>使用<code>javap</code>工具反汇编class文件输出字节码命令，工具使用格式<code>javap -c Animal.class</code></strong></p>
</li>
<li><p><strong>Animal.class</strong></p>
</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Animal.class</span></span><br><span class="line">Compiled from <span class="string">"InheritTest.java"</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Animal</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> number;</span><br><span class="line"></span><br><span class="line">  Animal();</span><br><span class="line">    Code:</span><br><span class="line">       <span class="number">0</span>: aload_0</span><br><span class="line">       1: invokespecial #1                  // Method java/lang/Object."&lt;init&gt;":()V</span><br><span class="line">       <span class="number">4</span>: aload_0</span><br><span class="line">       <span class="number">5</span>: bipush        <span class="number">10</span></span><br><span class="line">       7: putfield      #2                  // Field number:I</span><br><span class="line">      <span class="number">10</span>: aload_0</span><br><span class="line">      <span class="number">11</span>: dup</span><br><span class="line">      12: getfield      #2                  // Field number:I</span><br><span class="line">      <span class="number">15</span>: iconst_1</span><br><span class="line">      <span class="number">16</span>: iadd</span><br><span class="line">      17: putfield      #2                  // Field number:I</span><br><span class="line">      20: getstatic     #3                  // Field java/lang/System.out:Ljava/io/PrintStream;</span><br><span class="line">      <span class="number">23</span>: aload_0</span><br><span class="line">      24: getfield      #2                  // Field number:I</span><br><span class="line">      27: invokevirtual #4                  // Method java/io/PrintStream.println:(I)V</span><br><span class="line">      <span class="number">30</span>: <span class="keyword">return</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>Dog.class</strong></li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Dog.class</span></span><br><span class="line">Compiled from <span class="string">"InheritTest.java"</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dog</span> <span class="keyword">extends</span> <span class="title">Animal</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> number;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">final</span> <span class="keyword">int</span> defaultNumber;</span><br><span class="line"></span><br><span class="line">  Dog();</span><br><span class="line">    Code:</span><br><span class="line">       <span class="number">0</span>: aload_0</span><br><span class="line">       <span class="number">1</span>: bipush        <span class="number">20</span></span><br><span class="line">       3: invokespecial #1                  // Method "&lt;init&gt;":(I)V</span><br><span class="line">       <span class="number">6</span>: <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">  Dog(<span class="keyword">int</span>);</span><br><span class="line">    Code:</span><br><span class="line">       <span class="number">0</span>: aload_0</span><br><span class="line">       1: invokespecial #2                  // Method Animal."&lt;init&gt;":()V</span><br><span class="line">       <span class="number">4</span>: aload_0</span><br><span class="line">       <span class="number">5</span>: aload_0</span><br><span class="line">       6: invokevirtual #3                  // Method getDefaultNumber:()I</span><br><span class="line">       9: putfield      #4                  // Field number:I</span><br><span class="line">      <span class="number">12</span>: aload_0</span><br><span class="line">      <span class="number">13</span>: bipush        <span class="number">20</span></span><br><span class="line">      15: putfield      #5                  // Field defaultNumber:I</span><br><span class="line">      18: getstatic     #6                  // Field java/lang/System.out:Ljava/io/PrintStream;</span><br><span class="line">      21: ldc           #7                  // String This nonstatic block</span><br><span class="line">      23: invokevirtual #8                  // Method java/io/PrintStream.println:(Ljava/lang/String;)V</span><br><span class="line">      <span class="number">26</span>: aload_0</span><br><span class="line">      <span class="number">27</span>: iload_1</span><br><span class="line">      28: putfield      #4                  // Field number:I</span><br><span class="line">      31: getstatic     #6                  // Field java/lang/System.out:Ljava/io/PrintStream;</span><br><span class="line">      <span class="number">34</span>: aload_0</span><br><span class="line">      35: getfield      #4                  // Field number:I</span><br><span class="line">      38: invokevirtual #9                  // Method java/io/PrintStream.println:(I)V</span><br><span class="line">      <span class="number">41</span>: <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">int</span> <span class="title">getDefaultNumber</span><span class="params">()</span></span>;</span><br><span class="line">    Code:</span><br><span class="line">       <span class="number">0</span>: bipush        <span class="number">20</span></span><br><span class="line">       <span class="number">2</span>: ireturn</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>InheritTest.class</strong></li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// InheritTest.class</span></span><br><span class="line">Compiled from <span class="string">"InheritTest.java"</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">InheritTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">InheritTest</span><span class="params">()</span></span>;</span><br><span class="line">    Code:</span><br><span class="line">       <span class="number">0</span>: aload_0</span><br><span class="line">       1: invokespecial #1                  // Method java/lang/Object."&lt;init&gt;":()V</span><br><span class="line">       <span class="number">4</span>: <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(java.lang.String[])</span></span>;</span><br><span class="line">    Code:</span><br><span class="line">       0: new           #2                  // class Dog</span><br><span class="line">       <span class="number">3</span>: dup</span><br><span class="line">       4: invokespecial #3                  // Method Dog."&lt;init&gt;":()V</span><br><span class="line">       <span class="number">7</span>: pop</span><br><span class="line">       <span class="number">8</span>: <span class="keyword">return</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="4-字节码、源码对比分析"><a href="#4-字节码、源码对比分析" class="headerlink" title="4. 字节码、源码对比分析"></a>4. 字节码、源码对比分析</h2><ul>
<li><strong>由于<code>main()</code>方法中是直接调用Dog类的无参构造方法Constructor来创建类，因此我们主要来分析Dog类中字节码命令的执行过程</strong></li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Dog.class</span></span><br><span class="line">Compiled from <span class="string">"InheritTest.java"</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dog</span> <span class="keyword">extends</span> <span class="title">Animal</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> number;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">final</span> <span class="keyword">int</span> defaultNumber;</span><br><span class="line"></span><br><span class="line">  Dog();</span><br><span class="line">    Code:</span><br><span class="line">       <span class="number">0</span>: aload_0</span><br><span class="line">       <span class="number">1</span>: bipush        <span class="number">20</span></span><br><span class="line">       3: invokespecial #1                  // Method "&lt;init&gt;":(I)V</span><br><span class="line">       <span class="number">6</span>: <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">  Dog(<span class="keyword">int</span>);</span><br><span class="line">    Code:</span><br><span class="line">       <span class="number">0</span>: aload_0</span><br><span class="line">       1: invokespecial #2                  // Method Animal."&lt;init&gt;":()V</span><br><span class="line">       <span class="number">4</span>: aload_0</span><br><span class="line">       <span class="number">5</span>: aload_0</span><br><span class="line">       6: invokevirtual #3                  // Method getDefaultNumber:()I</span><br><span class="line">       9: putfield      #4                  // Field number:I</span><br><span class="line">      <span class="number">12</span>: aload_0</span><br><span class="line">      <span class="number">13</span>: bipush        <span class="number">20</span></span><br><span class="line">      15: putfield      #5                  // Field defaultNumber:I</span><br><span class="line">      18: getstatic     #6                  // Field java/lang/System.out:Ljava/io/PrintStream;</span><br><span class="line">      21: ldc           #7                  // String This nonstatic block</span><br><span class="line">      23: invokevirtual #8                  // Method java/io/PrintStream.println:(Ljava/lang/String;)V</span><br><span class="line">      <span class="number">26</span>: aload_0</span><br><span class="line">      <span class="number">27</span>: iload_1</span><br><span class="line">      28: putfield      #4                  // Field number:I</span><br><span class="line">      31: getstatic     #6                  // Field java/lang/System.out:Ljava/io/PrintStream;</span><br><span class="line">      <span class="number">34</span>: aload_0</span><br><span class="line">      35: getfield      #4                  // Field number:I</span><br><span class="line">      38: invokevirtual #9                  // Method java/io/PrintStream.println:(I)V</span><br><span class="line">      <span class="number">41</span>: <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">int</span> <span class="title">getDefaultNumber</span><span class="params">()</span></span>;</span><br><span class="line">    Code:</span><br><span class="line">       <span class="number">0</span>: bipush        <span class="number">20</span></span><br><span class="line">       <span class="number">2</span>: ireturn</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Dog.java</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dog</span> <span class="keyword">extends</span> <span class="title">Animal</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> number = getDefaultNumber();</span><br><span class="line">	<span class="keyword">final</span> <span class="keyword">int</span> defaultNumber = <span class="number">20</span>;</span><br><span class="line">	&#123;</span><br><span class="line">		System.out.println(<span class="string">"This nonstatic block"</span>);</span><br><span class="line">	&#125;</span><br><span class="line">	Dog()&#123;</span><br><span class="line">		<span class="keyword">this</span>(<span class="number">20</span>);</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	Dog(<span class="keyword">int</span> num)&#123;</span><br><span class="line">		number = num;</span><br><span class="line">		System.out.println(number);</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="function"><span class="keyword">int</span> <span class="title">getDefaultNumber</span><span class="params">()</span></span>&#123;</span><br><span class="line">		<span class="keyword">return</span> defaultNumber;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Output:</span></span><br><span class="line"><span class="comment">// 11</span></span><br><span class="line"><span class="comment">// This nonstatic block</span></span><br><span class="line"><span class="comment">// 20</span></span><br></pre></td></tr></table></figure>

<h3 id="a-无参构造方法Dog"><a href="#a-无参构造方法Dog" class="headerlink" title="a. 无参构造方法Dog()"></a>a. 无参构造方法Dog()</h3><ul>
<li><strong>首先分析<code>Dog()</code>方法：<code>Dog()</code>方法中的内容很简短，即直接调用当前类中的有参构造<code>Dog(int)</code>，并传入整型参数<code>20</code>，不多赘述</strong></li>
</ul>
<h3 id="b-有参构造方法Dog-int"><a href="#b-有参构造方法Dog-int" class="headerlink" title="b. 有参构造方法Dog(int)"></a>b. 有参构造方法Dog(int)</h3><ul>
<li><strong>其次分析<code>Dog(int)</code>方法：<code>Dog(int)</code>方法首先隐式调用了父类Animal的无参构造，然后才执行本构造方法中的实际内容</strong></li>
<li><strong>但是通过观察字节码指令可知，在通过编译构造函数Constructor生成类初始化方法<code>&lt;init&gt;()</code>时，并不只是将Constructor中的命令编译进<code>&lt;init&gt;()</code>方法中，同时还带有本类中实例成员的初始化代码以及非静态代码块编译成的字节码命令</strong></li>
<li><strong>这就说明实例初始化<code>&lt;init&gt;()</code>方法在生成过程中，实际上还收集了<code>所有实例成员的初始化代码</code>与<code>非静态代码块{}</code>，收集顺序和源码顺序相同，并将其放置于调用父类构造方法之后，在执行Constructor方法剩余命令之前。</strong></li>
<li><strong>由此我们得知<code>&lt;init&gt;()</code>方法的组成以及执行过程：调用Constructor中指定的父类<code>&lt;init&gt;()</code>方法  <code>==&gt;</code>  实例成员初始化语句和非静态代码  <code>==&gt;</code>  Constructor方法</strong></li>
</ul>
<h3 id="c-getDefaultNumber-方法"><a href="#c-getDefaultNumber-方法" class="headerlink" title="c. getDefaultNumber()方法"></a>c. getDefaultNumber()方法</h3><ul>
<li><strong><code>getDefaultNumber()</code>方法中的内容也很简单，实际上就是返回一个实例常量成员，但值得注意的是，由于编译器的<code>常量传播</code>优化特性，在<code>getDefaultNumber()</code>方法中，直接使用常量<code>20</code>代替<code>defaultNumber</code>，而并不是返回常量<code>defaultNumber</code>的当前值</strong></li>
<li><strong>通过对比源码和字节码命令我们也能看出，在调用<code>getDefaultNumber()</code>方法时，<code>defaultNumber</code>变量本身其实还并未被初始化，只是分配了内存，也就是说其当前值并非<code>20</code>，而是<code>int</code>类型的默认值<code>0</code>，但是由于Java编译器<code>常量传播</code>这一优化特性，因此<code>getDefaultNumber()</code>方法直接返回<code>20</code></strong></li>
</ul>
<hr>
<h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h2><ul>
<li><strong><code>&lt;init&gt;()</code>方法的组成以及执行过程：调用Constructor中指定的父类<code>&lt;init&gt;()</code>方法 <code>==&gt;</code> 实例成员初始化代码和非静态代码块 <code>==&gt;</code> Constructor方法剩余内容</strong></li>
<li><strong>其中类中类变量赋值和静态代码块在类加载时已经执行，具体参照《深入理解Java虚拟机：JVM高级特性与最佳实践（第二版）》中类初始化相关内容</strong></li>
<li><strong>类中的每个类构造Constructor方法（或者说类构造函数、类构造器）都会生成一个<code>&lt;init&gt;()</code>方法，根据实际情况调用其中的某一个或某几个</strong></li>
<li><strong>注意：本次实验结果还说明，若在同一个类的Constructor方法中调用另一个Constructor方法（如无参构造调用有参构造），则不会在当前Constructor方法生成的<code>&lt;init&gt;()</code>方法中插入本类中的非静态代码，而是在最终调用父类初始化代码的<code>&lt;init&gt;()</code>方法中插入</strong></li>
</ul>
<hr>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><blockquote>
<p>《深入理解Java虚拟机：JVM高级特性与最佳实践（第二版）》</p>
</blockquote>
<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Javap</tag>
        <tag>Javac</tag>
      </tags>
  </entry>
  <entry>
    <title>Java之从字节码角度解析i++和++i原理</title>
    <url>/2020/02/13/Java%E4%B9%8B%E4%BB%8E%E5%AD%97%E8%8A%82%E7%A0%81%E8%A7%92%E5%BA%A6%E8%A7%A3%E6%9E%90i++%E5%92%8C++i%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<hr>
<a id="more"></a>

<hr>
<h2 id="1-直接上代码："><a href="#1-直接上代码：" class="headerlink" title="1. 直接上代码："></a>1. 直接上代码：</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">int</span> i = <span class="number">0</span>,j = <span class="number">0</span>;</span><br><span class="line">		i=i++;</span><br><span class="line">		j=++j;</span><br><span class="line">		System.out.println(i);</span><br><span class="line">		System.out.println(j);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="2-执行结果"><a href="#2-执行结果" class="headerlink" title="2. 执行结果"></a>2. 执行结果</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span></span><br><span class="line"><span class="number">1</span></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="3-反汇编class文件输出："><a href="#3-反汇编class文件输出：" class="headerlink" title="3. 反汇编class文件输出："></a>3. 反汇编class文件输出：</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">javac Test.java</span><br><span class="line">javap -c Test.class &gt;&gt; Test_class_compile.txt</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Compiled from <span class="string">"Test.java"</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">Test</span><span class="params">()</span></span>;</span><br><span class="line">    Code:</span><br><span class="line">       <span class="number">0</span>: aload_0</span><br><span class="line">       1: invokespecial #1                  // Method java/lang/Object."&lt;init&gt;":()V</span><br><span class="line">       <span class="number">4</span>: <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(java.lang.String[])</span></span>;</span><br><span class="line">    Code:</span><br><span class="line">       <span class="number">0</span>: iconst_0</span><br><span class="line">       <span class="number">1</span>: istore_1</span><br><span class="line">       <span class="number">2</span>: iconst_0</span><br><span class="line">       <span class="number">3</span>: istore_2</span><br><span class="line">       <span class="number">4</span>: iload_1</span><br><span class="line">       <span class="number">5</span>: iinc          <span class="number">1</span>, <span class="number">1</span></span><br><span class="line">       <span class="number">8</span>: istore_1</span><br><span class="line">       <span class="number">9</span>: iinc          <span class="number">2</span>, <span class="number">1</span></span><br><span class="line">      <span class="number">12</span>: iload_2</span><br><span class="line">      <span class="number">13</span>: istore_2</span><br><span class="line">      14: getstatic     #2                  // Field java/lang/System.out:Ljava/io/PrintStream;</span><br><span class="line">      <span class="number">17</span>: iload_1</span><br><span class="line">      18: invokevirtual #3                  // Method java/io/PrintStream.println:(I)V</span><br><span class="line">      21: getstatic     #2                  // Field java/lang/System.out:Ljava/io/PrintStream;</span><br><span class="line">      <span class="number">24</span>: iload_2</span><br><span class="line">      25: invokevirtual #3                  // Method java/io/PrintStream.println:(I)V</span><br><span class="line">      <span class="number">28</span>: <span class="keyword">return</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="4-字节码执行过程解释"><a href="#4-字节码执行过程解释" class="headerlink" title="4. 字节码执行过程解释"></a>4. 字节码执行过程解释</h2><ul>
<li><strong>下面我们对主函数中主要字节码进行逐行分析，主要针对程序执行过程中<code>程序计数器、局部变量表、操作栈</code>的变化情况来描述程序执行过程，其中<code>程序计数器</code>记录字节码执行偏移量</strong></li>
<li><h3 id="执行偏移地址为0的指令时："><a href="#执行偏移地址为0的指令时：" class="headerlink" title="执行偏移地址为0的指令时："></a>执行偏移地址为0的指令时：</h3><img src="https://img-blog.csdnimg.cn/20200213105729682.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="执行偏移地址为0的指令时"></li>
<li><h3 id="执行偏移地址为1的指令时："><a href="#执行偏移地址为1的指令时：" class="headerlink" title="执行偏移地址为1的指令时："></a>执行偏移地址为1的指令时：</h3><img src="https://img-blog.csdnimg.cn/20200213105902666.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="执行偏移地址为1的指令时"></li>
<li><strong>由于2~3字节码执行任务与0~1相同，这里不再赘述，直接进入偏移地址4的指令执行状态</strong></li>
<li><h3 id="执行偏移地址为4的指令时："><a href="#执行偏移地址为4的指令时：" class="headerlink" title="执行偏移地址为4的指令时："></a>执行偏移地址为4的指令时：</h3><img src="https://img-blog.csdnimg.cn/20200213110423801.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="执行偏移地址为4的指令时"></li>
<li><h3 id="执行偏移地址为5的指令时："><a href="#执行偏移地址为5的指令时：" class="headerlink" title="执行偏移地址为5的指令时："></a>执行偏移地址为5的指令时：</h3><img src="https://img-blog.csdnimg.cn/20200213110619774.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="执行偏移地址为5的指令时"></li>
<li><h3 id="执行偏移地址为8的指令时："><a href="#执行偏移地址为8的指令时：" class="headerlink" title="执行偏移地址为8的指令时："></a>执行偏移地址为8的指令时：</h3></li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/20200213110711430.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="执行偏移地址为8的指令时"></p>
<ul>
<li><h3 id="执行偏移地址为9的指令时："><a href="#执行偏移地址为9的指令时：" class="headerlink" title="执行偏移地址为9的指令时："></a>执行偏移地址为9的指令时：</h3><img src="https://img-blog.csdnimg.cn/20200213110752670.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="执行偏移地址为9的指令时"></li>
<li><h3 id="执行偏移地址为12的指令时："><a href="#执行偏移地址为12的指令时：" class="headerlink" title="执行偏移地址为12的指令时："></a>执行偏移地址为12的指令时：</h3><img src="https://img-blog.csdnimg.cn/20200213110818700.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="执行偏移地址为12的指令时"></li>
<li><h3 id="执行偏移地址为13的指令时："><a href="#执行偏移地址为13的指令时：" class="headerlink" title="执行偏移地址为13的指令时："></a>执行偏移地址为13的指令时：</h3><img src="https://img-blog.csdnimg.cn/20200213110839449.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="执行偏移地址为13的指令时"></li>
<li><h3 id="因此最终i的值为0，j的值为1"><a href="#因此最终i的值为0，j的值为1" class="headerlink" title="因此最终i的值为0，j的值为1"></a>因此最终<code>i</code>的值为0，<code>j</code>的值为1</h3></li>
</ul>
<hr>
<h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h2><ul>
<li><strong><code>i=i++</code>的过程实际上就是先加载i到操作栈中，然后再执行<code>++</code>运算，即直接对局部变量表中i的值<code>+1</code>，然后直接将栈顶的值赋值给局部变量表中<code>i</code>的位置</strong></li>
<li><strong><code>j=++j</code>的过程实际上就是先对局部变量表中的<code>j</code>的值<code>+1</code>，然后将局部变量<code>j</code>的值加载到操作栈中，最后将栈顶元素赋值给局部变量<code>j</code>，因此<code>j=++j</code>与<code>++j</code>是等价的，因为压栈和出栈是连续的两个过程，即什么都没改变</strong></li>
</ul>
<hr>
<h2 id="6-推广"><a href="#6-推广" class="headerlink" title="6. 推广"></a>6. 推广</h2><ul>
<li><strong>同理，如表达式<code>(a++ + a--)</code>，假设初始时<code>a=10</code>，那么此表达式执行过程实际上就是先加载局部变量<code>a</code>的值到操作栈中，然后<code>++</code>对局部变量表中<code>a</code>的值直接<code>+1</code>，根据运算符优先级，然后再次加载<code>a</code>的值到栈中（此时<code>a</code>值为<code>11</code>），然后<code>--</code>对局部变量表中<code>a</code>的值直接<code>-1</code>，最后才进行加法运算<code>+</code>，即直接将操作栈顶两个与元素相加（此时操作栈中元素为<code>11</code>和<code>10</code>），故最后此表达式的结果为<code>21</code></strong></li>
</ul>
<hr>
<h2 id="7-参考资料"><a href="#7-参考资料" class="headerlink" title="7. 参考资料"></a>7. 参考资料</h2><blockquote>
<p>《深入理解Java虚拟机：JVM高级特性与最佳实践（第二版）》</p>
</blockquote>
<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Javap</tag>
        <tag>Javac</tag>
      </tags>
  </entry>
  <entry>
    <title>从零开始搭建Hexo个人博客到Github上</title>
    <url>/2020/02/11/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%90%AD%E5%BB%BAHexo%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%88%B0Github%E4%B8%8A/</url>
    <content><![CDATA[<hr>
<a id="more"></a>

<hr>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul>
<li>本次搭建的博客为基于Hexo框架的静态博客</li>
<li>本次是在Windows上搭建个人博客，其他系统大同小异，使用的是git bash来运行一些简单的Linux命令</li>
<li>Github国内访问速度较慢（特殊工具除外），所以这并不是最佳的搭建方案，有条件的话建议还是自己购买服务器进行搭建</li>
</ul>
<hr>
<h2 id="具体搭建步骤"><a href="#具体搭建步骤" class="headerlink" title="具体搭建步骤"></a>具体搭建步骤</h2><h3 id="1-下载Git"><a href="#1-下载Git" class="headerlink" title="1. 下载Git"></a>1. 下载Git</h3><ul>
<li><p><strong>本次搭建博客，我们主要会用到git bash工具来执行Linux命令，使用cmd也行。前往Git官网下载Git工具，并在git bash中配置好git（设置全局用户名、全局邮箱等），如：</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git config --global user.name <span class="string">"John Doe"</span></span><br><span class="line">git config --global user.email johndoe@example.com</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>没有Github账号记得先申请一个，后续所有命令都在git bash中执行</strong></p>
</li>
</ul>
<h3 id="2-下载node-js"><a href="#2-下载node-js" class="headerlink" title="2. 下载node.js"></a>2. 下载node.js</h3><ul>
<li><p><strong>前往node.js官网下载LTS版本（长期支持版），直接安装即可，因为后续Hexo的安装要用到npm工具：</strong><br><img src="https://img-blog.csdnimg.cn/20200211231905205.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="下载node.js"></p>
</li>
<li><p><strong>下载完成之后直接安装，安装完成之后使用 <code>node -v</code> 和 <code>npm -v</code> 命令来检查是否安装成功：</strong></p>
<p>  <img src="https://img-blog.csdnimg.cn/20200211232042694.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="检查node.js安装"></p>
</li>
</ul>
<h3 id="3-下载npm淘宝镜像"><a href="#3-下载npm淘宝镜像" class="headerlink" title="3. 下载npm淘宝镜像"></a>3. 下载npm淘宝镜像</h3><ul>
<li><p><strong>由于自带的npm工具下载hexo很慢（GFW牛逼~），所以先使用npm来下载cnpm工具，然后用cnpm下载hexo就会快得多：</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install -g cnpm --registry=https://registry.npm.taobao.org</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>如果命令执行失败了就多试几次。安装完成后使用<code>cnpm -v</code>来测试是否安装成功：</strong><br><img src="https://img-blog.csdnimg.cn/20200211232138754.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="安装cnpm"></p>
</li>
</ul>
<h3 id="4-下载hexo框架"><a href="#4-下载hexo框架" class="headerlink" title="4. 下载hexo框架"></a>4. 下载hexo框架</h3><ul>
<li><p><strong>使用cnpm工具下载hexo：</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cnpm install -g hexo-cli</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>使用 <code>hexo v</code> 命令来测试hexo是否安装成功：</strong><br><img src="https://img-blog.csdnimg.cn/20200211232255425.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="安装hexo"></p>
</li>
</ul>
<h3 id="5-启动hexo"><a href="#5-启动hexo" class="headerlink" title="5. 启动hexo"></a>5. 启动hexo</h3><ul>
<li><p><strong>先创建指定文件夹，用于hexo博客站点，本次我创建的文件夹名为<code>HexoBlogs</code>，之后的git bash命令都在此文件夹内执行：</strong></p>
<p>  <img src="https://img-blog.csdnimg.cn/20200211232335197.png" alt="创建博客站点文件夹"></p>
</li>
<li><p><strong>进入此文件夹，启动hexo框架之前先进行初始化：</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo init</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>初始化完成之后会创建一个默认的hello-world博客和默认的landscape主题。然后我们生成博客对应的页面</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo g <span class="comment"># 或者hexo generate</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>然后我们启动hexo服务（退出时命令行使用ctrl+c）</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo s <span class="comment"># 或者hexo server</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>前往<code>http://localhost:4000</code>页面查看本地博客页面是否生成成功：</strong></p>
</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/2020021123241594.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="localhost:4000"></p>
<h3 id="6-在Github上新建仓库"><a href="#6-在Github上新建仓库" class="headerlink" title="6. 在Github上新建仓库"></a>6. 在Github上新建仓库</h3><ul>
<li><p><strong>新建仓库命名格式必须为<code>&lt;Owner&gt;.github.io</code>，即“用户名.github.io”的格式，如<code>tomandersen-cc.github.io</code>：</strong><br><img src="https://img-blog.csdnimg.cn/20200211232436407.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="新建仓库"></p>
</li>
<li><p><strong>创建完成之后保留此页面：</strong><br><img src="https://img-blog.csdnimg.cn/20200211232455599.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="保留页面"></p>
<h3 id="7-上传本地hexo博客框架"><a href="#7-上传本地hexo博客框架" class="headerlink" title="7. 上传本地hexo博客框架"></a>7. 上传本地hexo博客框架</h3></li>
<li><p><strong>安装配置工具：</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cnpm install --save hexo-deployer-git</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>修改hexo相关配置，在之前创建的文件夹中，修改配置文件<code>_config.yml</code>，在最后几行的<code>deploy</code>模块中设置对应的参数<code>type</code> 、<code>repo</code>和<code>branch</code>：</strong></p>
<p>  <img src="https://img-blog.csdnimg.cn/20200211232517279.png" alt="配置参数"></p>
</li>
<li><p><strong>其中参数<code>repo</code>的值即为之前创建仓库页面所显示的仓库地址，即：</strong></p>
<p>  <img src="https://img-blog.csdnimg.cn/2020021123252556.png" alt="repo参数"></p>
</li>
<li><p><strong>修改完成后，保存退出</strong></p>
</li>
<li><p><strong>然后重新生成博客页面并上传至Github，依次输入以下命令：</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g <span class="comment"># 或者 hexo generate</span></span><br><span class="line">hexo d <span class="comment"># 或者 hexo deploy</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>通过之前设置的仓库名来访问上传的博客（之后仓库名不能更改）：</strong></p>
<p>  <img src="https://img-blog.csdnimg.cn/20200211232602295.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="访问博客"></p>
</li>
</ul>
<ul>
<li><strong>至此hexo博客就已经搭建完成，并且成功上传到Github了</strong></li>
</ul>
<hr>
<h2 id="发布博客"><a href="#发布博客" class="headerlink" title="发布博客"></a>发布博客</h2><ul>
<li><p><strong>使用<code>hexo n &lt;blogname&gt;</code>命令创建博客，如：<code>hexo n &quot;My first blog&quot;</code>：</strong><br><img src="https://img-blog.csdnimg.cn/20200211232614762.png" alt="创建博客"></p>
</li>
<li><p><strong>会在hexo站点文件夹下的 <code>source/_posts/</code>路径下创建对应名的.md文档，使用Markdown语法编辑此文档，然后再次创建博客页面，上传到Github即可实现博客发布：</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g <span class="comment"># 或者 hexo generate</span></span><br><span class="line">hexo d <span class="comment"># 或者 hexo deploy</span></span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h2 id="更换博客主题"><a href="#更换博客主题" class="headerlink" title="更换博客主题"></a>更换博客主题</h2><h3 id="1-下载主题"><a href="#1-下载主题" class="headerlink" title="1. 下载主题"></a>1. 下载主题</h3><ul>
<li><p><strong>去Github检索相关主题，当然也可以去官方主题市场进行下载，这里选择Github上的<a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank" rel="noopener"> material-indigo </a>主题作为例。</strong></p>
</li>
<li><p><strong>使用git bash执行<code>git clone</code>命令，将工程克隆到之前创建的博客文件夹下的<code>themes/indigo</code>路径下：</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/yscoder/hexo-theme-indigo.git themes/indigo</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>若clone速度太慢可以尝试修改hosts文件或者其他方式，这里就不多赘述</strong></p>
</li>
</ul>
<h3 id="2-配置主题"><a href="#2-配置主题" class="headerlink" title="2. 配置主题"></a>2. 配置主题</h3><ul>
<li><p><strong>修改博客站点文件夹下的<code>_config.yml</code>文件，将其中的<code>theme</code>参数设置成新下载的主题名：</strong><br>  <img src="https://img-blog.csdnimg.cn/20200211232745382.png" alt="配置主题"></p>
</li>
<li><p><strong>然后依旧是重新生成博客页面<code>hexo clean</code> <code>hexo g</code>，开启<code>hexo s</code>开启服务，在本地查看是否配置成功：</strong><br>  <img src="https://img-blog.csdnimg.cn/20200211232825846.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="本地配置检查"></p>
</li>
<li><p><strong>最后便可以使用<code>hexo d</code>命令上传至Github，实现博客发布</strong></p>
</li>
<li><p><strong>具体博客主题相关配置参考对应主题的官方文档</strong></p>
</li>
</ul>
<h2 id="PS：若生成的博客页面存在功能性故障，记得在浏览器浏览页面时按F12进入调试模式，便于定位故障原因"><a href="#PS：若生成的博客页面存在功能性故障，记得在浏览器浏览页面时按F12进入调试模式，便于定位故障原因" class="headerlink" title="PS：若生成的博客页面存在功能性故障，记得在浏览器浏览页面时按F12进入调试模式，便于定位故障原因"></a>PS：若生成的博客页面存在功能性故障，记得在浏览器浏览页面时按F12进入调试模式，便于定位故障原因</h2><hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>个人博客搭建</category>
      </categories>
      <tags>
        <tag>Blog</tag>
        <tag>Hexo</tag>
        <tag>Github</tag>
      </tags>
  </entry>
  <entry>
    <title>IDEA打包普通Java项目（Maven通用）</title>
    <url>/2020/02/10/IDEA%E6%89%93%E5%8C%85%E6%99%AE%E9%80%9AJava%E9%A1%B9%E7%9B%AE%EF%BC%88Maven%E9%80%9A%E7%94%A8%EF%BC%89/</url>
    <content><![CDATA[<hr>
<a id="more"></a>

<hr>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul>
<li><strong>此方式通过在当前Project下创建Artifact来实现项目打包</strong></li>
<li><strong>对于普通Java项目，此方式为通用的Java项目打包方式</strong></li>
<li><strong>对于Maven项目，和使用maven-compiler-plugin和maven-assembly-plugin等插件打包相比，优点：可以手动控制打包内容，缺点：当项目有新的依赖添加时也需要手动将依赖加入Artifact中或者新建Artifact并自动导入依赖</strong></li>
</ul>
<hr>
<h2 id="具体步骤"><a href="#具体步骤" class="headerlink" title="具体步骤"></a>具体步骤</h2><h3 id="1）打开Project-Structure"><a href="#1）打开Project-Structure" class="headerlink" title="1）打开Project Structure"></a>1）打开Project Structure</h3><p><strong>通过File-Project Structure，或者通过工具栏直接打开，或者使用快捷键Ctrl+Alt+Shift+S，都可以</strong></p>
<p><img src="https://img-blog.csdnimg.cn/20200308103538209.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="打开Project Structure"></p>
<h3 id="2）创建Artifact"><a href="#2）创建Artifact" class="headerlink" title="2）创建Artifact"></a>2）创建Artifact</h3><h4 id="a）打包项目依赖"><a href="#a）打包项目依赖" class="headerlink" title="a）打包项目依赖"></a>a）打包项目依赖</h4><p><strong>在Project Settings中找到Artifact，点击加号“+”添加Artifact，依次选择JAR——From modules with dependencies…</strong></p>
<p><img src="https://img-blog.csdnimg.cn/20200308103632494.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="创建Artifact"></p>
<p><strong>选择需要打包的Module，并设置Main Class，其余选项默认即可。注意：如果只是作为工具包使用或者包中有多个启动类就不要设置Main Class，在使用jar包时，通过设置-cp（同-classpath参数），然后附上全类名FQCN指定启动类即可</strong></p>
<p><img src="https://img-blog.csdnimg.cn/20200308103701739.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="创建Artifact"></p>
<h4 id="b）不打包项目依赖"><a href="#b）不打包项目依赖" class="headerlink" title="b）不打包项目依赖"></a>b）不打包项目依赖</h4><p><strong>在Project Settings中找到Artifact，点击加号“+”添加Artifact，依次选择JAR——Empty</strong><br><img src="https://img-blog.csdnimg.cn/2020030810372987.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="创建Artifact"></p>
<p><strong>可以自定义设置Artifact名字、jar包输出路径、jar包名称。然后创建MANIFEST.MF文件，可以通过此文件指明主类</strong></p>
<p><img src="https://img-blog.csdnimg.cn/20200308103748452.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="创建Artifact"></p>
<p><strong>将工程编译输出文件添加至Artifact中，按需指定jar包主类全类名</strong></p>
<p><img src="https://img-blog.csdnimg.cn/20200308103846873.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="添加文件"></p>
<p><strong>按需提取工程使用的依赖到Artifact中，用于之后打包</strong></p>
<p><img src="https://img-blog.csdnimg.cn/20200308103911438.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="添加依赖"></p>
<p><strong>点击Apply保存Artifact</strong></p>
<h3 id="3）打包项目"><a href="#3）打包项目" class="headerlink" title="3）打包项目"></a>3）打包项目</h3><p><strong>依次点击Build——Build Artifact</strong></p>
<p><img src="https://img-blog.csdnimg.cn/20200308103925767.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="Build Artifact"><br><img src="https://img-blog.csdnimg.cn/20200308103956393.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="Build Artifact"></p>
<p><strong>Build完成之后会在之前设定的输出路径下生成jar包</strong></p>
<hr>
<h3 id="4）运行jar包"><a href="#4）运行jar包" class="headerlink" title="4）运行jar包"></a>4）运行jar包</h3><h4 id="a）已设置主类"><a href="#a）已设置主类" class="headerlink" title="a）已设置主类"></a>a）已设置主类</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">java -jar JavaStudyProject.jar</span><br></pre></td></tr></table></figure>

<h4 id="b）未设置主类"><a href="#b）未设置主类" class="headerlink" title="b）未设置主类"></a>b）未设置主类</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">java -cp JavaStudyProject.jar TestCode.HelloWorld</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>IDEA</category>
      </categories>
      <tags>
        <tag>IDEA</tag>
        <tag>Java</tag>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title>IDEA打包Maven项目</title>
    <url>/2020/02/10/IDEA%E6%89%93%E5%8C%85Maven%E9%A1%B9%E7%9B%AE/</url>
    <content><![CDATA[<hr>
<a id="more"></a>

<hr>
<h2 id="1-在pom-xml文件中添加配置"><a href="#1-在pom-xml文件中添加配置" class="headerlink" title="1. 在pom.xml文件中添加配置"></a>1. 在pom.xml文件中添加配置</h2><figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;!--编译打包插件--&gt;</span><br><span class="line">    &lt;build&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">        	&lt;!--配置Maven项目compiler插件--&gt;</span><br><span class="line">        	&lt;!--此工具不会打包依赖--&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.3.2&lt;/version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;/source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;/target&gt;</span><br><span class="line">                &lt;/configuration&gt;</span><br><span class="line">            &lt;/plugin&gt;</span><br><span class="line">            </span><br><span class="line">            &lt;!--配置Maven项目assembly插件--&gt;</span><br><span class="line">            &lt;!--此工具会将全部依赖打包--&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;descriptorRefs&gt;</span><br><span class="line">                        &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;</span><br><span class="line">                    &lt;/descriptorRefs&gt;</span><br><span class="line">                    &lt;archive&gt;</span><br><span class="line">                        &lt;manifest&gt;</span><br><span class="line">                        	&lt;!--&gt;此处设置成主类的类全名&lt;--&gt;</span><br><span class="line">                            &lt;mainClass&gt;com.TomAndersen.AppMain&lt;/mainClass&gt;</span><br><span class="line">                        &lt;/manifest&gt;</span><br><span class="line">                    &lt;/archive&gt;</span><br><span class="line">                &lt;/configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;make-assembly&lt;/id&gt;</span><br><span class="line">                        &lt;phase&gt;package&lt;/phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;single&lt;/goal&gt;</span><br><span class="line">                        &lt;/goals&gt;</span><br><span class="line">                    &lt;/execution&gt;</span><br><span class="line">                &lt;/executions&gt;</span><br><span class="line">            &lt;/plugin&gt;</span><br><span class="line">        &lt;/plugins&gt;</span><br><span class="line">    &lt;/build&gt;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="2-使用Maven项目工具打包程序"><a href="#2-使用Maven项目工具打包程序" class="headerlink" title="2. 使用Maven项目工具打包程序"></a>2. 使用Maven项目工具打包程序</h2><ul>
<li><p><strong>使用Maven—Lifecycle—package组件</strong></p>
<p>  <img src="https://img-blog.csdnimg.cn/20200210105333758.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="使用package组件"></p>
</li>
<li><p><strong>控制台输出</strong><br><img src="https://img-blog.csdnimg.cn/20200210105102266.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="控制台输出"></p>
</li>
</ul>
<hr>
<h2 id="3-生成jar包"><a href="#3-生成jar包" class="headerlink" title="3. 生成jar包"></a>3. 生成jar包</h2><ul>
<li><strong>打包成功之后会在<code>src</code>同级目录下的<code>target</code>文件夹中生成带依赖和不带依赖的jar包，包名前缀与<code>artifactId</code>相同</strong><br><img src="https://img-blog.csdnimg.cn/20200210105816470.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="生成两个jar包"></li>
<li><strong>注意 1：如果使用不带依赖的jar包程序，则程序执行时依赖于本地的运行环境</strong></li>
<li><strong>注意 2：如果使用带依赖的jar包程序，则程序执行时不依赖本地的运行环境，而只依赖于开发环境</strong></li>
</ul>
<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>IDEA</category>
      </categories>
      <tags>
        <tag>IDEA</tag>
        <tag>Java</tag>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title>VMware中搭建Hadoop集群简易步骤（文字版）</title>
    <url>/2020/02/09/VMware%E4%B8%AD%E6%90%AD%E5%BB%BAHadoop%E9%9B%86%E7%BE%A4%E7%AE%80%E6%98%93%E6%AD%A5%E9%AA%A4%EF%BC%88%E6%96%87%E5%AD%97%E7%89%88%EF%BC%89/</url>
    <content><![CDATA[<hr>
<a id="more"></a>

<hr>
<h2 id="所用各工具版本："><a href="#所用各工具版本：" class="headerlink" title="所用各工具版本："></a>所用各工具版本：</h2><ul>
<li>系统镜像：CentOS-7-x86_64-DVD-1810</li>
<li>VM version：VMware Workstation Pro15</li>
<li>Java version：jdk-8u221-linux-x64</li>
<li>Hadoop version：hadoop-2.7.7</li>
</ul>
<hr>
<h2 id="准备步骤："><a href="#准备步骤：" class="headerlink" title="准备步骤："></a>准备步骤：</h2><ul>
<li>下载CentOS-7镜像、下载匹配的Sun JDK包、下载匹配的Hadoop包</li>
<li>将Vmware中的VMnet8网络设置成“NAT模式”，设置子网IP、子网掩码、网关</li>
</ul>
<hr>
<h2 id="集群规划："><a href="#集群规划：" class="headerlink" title="集群规划："></a>集群规划：</h2><ul>
<li>集群搭建之前，应该进行 <strong>集群规划</strong> ，文件中的某些配置需要基于对集群的规划，进行配置之前，需要先计划好Hadoop中的各个组件服务器应该搭载在哪台主机上，实现 <strong>负载均衡</strong>，避免由于宕机而造成不可逆损失，<strong>集群规划</strong> 是搭建分布式环境之前的最重要步骤之一，本次实验中具体规划如下：</li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th>hadoop101</th>
<th>hadoop102</th>
<th>hadoop103</th>
</tr>
</thead>
<tbody><tr>
<td><strong>HDFS</strong></td>
<td>NameNode <br> DateNode</td>
<td>DateNode</td>
<td>DateNode <br> SecondaryNameNode</td>
</tr>
<tr>
<td><strong>YARN</strong></td>
<td>NodeManager</td>
<td>ResourceManager <br> NodeManager</td>
<td>NodeManager</td>
</tr>
</tbody></table>
<hr>
<h2 id="正式步骤："><a href="#正式步骤：" class="headerlink" title="正式步骤："></a>正式步骤：</h2><h3 id="1-安装系统镜像"><a href="#1-安装系统镜像" class="headerlink" title="1.    安装系统镜像"></a>1.    安装系统镜像</h3><ul>
<li><strong>“软件安装” 选择“Server with GUI”</strong>，这样就具备了如ssh、UI界面等一系列基本工具，<strong>“磁盘划分”</strong> 选择默认或者自己手动分区都可，自定义最好</li>
</ul>
<h3 id="2-创建root用户和普通用户"><a href="#2-创建root用户和普通用户" class="headerlink" title="2.    创建root用户和普通用户"></a>2.    创建root用户和普通用户</h3><ul>
<li>注意<strong>普通用户的用户名必须要全部小写</strong>，本次创建的用户名为<code>tomandersen</code></li>
</ul>
<h3 id="3-安装Vmware-Tools"><a href="#3-安装Vmware-Tools" class="headerlink" title="3.    安装Vmware Tools"></a>3.    安装Vmware Tools</h3><ul>
<li>打开桌面自带的磁盘镜像，解压tar.gz到桌面，执行文件夹中<code>vmware-install.pl</code>文件，安装完成后，在Linux系统中调整分辨率到合适配置</li>
</ul>
<h3 id="4-添加普通用户sudo权限并实现免密使用sudo命令"><a href="#4-添加普通用户sudo权限并实现免密使用sudo命令" class="headerlink" title="4.    添加普通用户sudo权限并实现免密使用sudo命令"></a>4.    添加普通用户sudo权限并实现免密使用sudo命令</h3><ul>
<li><strong>方法1：</strong> 使用 <strong>visudo命令</strong> 或者直接手动修改 <strong>/etc/sudoers配置文件</strong>，在<code>root ALL=(ALL) ALL</code>后方添加设置，格式为 <code>&lt;用户名&gt;    ALL=(ALL) NOPASSWD:ALL</code>，如<code>tomandersen    ALL=(ALL)       NOPASSWD:ALL</code>即可实现指定用户使用sudo命令无需密码，也可以使用命令“sudo -s”实现无密码登录root用户。</li>
<li><strong>方法2：</strong> 或者也可以使用实现组内用户全部免密使用sudo命令：①创建名为hadoop的用户组，②将用户添加到hadoop组，③将root组成员设置成全部免密：格式为<code>%&lt;组名&gt;    ALL=(ALL) NOPASSWD:ALL</code>，如<code>%hadoop    ALL=(ALL) NOPASSWD:ALL</code></li>
</ul>
<h3 id="5-修改网卡名（可以不修改）"><a href="#5-修改网卡名（可以不修改）" class="headerlink" title="5.    修改网卡名（可以不修改）"></a>5.    修改网卡名（可以不修改）</h3><ul>
<li><a href="https://blog.csdn.net/TomAndersen/article/details/104188808" target="_blank" rel="noopener">CentOS 7修改网卡名</a></li>
</ul>
<h3 id="6-设置静态IP："><a href="#6-设置静态IP：" class="headerlink" title="6.    设置静态IP："></a>6.    设置静态IP：</h3><ul>
<li><a href="https://blog.csdn.net/TomAndersen/article/details/104189273" target="_blank" rel="noopener">CentOS 7设置网卡静态IP</a></li>
</ul>
<h3 id="7-修改主机名："><a href="#7-修改主机名：" class="headerlink" title="7.    修改主机名："></a>7.    修改主机名：</h3><ul>
<li><strong>方法1：</strong> 可以手动编辑 <strong>/etc/sysconfig/network文件</strong>，添加或者设置<code>NETWORKING=yes</code>和<code>HOSTNAME=&lt;新主机名&gt;</code>如<code>HOSTNAME=hadoop101</code>，<strong>重启生效</strong></li>
<li><strong>方法2：</strong> 使用命令格式为<code>hostnamectl set-hostname &lt;新主机名&gt;</code>，如<code>hostnamectl set-hostname hadoop101</code>，<strong>立即生效</strong></li>
<li><strong>方法3：</strong> 修改/etc/hostname文件，直接添加进这个文件的字符串即为主机名，如<code>hadoop101</code>，<strong>重启生效</strong></li>
</ul>
<h3 id="8-配置-etc-hosts文件"><a href="#8-配置-etc-hosts文件" class="headerlink" title="8.    配置/etc/hosts文件"></a>8.    配置/etc/hosts文件</h3><ul>
<li>添加集群中各个主机IP地址与主机名的映射关系，如<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="number">192.168</span><span class="number">.126</span><span class="number">.101</span> hadoop101</span><br><span class="line"><span class="number">192.168</span><span class="number">.126</span><span class="number">.102</span> hadoop102</span><br><span class="line"><span class="number">192.168</span><span class="number">.126</span><span class="number">.103</span> hadoop103</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="9-禁用防火墙-firewalld"><a href="#9-禁用防火墙-firewalld" class="headerlink" title="9.    禁用防火墙 firewalld"></a>9.    禁用防火墙 firewalld</h3><ul>
<li><code>systemctl disable firewalld.service</code></li>
<li>注意：在CentOS 7或RHEL 7或Fedora中防火墙由firewalld来管理</li>
</ul>
<h3 id="10-在-opt-下创建module和software文件夹并将所属权赋给之前创建的普通用户"><a href="#10-在-opt-下创建module和software文件夹并将所属权赋给之前创建的普通用户" class="headerlink" title="10.    在/opt/下创建module和software文件夹并将所属权赋给之前创建的普通用户"></a>10.    在/opt/下创建module和software文件夹并将所属权赋给之前创建的普通用户</h3><ul>
<li>创建文件夹：<code>sudo mkdir module</code>和<code>sudo mkdir software</code></li>
<li>改变文件夹所属组和所属用户：<code>sudo chown tomandersen:tomandersen  module/ software/</code></li>
<li>以后相关大数据组件都存放于这两个路径下，便于管理<h3 id="11-使用XShell工具远程连接主机Hadoop101将JDK和Hadoop包传入到-opt-software-路径下"><a href="#11-使用XShell工具远程连接主机Hadoop101将JDK和Hadoop包传入到-opt-software-路径下" class="headerlink" title="11.    使用XShell工具远程连接主机Hadoop101将JDK和Hadoop包传入到/opt/software/路径下"></a>11.    使用XShell工具远程连接主机Hadoop101将JDK和Hadoop包传入到/opt/software/路径下</h3></li>
<li>可以在XShell中使用<code>rz</code>命令，或者直接拖拽文件到对应区域</li>
</ul>
<h3 id="12-卸载现有JDK"><a href="#12-卸载现有JDK" class="headerlink" title="12.    卸载现有JDK"></a>12.    卸载现有JDK</h3><ul>
<li>使用yum命令查看是否安装有JDK：<code>sudo yum list installed | grep java</code>，然后将jdk相关包删除：<code>sudo yum remove &lt;包名&gt;</code></li>
<li>自带的是Open JDK，这里我们使用Sun JDK</li>
</ul>
<h3 id="13-重装JDK"><a href="#13-重装JDK" class="headerlink" title="13.    重装JDK"></a>13.    重装JDK</h3><ul>
<li>将<code>/opt/software/</code>路径下的JDK解压到<code>/opt/module/</code>路径下：<code>tar -zxvf jdk-8u221-linux-x64.tar.gz -C /opt/module/</code>（因为解压目录不在当前路径下所以使用tar命令需要增加-C参数）</li>
<li>设置和添加全局环境变量<code>JAVA_HOME</code>和<code>PATH</code>：使用sudo+vi命令修改<code>/etc/profile</code>文件，在末尾添加<code>export JAVA_HOME=/opt/module/jdk1.8.0_221</code>和<code>export PATH=$PATH:$JAVA_HOME/bin</code></li>
<li>重新加载/etc/profile文件使其生效：<code>source /etc/profile</code>，检查环境变量JAVA_HOME：<code>which java</code></li>
</ul>
<h3 id="14-安装Hadoop"><a href="#14-安装Hadoop" class="headerlink" title="14.    安装Hadoop"></a>14.    安装Hadoop</h3><ul>
<li>同样将<code>/opt/software/</code>路径下的Hadoop包解压到<code>/opt/module/</code>路径下：<code>tar -xzvf hadoop-2.7.7.tar.gz -C /opt/module/</code></li>
<li>设置和添加全局环境变量<code>HADOOP_HOME</code>和<code>PATH</code>：使用sudo+vi命令修改/etc/profile文件，在末尾添加<code>export HADOOP_HOME=/opt/module/hadoop-2.7.7</code>、<code>export PATH=$PATH:$HADOOP_HOME/bin</code></li>
<li>重新加载/etc/profile文件使其生效：<code>source /etc/profile</code>，检查是否安装成功：<code>hadoop version</code></li>
</ul>
<h3 id="15-配置Hadoop"><a href="#15-配置Hadoop" class="headerlink" title="15.    配置Hadoop"></a>15.    配置Hadoop</h3><ul>
<li>进入<code>hadoop-2.7.7/etc/hadoop</code>路径下配置相关文件，具体参考<a href="https://blog.csdn.net/TomAndersen/article/details/104220449" target="_blank" rel="noopener">Hadoop集群极简入门基础配置教程</a></li>
</ul>
<h3 id="16-完全拷贝虚拟机并且更改新其他主机名和IP"><a href="#16-完全拷贝虚拟机并且更改新其他主机名和IP" class="headerlink" title="16.    完全拷贝虚拟机并且更改新其他主机名和IP"></a>16.    完全拷贝虚拟机并且更改新其他主机名和IP</h3><ul>
<li>完全拷贝虚拟机<code>hadoop101</code>，生成<code>hadoop102</code>和<code>hadoop103</code>，这样<code>hadoop101</code>的配置信息也一并同步到了其他主机中</li>
<li>在<code>hadoop102</code>和<code>hadoop103</code>中将主机名分别设置成对应主机名：<code>hostnamectl set-hostname hadoop102</code>和<code>hostnamectl set-hostname hadoop103</code></li>
<li>分别修改主机IP地址，本次实验中<code>hadoop102</code>和<code>hadoop103</code>地址分别设置为<code>192.168.126.102</code>和<code>192.168.126.103</code>，然后重启网卡更新IP地址</li>
</ul>
<h3 id="17-设置各主机间ssh免密登录"><a href="#17-设置各主机间ssh免密登录" class="headerlink" title="17.    设置各主机间ssh免密登录"></a>17.    设置各主机间ssh免密登录</h3><ul>
<li><a href="https://blog.csdn.net/TomAndersen/article/details/104227687" target="_blank" rel="noopener">实现主机之间ssh免密登录</a></li>
</ul>
<h3 id="18-使用ntp进行集群时间同步"><a href="#18-使用ntp进行集群时间同步" class="headerlink" title="18.    使用ntp进行集群时间同步"></a>18.    使用ntp进行集群时间同步</h3><ul>
<li><a href="https://blog.csdn.net/TomAndersen/article/details/104239795" target="_blank" rel="noopener">Linux使用ntp工具实现集群主机间时间同步</a></li>
</ul>
<h3 id="19-格式化NameNode节点"><a href="#19-格式化NameNode节点" class="headerlink" title="19.    格式化NameNode节点"></a>19.    格式化NameNode节点</h3><ul>
<li>在第一次启动集群之前需要格式化NameNode节点<code>hdfs namenode -format</code>或者<code>hadoop namenode -format</code>，建议用前者，后者命令将被弃用</li>
<li>如果不是第一次格式化NameNode，需要事先删除<code>tmp</code>和<code>logs</code>文件夹</li>
</ul>
<h3 id="20-启动HDFS集群"><a href="#20-启动HDFS集群" class="headerlink" title="20.    启动HDFS集群"></a>20.    启动HDFS集群</h3><ul>
<li>使用<code>hadoop-2.7.7/sbin/</code>路径下的<code>start-dfs.sh</code>脚本启动HDFS集群，在节点上使用<code>jps</code>命令查看Java进程，观察进程启动情况是否和集群规划中的匹配</li>
<li><strong>注意：只能在NameNode主机上启动HDFS集群，否则无法启动NameNode进程，而只能启动DataNode</strong></li>
</ul>
<h3 id="21-启动YARN集群："><a href="#21-启动YARN集群：" class="headerlink" title="21.    启动YARN集群："></a>21.    启动YARN集群：</h3><ul>
<li>使用<code>hadoop-2.7.7/sbin/</code>路径下的<code>start-yarn.sh</code>脚本启动YARN集群，在节点上使用<code>jps</code>命令查看Java进程，观察进程启动情况是否和集群规划中的匹配</li>
<li><strong>注意：只能在ResourceManager主机上启动YARN集群，否则无法启动ResourceManager进程，而只能启动NodeManager</strong></li>
</ul>
<h3 id="22-运行测试："><a href="#22-运行测试：" class="headerlink" title="22.    运行测试："></a>22.    运行测试：</h3><ul>
<li>使用Hadoop自带的例子测试集群是否搭建成功，观察运行结果：</li>
<li><code>hadoop jar /opt/module/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar pi 10 10</code></li>
</ul>
<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>CentOS7</tag>
        <tag>大数据</tag>
        <tag>Hadoop</tag>
        <tag>VMware</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux使用ntp工具实现集群主机间时间同步</title>
    <url>/2020/02/09/Linux%E4%BD%BF%E7%94%A8ntp%E5%B7%A5%E5%85%B7%E5%AE%9E%E7%8E%B0%E9%9B%86%E7%BE%A4%E4%B8%BB%E6%9C%BA%E9%97%B4%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5/</url>
    <content><![CDATA[<hr>
<a id="more"></a>

<hr>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul>
<li><strong>NTP（Network Time Protocol，网络时间协议）是用来使计算机时间同步化的一种协议，它可以使计算机对其服务器或时钟源（如石英钟，GPS等等)做同步化，它可以提供高精准度的时间校正（LAN上与标准间差小于1毫秒，WAN上几十毫秒），且可介由加密确认的方式来防止恶毒的协议攻击。NTP的目的是在无序的Internet环境中提供精确和健壮的时间服务</strong></li>
<li><strong>Linux上可以通过ntp工具来实现各个主机间的系统时间和硬件时间同步</strong></li>
<li><strong>本次实验集群主机系统为CentOS 7，集群主机在 192.168.126.0/24 网段</strong></li>
</ul>
<hr>
<h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><ul>
<li><strong>实现集群内主机时间同步</strong></li>
</ul>
<hr>
<h2 id="具体步骤："><a href="#具体步骤：" class="headerlink" title="具体步骤："></a>具体步骤：</h2><h3 id="1-配置集群时间服务器"><a href="#1-配置集群时间服务器" class="headerlink" title="1. 配置集群时间服务器"></a>1. 配置集群时间服务器</h3><h4 id="（1）检查-ntp-是否安装"><a href="#（1）检查-ntp-是否安装" class="headerlink" title="（1）检查 ntp 是否安装"></a>（1）检查 ntp 是否安装</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop101 ~]$ yum list installed | grep ntp</span><br><span class="line">fontpackages-filesystem.noarch          1.44-8.el7                     @anaconda</span><br><span class="line">ntp.x86_64                              4.2.6p5-29.el7.centos          @base    </span><br><span class="line">ntpdate.x86_64                          4.2.6p5-29.el7.centos          @base    </span><br><span class="line">python-ntplib.noarch                    0.3.2-1.el7                    @anaconda</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200209201010501.png" alt="检查 ntp 是否安装"></p>
<ul>
<li><strong>若未安装则进行安装</strong></li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop101 ~]$ sudo yum install ntp</span><br></pre></td></tr></table></figure>
<h4 id="（2）修改-ntp-配置文件-etc-ntp-conf"><a href="#（2）修改-ntp-配置文件-etc-ntp-conf" class="headerlink" title="（2）修改 ntp 配置文件 /etc/ntp.conf"></a>（2）修改 ntp 配置文件 /etc/ntp.conf</h4><ul>
<li><strong>必须使用root用户执行命令</strong></li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">[root@hadoop101 ~]<span class="meta"># vim /etc/ntp.conf</span></span><br></pre></td></tr></table></figure>
<p><strong>修改内容如下：</strong></p>
<ul>
<li><strong>a）授权当前网段内所有主机可通过此主机查询和同步时间</strong></li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">restrict 192.168.126.0 mask 255.255.255.0 nomodify notrap</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>b）设置无网络连接时使用本地时间为集群机器提供时间同步</strong><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">server 127.127.1.0</span><br><span class="line">fudge 127.127.1.0 stratum 10</span><br></pre></td></tr></table></figure></li>
<li><strong>具体如下图所示</strong></li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/20200209215655224.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="修改 ntp 配置文件 /etc/ntp.conf"></p>
<ul>
<li><strong>注意：若要关闭ntp网络时间同步，将默认的公共服务器server 0~server 3全都注释即可</strong></li>
</ul>
<h4 id="（3）修改-ntp-配置文件-etc-sysconfig-ntpd"><a href="#（3）修改-ntp-配置文件-etc-sysconfig-ntpd" class="headerlink" title="（3）修改 ntp 配置文件 /etc/sysconfig/ntpd"></a>（3）修改 ntp 配置文件 /etc/sysconfig/ntpd</h4><ul>
<li><strong>使用root用户编辑文件/etc/sysconfig/ntpd，插入以下内容实现硬件时间和系统时间一起同步</strong><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SYNC_HWCLOCK=yes</span><br></pre></td></tr></table></figure>
<img src="https://img-blog.csdnimg.cn/20200209204232789.png" alt="实现硬件时间和系统时间一起同步"><h4 id="（4）重启-ntp-服务"><a href="#（4）重启-ntp-服务" class="headerlink" title="（4）重启 ntp 服务"></a>（4）重启 ntp 服务</h4><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">[root@hadoop101 /]<span class="meta"># systemctl restart ntpd.service</span></span><br></pre></td></tr></table></figure>
<h4 id="（5）设置-ntp-服务开机自启"><a href="#（5）设置-ntp-服务开机自启" class="headerlink" title="（5）设置 ntp 服务开机自启"></a>（5）设置 ntp 服务开机自启</h4><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">[root@hadoop101 /]<span class="meta"># systemctl enable ntpd.service</span></span><br></pre></td></tr></table></figure>
<h3 id="2-配置所有其他客户主机"><a href="#2-配置所有其他客户主机" class="headerlink" title="2. 配置所有其他客户主机"></a>2. 配置所有其他客户主机</h3><h4 id="（1）创建时间同步计划任务"><a href="#（1）创建时间同步计划任务" class="headerlink" title="（1）创建时间同步计划任务"></a>（1）创建时间同步计划任务</h4></li>
<li><strong>在其他主机上使用 crontab工具 创建计划任务，设置每1分钟同步一次时间</strong></li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">[root@hadoop102 ~]<span class="meta"># crontab -e</span></span><br></pre></td></tr></table></figure>

<ul>
<li><strong>插入以下内容，不能有多余空格</strong></li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">*/1 * * * * /usr/sbin/ntpdate hadoop101</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>其他 crontab 使用可以查看：</strong><a href="https://blog.csdn.net/TomAndersen/article/details/104237945" target="_blank" rel="noopener">Linux中使用crond工具创建定时任务</a><h4 id="（2）确保-crond-service-和-ntpd-service开机启动"><a href="#（2）确保-crond-service-和-ntpd-service开机启动" class="headerlink" title="（2）确保 crond.service 和 ntpd.service开机启动"></a>（2）确保 crond.service 和 ntpd.service开机启动</h4></li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo systemctl <span class="built_in">enable</span> crond.service</span><br><span class="line">sudo systemctl <span class="built_in">enable</span> ntpd.service</span><br></pre></td></tr></table></figure>

<h3 id="3-测试同步结果"><a href="#3-测试同步结果" class="headerlink" title="3. 测试同步结果"></a>3. 测试同步结果</h3><ul>
<li><strong>使用 date 命令修改任意客户主机时间</strong></li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">[root@hadoop103 TomAndersen]<span class="meta"># date -s <span class="meta-string">"2017-9-11 11:11:11"</span></span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>1分钟之后检查时间是否自动同步成功</strong><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">[root@hadoop103 TomAndersen]<span class="meta"># date</span></span><br><span class="line"><span class="number">2020</span>年 <span class="number">02</span>月 <span class="number">09</span>日 星期日 <span class="number">21</span>:<span class="number">51</span>:<span class="number">11</span> CST</span><br></pre></td></tr></table></figure>
</li>
</ul>
<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CentOS7</tag>
        <tag>Linux</tag>
        <tag>ntp</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux中使用crond工具创建定时任务</title>
    <url>/2020/02/09/Linux%E4%B8%AD%E4%BD%BF%E7%94%A8crond%E5%B7%A5%E5%85%B7%E5%88%9B%E5%BB%BA%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/</url>
    <content><![CDATA[<hr>
<a id="more"></a>

<hr>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul>
<li><strong>crond是一个linux下的定时执行工具（相当于windows下的scheduled task），可以在无需人工干预的情况下定时地运行任务。crond工具提供crontab命令来设置定时任务，属于守护进程，只能精确到分钟，可以设定周期性执行Linux命令或者Shell脚本，每分钟crond都会检查是否有定时任务需要执行</strong></li>
<li><strong>本次实验系统为CentOS 7</strong></li>
</ul>
<hr>
<h2 id="操作步骤"><a href="#操作步骤" class="headerlink" title="操作步骤"></a>操作步骤</h2><h3 id="1-检查crond工具是否安装"><a href="#1-检查crond工具是否安装" class="headerlink" title="(1) 检查crond工具是否安装"></a>(1) 检查crond工具是否安装</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum list installed | grep crontabs</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/2020020917463039.png" alt="检查crond工具是否安装"></p>
<ul>
<li><strong>若未安装，则使用如下所示命令安装</strong></li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo yum install crontabs</span><br></pre></td></tr></table></figure>

<h3 id="2-检查crond服务是否开启"><a href="#2-检查crond服务是否开启" class="headerlink" title="(2) 检查crond服务是否开启"></a>(2) 检查crond服务是否开启</h3><ul>
<li><strong>由于是CentOS7所以使用 systemctl 命令，而非 service 命令</strong><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl status crond.service</span><br></pre></td></tr></table></figure>
<img src="https://img-blog.csdnimg.cn/20200209174834770.png" alt="检查crond服务是否开启"></li>
<li><strong>若未开启，则使用如下所示命令开启服务</strong><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo systemctl start crond.service</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="3-使用crond工具创建任务计划"><a href="#3-使用crond工具创建任务计划" class="headerlink" title="(3) 使用crond工具创建任务计划"></a>(3) 使用crond工具创建任务计划</h3><ul>
<li><p><strong>crontab命令使用方法</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Usage:</span><br><span class="line"> crontab [options] file</span><br><span class="line"> crontab [options]</span><br><span class="line"> crontab -n [hostname]</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line"> -u &lt;user&gt;  define user</span><br><span class="line"> -e         edit user<span class="string">'s crontab</span></span><br><span class="line"><span class="string"> -l         list user'</span>s crontab</span><br><span class="line"> -r         delete user<span class="string">'s crontab</span></span><br><span class="line"><span class="string"> -i         prompt before deleting</span></span><br><span class="line"><span class="string"> -n &lt;host&gt;  set host in cluster to run users'</span> crontabs</span><br><span class="line"> -c         get host <span class="keyword">in</span> cluster to run users<span class="string">' crontabs</span></span><br><span class="line"><span class="string"> -s         selinux context</span></span><br><span class="line"><span class="string"> -x &lt;mask&gt;  enable debugging</span></span><br><span class="line"><span class="string"> # 注意 crontab -r 是删除用户的所有定时任务(慎用！)</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>可以通过 /etc/crontab 文件查看任务定义格式和设定任务执行环境</strong><br><img src="https://img-blog.csdnimg.cn/20200209181629395.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="任务定义格式"></p>
</li>
<li><p><strong>字段：分（0-59），时（0-23），日（1-31），月（1-12），星期（0-6，其中0和7代表星期天）</strong></p>
</li>
<li><p><strong>星号（*）：代表所有可能的值，例如day字段如果是星号，则表示在满足其它字段的制约条件后每天都执行该命令操作</strong></p>
</li>
<li><p><strong>逗号（,）：可以用逗号隔开的值指定一个列表范围，例如，“1,2,5,7,8,9”，则表示仅在这些时刻都会执行</strong></p>
</li>
<li><p><strong>中杠（-）：可以用整数之间的中杠表示一个整数范围，例如“2-6”表示“2,3,4,5,6”，则表示仅在这些时刻都会执行</strong></p>
</li>
<li><p><strong>正斜杠（/）：可以用正斜线指定时间的间隔频率，例如“0-23/2”表示0~23时范围每两小时执行一次。同时星号可以和正斜杠一起使用，例如*/10，如果用在minute字段，表示每十分钟执行一次</strong></p>
</li>
</ul>
<h4 id="方法1：使用crontab命令编辑当前用户定时任务（立即生效）"><a href="#方法1：使用crontab命令编辑当前用户定时任务（立即生效）" class="headerlink" title="方法1：使用crontab命令编辑当前用户定时任务（立即生效）"></a>方法1：使用crontab命令编辑当前用户定时任务（立即生效）</h4><ul>
<li><p><strong>以“每分钟定时将日期写入指定文件中”为例</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">crontab -e</span><br></pre></td></tr></table></figure></li>
<li><p><strong>在编辑器中插入如下指令（注意此时不要追加用户，否则无法执行，因为此方法是直接设置当前用户的定时任务）</strong></p>
</li>
<li><p><strong>此命令作用是“每分钟结束时将当前时间追加输出到currentDate文件中”，若不加斜杠，则是在每个月每天每小时第1分钟结束时执行此任务。若将 */1 设置成小时，则是每个月每天的每个小时结束时执行任务，后续依据 /etc/crontab 中介绍类推即可</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">*/1 * * * * date &gt;&gt; /home/TomAndersen/currentDate</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>检查插入结果</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop101 bin]$ crontab -l</span><br><span class="line">*/1 * * * * date &gt;&gt; /home/TomAndersen/currentDate</span><br></pre></td></tr></table></figure>
<h4 id="方法2：编辑-etc-crontab-文件，按照格式插入（生效较慢）"><a href="#方法2：编辑-etc-crontab-文件，按照格式插入（生效较慢）" class="headerlink" title="方法2：编辑 /etc/crontab 文件，按照格式插入（生效较慢）"></a>方法2：编辑 /etc/crontab 文件，按照格式插入（生效较慢）</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">*/1 * * * * tomandersen date &gt;&gt; /home/TomAndersen/currentDate</span><br></pre></td></tr></table></figure>
<h3 id="4-检查是否设置成功"><a href="#4-检查是否设置成功" class="headerlink" title="(4) 检查是否设置成功"></a>(4) 检查是否设置成功</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop101 bin]$ cat /home/TomAndersen/currentDate </span><br><span class="line">2020年 02月 09日 星期日 18:12:01 CST</span><br><span class="line">2020年 02月 09日 星期日 18:13:01 CST</span><br><span class="line">2020年 02月 09日 星期日 18:14:01 CST</span><br><span class="line">2020年 02月 09日 星期日 18:15:01 CST</span><br><span class="line">2020年 02月 09日 星期日 18:16:02 CST</span><br><span class="line">2020年 02月 09日 星期日 18:17:01 CST</span><br><span class="line">2020年 02月 09日 星期日 18:18:01 CST</span><br><span class="line">2020年 02月 09日 星期日 18:19:01 CST</span><br><span class="line">2020年 02月 09日 星期日 18:20:01 CST</span><br></pre></td></tr></table></figure>
</li>
</ul>
<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CentOS7</tag>
        <tag>Linux</tag>
        <tag>crond</tag>
      </tags>
  </entry>
  <entry>
    <title>systemctl和service、chkconfig常用命令对比</title>
    <url>/2020/02/09/systemctl%E5%92%8Cservice%E3%80%81chkconfig%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%AF%B9%E6%AF%94/</url>
    <content><![CDATA[<hr>
<a id="more"></a>

<hr>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul>
<li><strong>systemctl 是RHEL 7的系统服务管理器指令，它融合之前 service 和 chkconfig 的功能于一体，是 service 命令和 chkconfig 命令的集合和代替。可以使用它永久性或只在当前会话中启用/禁用服务</strong></li>
<li><strong>CentOS是基于 RHEL (RedHat Enterprise Linux) 的Linux发行版本之一，CentOS 7也是使用systemctl，取代了service 和 chkconfig，在CentOS 7中使用后两个命令时会被重定向到使用等价的 systemctl 命令</strong></li>
<li><strong>以 sshd.service 服务为例</strong></li>
</ul>
<hr>
<h2 id="systemctl-常用命令"><a href="#systemctl-常用命令" class="headerlink" title="systemctl 常用命令"></a>systemctl 常用命令</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">列出所有服务：systemctl list-unit-files --<span class="built_in">type</span>=service</span><br><span class="line">列出服务层级和依赖关系：systemctl list-dependencies sshd.service</span><br><span class="line"></span><br><span class="line">显示服务状态：systemctl status sshd.service</span><br><span class="line">启动服务：systemctl start sshd.service</span><br><span class="line">关闭服务：systemctl stop sshd.service</span><br><span class="line">重启服务：systemctl restart sshd.service</span><br><span class="line"></span><br><span class="line">设置服务开机自启动：systemctl <span class="built_in">enable</span> sshd.service</span><br><span class="line">禁止服务开机自启动：systemctl <span class="built_in">disable</span> sshd.service</span><br><span class="line"></span><br><span class="line">查看服务是否自启动：systemctl is-enabled sshd.service</span><br><span class="line">列出系统所有服务的启动情况：systemctl list-units --<span class="built_in">type</span>=service</span><br><span class="line">列出所有自启动服务：systemctl list-unit-files|grep enabled</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="service、chkconfig-常用命令"><a href="#service、chkconfig-常用命令" class="headerlink" title="service、chkconfig 常用命令"></a>service、chkconfig 常用命令</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">显示服务状态：service sshd status</span><br><span class="line">启动服务：service sshd start</span><br><span class="line">关闭服务：service sshd stop</span><br><span class="line">重启服务：service sshd restart</span><br><span class="line"></span><br><span class="line">设置服务自启动：chkconfig --level 3 sshd on</span><br><span class="line">禁止服务自启动：chkconfig --level 3 sshd off</span><br><span class="line"></span><br><span class="line">查看服务是否自启动：chkconfig --list sshd</span><br><span class="line">列出系统所有服务的启动情况：chkconfig --list</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.cnblogs.com/anliven/p/7944790.html" target="_blank" rel="noopener">Linux - 利用systemctl命令管理服务</a><br><a href="https://blog.csdn.net/qq_38265137/article/details/83081881" target="_blank" rel="noopener">Linux下systemctl命令和service、chkconfig命令的区别</a></p>
<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>CentOS7</tag>
        <tag>Linux</tag>
        <tag>Shell</tag>
        <tag>systemctl</tag>
        <tag>service</tag>
        <tag>chkconfig</tag>
      </tags>
  </entry>
  <entry>
    <title>实现Linux主机之间ssh免密登录</title>
    <url>/2020/02/08/%E5%AE%9E%E7%8E%B0Linux%E4%B8%BB%E6%9C%BA%E4%B9%8B%E9%97%B4ssh%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95/</url>
    <content><![CDATA[<hr>
<a id="more"></a>

<hr>
<h2 id="1-需求："><a href="#1-需求：" class="headerlink" title="1. 需求："></a>1. 需求：</h2><ul>
<li><strong>机器A上已有用户a，想要实现此用户能够免密使用ssh工具登录机器B、C等</strong></li>
</ul>
<hr>
<h2 id="2-命令使用示例："><a href="#2-命令使用示例：" class="headerlink" title="2. 命令使用示例："></a>2. 命令使用示例：</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop101 .ssh]$ ssh-keygen -t rsa</span><br><span class="line">[tomandersen@hadoop101 .ssh]$ ssh-copy-id tomandersen@hadoop102</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="3-具体实现步骤："><a href="#3-具体实现步骤：" class="headerlink" title="3. 具体实现步骤："></a>3. 具体实现步骤：</h2><h3 id="1）创建-ssh文件夹"><a href="#1）创建-ssh文件夹" class="headerlink" title="1）创建.ssh文件夹"></a>1）创建.ssh文件夹</h3><ul>
<li><strong>进入待实现免密登录用户的home目录下，本次实验中为 /home/tomandersen，使用ssh工具连接本机，之后便会在此路径下创建.ssh文件夹</strong></li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop101 ~]$ <span class="built_in">cd</span> ~</span><br><span class="line">[tomandersen@hadoop101 ~]$ ssh localhost</span><br><span class="line">Last login: Sat Feb  8 20:25:27 2020 from localhost</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200208202800447.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="创建.ssh文件夹"></p>
<h3 id="2）生成公钥和私钥"><a href="#2）生成公钥和私钥" class="headerlink" title="2）生成公钥和私钥"></a>2）生成公钥和私钥</h3><ul>
<li><strong>进入.ssh文件夹中，通过 ssh-keygen 命令使用rsa加密算法生成公钥 id_rsa.pub和私钥 id_rsa</strong></li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop101 .ssh]$ ssh-keygen -t rsa</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200208203300779.png" alt="生成公钥和私钥"></p>
<h3 id="3）向指定主机上的用户发送公钥"><a href="#3）向指定主机上的用户发送公钥" class="headerlink" title="3）向指定主机上的用户发送公钥"></a>3）向指定主机上的用户发送公钥</h3><ul>
<li><strong>使用 ssh-copy-id 命令向需要登录上的远程主机的某个用户发送创建的公钥，实际上就是是其用户.ssh目录下的 know_hosts 文件尾部添加公钥密文</strong></li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop101 .ssh]$ ssh-copy-id tomandersen@hadoop102</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200208204106370.png" alt="发送公钥"></p>
<ul>
<li><strong>由于之前已经发送过了，所以显示文件已经存在</strong></li>
</ul>
<h3 id="4）测试是否成功"><a href="#4）测试是否成功" class="headerlink" title="4）测试是否成功"></a>4）测试是否成功</h3><hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Hadoop</tag>
        <tag>ssh</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux系统vi或者vim编辑器中如何显示（关闭）行号</title>
    <url>/2020/02/08/Linux%E7%B3%BB%E7%BB%9Fvi%E6%88%96%E8%80%85vim%E7%BC%96%E8%BE%91%E5%99%A8%E4%B8%AD%E5%A6%82%E4%BD%95%E6%98%BE%E7%A4%BA%EF%BC%88%E5%85%B3%E9%97%AD%EF%BC%89%E8%A1%8C%E5%8F%B7/</url>
    <content><![CDATA[<hr>
<a id="more"></a>

<hr>
<h2 id="临时显示行号"><a href="#临时显示行号" class="headerlink" title="临时显示行号"></a>临时显示行号</h2><ul>
<li>进入vi或者vim编辑器的<strong>底线命令模式</strong>下，输入 <strong>:set nu</strong> 或者 <strong>:set number</strong>，按下回车就显示行号了<br><img src="https://img-blog.csdnimg.cn/20200208181501626.png" alt="临时显示行号"></li>
</ul>
<hr>
<h2 id="每次启动时显示行号"><a href="#每次启动时显示行号" class="headerlink" title="每次启动时显示行号"></a>每次启动时显示行号</h2><p>-使用sudo命令编辑 <strong>/etc/virc</strong> 和  <strong>/etc/vimrc</strong> 文件，在第一行插入 <strong>set nu</strong> 或者 <strong>set number</strong> 命令即可<br><img src="https://img-blog.csdnimg.cn/20200208181342885.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="每次启动时显示行号"></p>
<hr>
<h2 id="临时关闭行号"><a href="#临时关闭行号" class="headerlink" title="临时关闭行号"></a>临时关闭行号</h2><ul>
<li>进入vi或者vim编辑器的<strong>底线命令模式</strong>下，输入 <strong>:set nonu</strong> 或者 <strong>:set nonumber</strong>，按下回车就显示行号了</li>
</ul>
<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CentOS7</tag>
        <tag>Linux</tag>
        <tag>vi/vim</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop集群入门级安装部署教程</title>
    <url>/2020/02/08/Hadoop%E9%9B%86%E7%BE%A4%E5%85%A5%E9%97%A8%E7%BA%A7%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<hr>
<a id="more"></a>

<hr>
<h2 id="集群规划"><a href="#集群规划" class="headerlink" title="集群规划"></a>集群规划</h2><ul>
<li>首先进行 <strong>集群规划</strong> ，文件中的某些配置需要基于对集群的规划，进行配置之前，需要先计划好Hadoop中的各个组件服务器应该搭载在哪台主机上，实现 <strong>负载均衡</strong>，避免由于宕机而造成不可逆损失，<strong>集群规划</strong> 是搭建分布式环境之前的最重要步骤之一，本次实验中具体规划如下：</li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th>hadoop101</th>
<th>hadoop102</th>
<th>hadoop103</th>
</tr>
</thead>
<tbody><tr>
<td><strong>HDFS</strong></td>
<td>NameNode <br> DataNode</td>
<td>DataNode</td>
<td>DataNode <br> SecondaryNameNode</td>
</tr>
<tr>
<td><strong>YARN</strong></td>
<td>NodeManager</td>
<td>ResourceManager <br> NodeManager</td>
<td>NodeManager</td>
</tr>
</tbody></table>
<ul>
<li><strong>操作系统：CentOS-7-x86_64</strong></li>
<li><strong>Hadoop版本：hadoop-2.7.7</strong></li>
</ul>
<hr>
<h2 id="1-核心配置文件"><a href="#1-核心配置文件" class="headerlink" title="1. 核心配置文件"></a>1. 核心配置文件</h2><ul>
<li>配置文件在 <strong>/hadoop-2.7.7/etc/hadoop</strong> 路径下</li>
<li><strong>注意：在进行配置之前需要在 /etc/hosts 文件中建立IP到主机名的映射</strong><h3 id="（1）配置core-site-xml"><a href="#（1）配置core-site-xml" class="headerlink" title="（1）配置core-site.xml"></a>（1）配置core-site.xml</h3></li>
<li>编辑 <strong>core-site.xml</strong> 文件，在configuration中插入配置信息，具体配置如下所示：</li>
</ul>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定HDFS文件系统访问地址,将其设置为NameNode的地址--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;hdfs:/</span><span class="regexp">/hadoop101:9000&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--指定Hadoop运行时产生文件的存储目录--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;property&gt;</span></span><br><span class="line"><span class="regexp">        &lt;name&gt;hadoop.tmp.dir&lt;/</span>name&gt;</span><br><span class="line">        &lt;value&gt;<span class="regexp">/opt/m</span>odule/hadoop<span class="number">-2.7</span><span class="number">.7</span>/tmp&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">&lt;<span class="regexp">/configuration&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="2-HDFS配置文件"><a href="#2-HDFS配置文件" class="headerlink" title="2. HDFS配置文件"></a>2. HDFS配置文件</h2><ul>
<li>配置文件路径同上</li>
</ul>
<h3 id="（1）配置hadoop-env-sh"><a href="#（1）配置hadoop-env-sh" class="headerlink" title="（1）配置hadoop-env.sh"></a>（1）配置hadoop-env.sh</h3><ul>
<li>编辑 <strong>hadoop-env.sh</strong> 文件，将其中的 <strong>JAVA_HOME</strong> 环境变量修改为本机Java安装绝对路径，如：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 修改前：export JAVA_HOME=$&#123;JAVA_HOME&#125;，修改后：</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_221/</span><br></pre></td></tr></table></figure>

<h3 id="（2）配置hdfs-site-xml"><a href="#（2）配置hdfs-site-xml" class="headerlink" title="（2）配置hdfs-site.xml"></a>（2）配置hdfs-site.xml</h3><ul>
<li>编辑 <strong>hdfs-site.xml</strong> 文件，在configuration中插入配置信息，具体配置如下所示：</li>
</ul>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定HDFS副本因子数--&gt;</span><br><span class="line">    &lt;!--由于实验主机磁盘空间不足,本次实验中设置为<span class="number">1</span>,一般需要设置为<span class="number">3</span>--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;1&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--以下是NameNode配置--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;!--指定NameNode节点的Web UI地址--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;property&gt;</span></span><br><span class="line"><span class="regexp">        &lt;name&gt;dfs.namenode.http-address&lt;/</span>name&gt;</span><br><span class="line">        &lt;value&gt;hadoop101:<span class="number">50070</span>&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定NameNode节点上存储name table(fsimage)文件的本地路径--&gt;</span><br><span class="line">    &lt;!--默认值:file:<span class="comment">//$&#123;hadoop.tmp.dir&#125;/dfs/name--&gt;</span></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.name.dir&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;file:/</span><span class="regexp">/$&#123;hadoop.tmp.dir&#125;/</span>dfs/namenode/fsimage&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定NameNode节点上存储transaction(edits)文件的本地路径--&gt;</span><br><span class="line">    &lt;!--默认值:$&#123;dfs.namenode.name.dir&#125;--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.edits.dir&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;file:/</span><span class="regexp">/$&#123;hadoop.tmp.dir&#125;/</span>dfs/namenode/edits&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定DataNode节点上存储Blocks文件的本地路径,此处为修改--&gt;</span><br><span class="line">    &lt;!--默认值:file:<span class="comment">//$&#123;hadoop.tmp.dir&#125;/dfs/data--&gt;</span></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.data.dir&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;file:/</span><span class="regexp">/$&#123;hadoop.tmp.dir&#125;/</span>dfs/datanode/data&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--以下是SecondaryNameNode配置--&gt;</span><br><span class="line">    &lt;!--指定NameNode辅助名称节点SecondaryNameNode的Web UI地址--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.secondary.http-address&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;hadoop103:50090&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--指定SecondaryNameNode节点上存储temporary images文件的本地路径--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;!--默认值:file:/</span><span class="regexp">/$&#123;hadoop.tmp.dir&#125;/</span>dfs/namesecondary--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.checkpoint.dir&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;file:/</span><span class="regexp">/$&#123;hadoop.tmp.dir&#125;/</span>dfs/namesecondary/fsimage&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定SecondaryNameNode节点上存储temporary edits文件的本地路径--&gt;</span><br><span class="line">    &lt;!--默认值:$&#123;dfs.namenode.checkpoint.dir&#125;--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.checkpoint.edits.dir&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;file:/</span><span class="regexp">/$&#123;hadoop.tmp.dir&#125;/</span>dfs/namesecondary/edits&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">&lt;<span class="regexp">/configuration&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="3-YARN配置文件"><a href="#3-YARN配置文件" class="headerlink" title="3. YARN配置文件"></a>3. YARN配置文件</h2><ul>
<li>配置文件路径同上</li>
</ul>
<h3 id="（1）配置yarn-env-sh"><a href="#（1）配置yarn-env-sh" class="headerlink" title="（1）配置yarn-env.sh"></a>（1）配置yarn-env.sh</h3><ul>
<li>编辑 <strong>yarn-env.sh</strong> 文件，将其中的 <strong>JAVA_HOME</strong> 环境变量修改为本机Java安装绝对路径，如：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 修改前：# export JAVA_HOME=/home/y/libexec/jdk1.6.0/，修改后：</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_221/</span><br></pre></td></tr></table></figure>
<h3 id="（2）配置yarn-site-xml"><a href="#（2）配置yarn-site-xml" class="headerlink" title="（2）配置yarn-site.xml"></a>（2）配置yarn-site.xml</h3><ul>
<li>编辑 <strong>yarn-site.xml</strong> 文件，在configuration中插入配置信息，具体配置如下所示：</li>
</ul>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--Site specific YARN configuration properties--&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--设置Reducer获取数据的方式--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;mapreduce_shuffle&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--指定YARN中ResourceManager的ip地址--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;property&gt;</span></span><br><span class="line"><span class="regexp">        &lt;name&gt;yarn.resourcemanager.hostname&lt;/</span>name&gt;</span><br><span class="line">        &lt;value&gt;hadoop102&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--此参数指的是nodemanager的可用内存大小,单位为Mb,设置为主机内存大小--&gt;</span><br><span class="line">    &lt;!--本次实验主机内存大小为<span class="number">2</span>GB,此参数根据各机器分配的物理内存大小设置,若大于物理内存值会影响程序运行效率--&gt;</span><br><span class="line">    &lt;!--默认值:<span class="number">8192</span>--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;2048&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp">    </span></span><br><span class="line"><span class="regexp">&lt;/</span>configuration&gt;</span><br></pre></td></tr></table></figure>
<h2 id="4-MapReduce配置文件"><a href="#4-MapReduce配置文件" class="headerlink" title="4. MapReduce配置文件"></a>4. MapReduce配置文件</h2><ul>
<li>配置文件路径同上</li>
</ul>
<h3 id="（1）配置mapred-env-sh"><a href="#（1）配置mapred-env-sh" class="headerlink" title="（1）配置mapred-env.sh"></a>（1）配置mapred-env.sh</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 修改前：# export JAVA_HOME=/home/y/libexec/jdk1.6.0/，修改后：</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_221/</span><br></pre></td></tr></table></figure>
<h3 id="（2）配置mapred-site-xml"><a href="#（2）配置mapred-site-xml" class="headerlink" title="（2）配置mapred-site.xml"></a>（2）配置mapred-site.xml</h3><ul>
<li>将 <strong>mapred-site.xml.template</strong> 文件复制一份名为 <strong>mapred-site.xml</strong> </li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cp mapred-site.xml.template mapred-site.xml</span><br></pre></td></tr></table></figure>
<ul>
<li>编辑 <strong>mapred-site.xml</strong> 文件，在configuration中插入配置信息，具体配置如下所示：</li>
</ul>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&lt;!-- Put site-specific property overrides <span class="keyword">in</span> <span class="keyword">this</span> file. --&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定MR程序运行框架,设置为YARN上运行,默认是在本地运行--&gt;</span><br><span class="line">    &lt;!--默认值:local--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;yarn&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp">    </span></span><br><span class="line"><span class="regexp">&lt;/</span>configuration&gt;</span><br></pre></td></tr></table></figure>
<h2 id="5-slaves配置文件"><a href="#5-slaves配置文件" class="headerlink" title="5. slaves配置文件"></a>5. slaves配置文件</h2><ul>
<li>配置文件路径同上</li>
</ul>
<h3 id="（1）配置slaves文件"><a href="#（1）配置slaves文件" class="headerlink" title="（1）配置slaves文件"></a>（1）配置slaves文件</h3><ul>
<li>此文件内容主要是用于作为后续集群脚本启动时的参数，此文件中声明的主机共同组成集群，</li>
<li><strong>注意：文件中内容必须一行一个hostname，不允许有任何多余空格，使用主机名配置此文件之前，应该事先在 /etc/hosts 文件中设置好主机IP到主机名的映射</strong>，具体配置如下所示：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hadoop101</span><br><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br></pre></td></tr></table></figure>
<h2 id="6-同步集群配置"><a href="#6-同步集群配置" class="headerlink" title="6. 同步集群配置"></a>6. 同步集群配置</h2><ul>
<li><strong>同步集群中各个主机的配置文件</strong></li>
<li><strong>至此核心配置文件、HDFS、YARN、MapReduce、slaves、hosts共9个文件都已经配置完成，后续可在此基础上增加配置</strong></li>
</ul>
<hr>
<h2 id="7-启动集群测试"><a href="#7-启动集群测试" class="headerlink" title="7. 启动集群测试"></a>7. 启动集群测试</h2><ul>
<li>启动集群之前最好设置 <strong>ssh免密登录</strong>，避免启动过程中需要频繁输入密码<br><a href="https://blog.csdn.net/TomAndersen/article/details/104227687" target="_blank" rel="noopener">实现主机之间ssh免密登录</a></li>
</ul>
<h3 id="（1）格式化NameNode节点"><a href="#（1）格式化NameNode节点" class="headerlink" title="（1）格式化NameNode节点"></a>（1）格式化NameNode节点</h3><ul>
<li>如果是第一次启动，记得使用hdfs命令格式化NameNode节点。如果不是第一次记得先删除<code>tmp</code>和<code>logs</code>文件夹（都在hadoop文件夹内）。格式化命令如下：<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop101 hadoop]$ hdfs namenode -format</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="（2）启动HDFS集群"><a href="#（2）启动HDFS集群" class="headerlink" title="（2）启动HDFS集群"></a>（2）启动HDFS集群</h3><ul>
<li><strong>注意：在哪台主机上配置的NameNode，哪台主机就是HDFS的客户端，只能在那台主机上启动HDFS集群，否则将无法正常启动NameNode，在其他节点上启动HDFS会将其作为DataNode启动，导致集群缺少ResourceManager（关闭时也应如此）。这也是“start-all.sh”等工具被弃用的重要原因之一</strong><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop101 hadoop]$ start-dfs.sh</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="（3）启动YARN集群"><a href="#（3）启动YARN集群" class="headerlink" title="（3）启动YARN集群"></a>（3）启动YARN集群</h3><ul>
<li><strong>注意：在哪台主机上配置的ResourceManager，哪台主机就是YARN的客户端，只能在那台主机上启动YARN集群，否则将无法正常启动ResourceManager，在其他节点上启动YARN会将其作为NodeManager启动，导致集群缺少ResourceManager（关闭时也应如此）</strong><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop102 hadoop]$ start-yarn.sh</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="（4）在各个节点上查看Java进程"><a href="#（4）在各个节点上查看Java进程" class="headerlink" title="（4）在各个节点上查看Java进程"></a>（4）在各个节点上查看Java进程</h3><ul>
<li>使用 <strong>jps命令</strong> 查看Java进程，观察进程角色是否和之前的集群规划相匹配，本次实验中各节点进程状态如下：</li>
<li><strong>hadoop101：</strong><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop101 hadoop]$ jps</span><br><span class="line">23505 NodeManager</span><br><span class="line">23915 NameNode</span><br><span class="line">24270 Jps</span><br></pre></td></tr></table></figure></li>
<li><strong>hadoop102：</strong><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop102 hadoop]$ jps</span><br><span class="line">26327 Jps</span><br><span class="line">25784 NodeManager</span><br><span class="line">25631 ResourceManager</span><br></pre></td></tr></table></figure></li>
<li><strong>hadoop103：</strong><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[tomandersen@hadoop103 hadoop]$ jps</span><br><span class="line">18177 Jps</span><br><span class="line">17699 NodeManager</span><br><span class="line">18093 SecondaryNameNode</span><br></pre></td></tr></table></figure></li>
<li>最后也可以使用Hadoop自带的例子测试运行，此程序是用于计算圆周率</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hadoop jar /opt/module/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar pi 10 10</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>CentOS7</tag>
        <tag>大数据</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>Shell中的括号、方括号、花括号、双括号和双方括号使用场景总结</title>
    <url>/2020/02/07/Shell%E4%B8%AD%E7%9A%84%E6%8B%AC%E5%8F%B7%E3%80%81%E6%96%B9%E6%8B%AC%E5%8F%B7%E3%80%81%E8%8A%B1%E6%8B%AC%E5%8F%B7%E3%80%81%E5%8F%8C%E6%8B%AC%E5%8F%B7%E5%92%8C%E5%8F%8C%E6%96%B9%E6%8B%AC%E5%8F%B7%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<hr>
<a id="more"></a>

<hr>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近在学习Shell脚本编程时，发现别人程序的 <strong>if-then代码块</strong> 中if的条件语句中存在 <strong>双括号(())、双中括号[[]]</strong> 的使用，因而查阅了相关资料，同时也看到了一篇不错的博文，对Shell脚本中括号的使用作出了总结，特此记录和分享</p>
<hr>
<h2 id="1-括号"><a href="#1-括号" class="headerlink" title="1. 括号( )"></a>1. 括号( )</h2><ul>
<li>括号一般在命令替换时使用，与美元符号$配合使用，如</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment"># 输出今年的年份</span></span><br><span class="line">year=$(date +%Y)</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"This year is <span class="variable">$year</span>"</span></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="2-方括号"><a href="#2-方括号" class="headerlink" title="2. 方括号[ ]"></a>2. 方括号[ ]</h2><ul>
<li><strong>Shell中的方括号一般有两种使用场景，一种是和美元符号$搭配用于Shell中整型数据运算；另一种是单独使用，作为test命令的简写形式</strong></li>
</ul>
<h3 id="2-1-搭配美元符号-用于整型计算"><a href="#2-1-搭配美元符号-用于整型计算" class="headerlink" title="2.1 搭配美元符号$用于整型计算"></a>2.1 搭配美元符号$用于整型计算</h3><ul>
<li><strong>示例如下：</strong></li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1. 用于整型数据计算</span></span><br><span class="line">var1=100</span><br><span class="line">var2=200</span><br><span class="line"><span class="comment"># 可以使用 $+双方括号来表示整型运算</span></span><br><span class="line">var3=$[ <span class="variable">$var1</span>+<span class="variable">$var2</span>+1 ]</span><br><span class="line"><span class="comment"># 也可以使用 $+双括号来表示整型运算</span></span><br><span class="line">var4=$((<span class="variable">$var1</span> + <span class="variable">$var2</span> + 1))</span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$var3</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$var4</span></span><br></pre></td></tr></table></figure>



<ul>
<li>由于 <strong>if-then语句</strong> 不能测试命令状态码之外的条件，所以Bash Shell提供了 <strong>test命令</strong> 用于帮助 <strong>if-then语句</strong> 测试其他的条件，如<strong>数值比较、字符串比较、文件比较等</strong>，而test命令的简写形式就是<strong>方括号[  ]</strong>，其中第一个方括号和第二个方括号之前都必须加上空格，否则会报错</li>
</ul>
<h3 id="2-2-数值比较"><a href="#2-2-数值比较" class="headerlink" title="2.2 数值比较"></a>2.2 数值比较</h3><table>
<thead>
<tr>
<th>比较</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>n1 -eq n2</td>
<td>检查n1是否与n2相等</td>
</tr>
<tr>
<td>n1 -ge n2</td>
<td>检查n1是否大于或等于n2</td>
</tr>
<tr>
<td>n1 -gt n2</td>
<td>检查n1是否大于n2</td>
</tr>
<tr>
<td>n1 -le n2</td>
<td>检查n1是否小于或等于n2</td>
</tr>
<tr>
<td>n1 -lt n2</td>
<td>检查n1是否小于n2</td>
</tr>
<tr>
<td>n1 -ne n2</td>
<td>检查n1是否不等于n2</td>
</tr>
</tbody></table>
<ul>
<li><strong>示例如下：</strong></li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment"># 2. 数值比较</span></span><br><span class="line">n1=20</span><br><span class="line">n2=10</span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$n1</span> -ge <span class="variable">$n2</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"n1 is greater than or euqal to n2"</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"n1 is less than n2"</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>注意：Bash Shell只能直接处理整数，赋值浮点数会报错</strong></li>
</ul>
<h3 id="2-3-字符串比较"><a href="#2-3-字符串比较" class="headerlink" title="2.3 字符串比较"></a>2.3 字符串比较</h3><table>
<thead>
<tr>
<th>比较</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>str1 = str2</td>
<td>检查str1是否和str2相同</td>
</tr>
<tr>
<td>str1 != str2</td>
<td>检查str1是否和str2不同</td>
</tr>
<tr>
<td>str1 &lt; str2</td>
<td>检查str1是否比str2小</td>
</tr>
<tr>
<td>str1 &gt; str2</td>
<td>检查str1是否比str2大</td>
</tr>
<tr>
<td>-n str1</td>
<td>检查str1的长度是否非0</td>
</tr>
<tr>
<td>-z str1</td>
<td>检查str1的长度是否为0</td>
</tr>
</tbody></table>
<ul>
<li><strong>示例如下：</strong></li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3. 字符串比较</span></span><br><span class="line">user=root</span><br><span class="line"><span class="keyword">if</span> [ $(whoami)=<span class="variable">$user</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"root is online"</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"root is offline"</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure>

<h3 id="2-4-文件比较"><a href="#2-4-文件比较" class="headerlink" title="2.4 文件比较"></a>2.4 文件比较</h3><table>
<thead>
<tr>
<th>比较</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>-d file</td>
<td>检查file是否存在并是一个目录</td>
</tr>
<tr>
<td>-e file</td>
<td>检查file是否存在</td>
</tr>
<tr>
<td>-f file</td>
<td>检查file是否存在并是一个文件</td>
</tr>
<tr>
<td>-r file</td>
<td>检查file是否存在并可读</td>
</tr>
<tr>
<td>-s file</td>
<td>检查file是否存在并非空</td>
</tr>
<tr>
<td>-w file</td>
<td>检查file是否存在并可写</td>
</tr>
<tr>
<td>-x file</td>
<td>检查file是否存在并可执行</td>
</tr>
<tr>
<td>-O file</td>
<td>检查file是否存在并属当前用户所有</td>
</tr>
<tr>
<td>-x file</td>
<td>检查file是否存在并可执行</td>
</tr>
<tr>
<td>-G file</td>
<td>检查file是否存在并且默认组与当前用户相同</td>
</tr>
<tr>
<td>file1 -nt file2</td>
<td>检查file1是否比file2新</td>
</tr>
<tr>
<td>file1 -ot file2</td>
<td>检查file1是否比file2旧</td>
</tr>
</tbody></table>
<ul>
<li><strong>示例如下：</strong></li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 4. 文件比较</span></span><br><span class="line">fileName=test3</span><br><span class="line"><span class="keyword">if</span> [ -e <span class="variable">$fileName</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"<span class="variable">$fileName</span>  exists"</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"<span class="variable">$fileName</span> doesn't exists"</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure>
<hr>
<h2 id="2-花括号"><a href="#2-花括号" class="headerlink" title="2. 花括号{ }"></a>2. 花括号{ }</h2><ul>
<li><strong>花括号</strong>一般用于需要变量和字符串组合输出时，若想要实现变量后拼接字符串就需要使用花括号</li>
<li><strong>示例如下：</strong></li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment"># 花括号使用练习</span></span><br><span class="line">var=50</span><br><span class="line">var1=100</span><br><span class="line">var2=200</span><br><span class="line"><span class="comment"># 若想要实现var变量后拼接字符串就需要使用花括号</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$var1</span> <span class="variable">$&#123;var&#125;</span>1</span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$var2</span> <span class="variable">$&#123;var&#125;</span>2</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="3-双括号"><a href="#3-双括号" class="headerlink" title="3. 双括号(( ))"></a>3. 双括号(( ))</h2><ul>
<li><strong>双括号</strong>允许在比较语句中使用<strong>高级数学表达式</strong>，<strong>也可以与美元符号搭配，用于整型数据计算</strong></li>
</ul>
<table>
<thead>
<tr>
<th>符号</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>val++</td>
<td>后增</td>
</tr>
<tr>
<td>val--</td>
<td>后减</td>
</tr>
<tr>
<td>++val</td>
<td>先增</td>
</tr>
<tr>
<td>--val</td>
<td>先减</td>
</tr>
<tr>
<td>！</td>
<td>逻辑求反</td>
</tr>
<tr>
<td>～</td>
<td>按位求反</td>
</tr>
<tr>
<td>**</td>
<td>幂运算</td>
</tr>
<tr>
<td>&lt;&lt;</td>
<td>左移位</td>
</tr>
<tr>
<td>&gt;&gt;</td>
<td>右移位</td>
</tr>
<tr>
<td>&amp;</td>
<td>布尔与</td>
</tr>
<tr>
<td>|</td>
<td>布尔或</td>
</tr>
<tr>
<td>&amp;&amp;</td>
<td>逻辑与</td>
</tr>
<tr>
<td>||</td>
<td>逻辑或</td>
</tr>
</tbody></table>
<ul>
<li><strong>示例如下：</strong><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment"># 双括号使用练习</span></span><br><span class="line"><span class="comment"># 用于高级数学表达式</span></span><br><span class="line">var1=10</span><br><span class="line"><span class="keyword">if</span> ((<span class="variable">$var1</span> &gt;= 10)); <span class="keyword">then</span></span><br><span class="line">    <span class="keyword">for</span> ((i = 0; i &lt; 3; i++)); <span class="keyword">do</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="variable">$i</span></span><br><span class="line">    <span class="keyword">done</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure>

</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用于整型数据计算</span></span><br><span class="line">var1=100</span><br><span class="line">var2=200</span><br><span class="line"><span class="comment"># 可以使用$+双方括号来表示整型运算</span></span><br><span class="line">var3=$[ <span class="variable">$var1</span>+<span class="variable">$var2</span>+1 ]</span><br><span class="line"><span class="comment"># 也可以使用$+双括号来表示整型运算</span></span><br><span class="line">var4=$((<span class="variable">$var1</span> + <span class="variable">$var2</span> + 1))</span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$var3</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$var4</span></span><br></pre></td></tr></table></figure>
<hr>
<h2 id="4-双方括号"><a href="#4-双方括号" class="headerlink" title="4. 双方括号[[ ]]"></a>4. 双方括号[[ ]]</h2><ul>
<li>双方括号提供了针对字符串比较的高级特性，能够使用数学符号比较字符串，并实现了模式匹配</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment"># 双方括号使用练习</span></span><br><span class="line">fileName=test5</span><br><span class="line"><span class="keyword">if</span> [[ <span class="variable">$fileName</span>==<span class="built_in">test</span>* ]]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"This is a test file!"</span></span><br><span class="line">    <span class="keyword">if</span> [[ <span class="variable">$fileName</span>==test5 ]]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"This file is test5!"</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure>


<ul>
<li><strong>注意：不是所有的Shell都支持双方括号</strong></li>
</ul>
<hr>
<h2 id="5-参考资料"><a href="#5-参考资料" class="headerlink" title="5. 参考资料"></a>5. 参考资料</h2><blockquote>
<p>《Linux命令行与shell脚本编程大全（第三版）》</p>
</blockquote>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>CentOS7</tag>
        <tag>Linux</tag>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title>CentOS7设置网卡静态IP</title>
    <url>/2020/02/05/CentOS7%E8%AE%BE%E7%BD%AE%E7%BD%91%E5%8D%A1%E9%9D%99%E6%80%81IP/</url>
    <content><![CDATA[<hr>
<a id="more"></a>

<hr>
<h2 id="修改后"><a href="#修改后" class="headerlink" title="修改后"></a>修改后</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[TomAndersen@localhost ~]$ ifconfig </span><br><span class="line">eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500</span><br><span class="line">        inet 192.168.126.101  netmask 255.255.255.0  broadcast 192.168.126.255</span><br><span class="line">        inet6 fe80::76e2:b29e:666:450c  prefixlen 64  scopeid 0x20&lt;link&gt;</span><br><span class="line">        ether 00:0c:29:35:00:eb  txqueuelen 1000  (Ethernet)</span><br><span class="line">        RX packets 14  bytes 2147 (2.0 KiB)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 90  bytes 11605 (11.3 KiB)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200205215402676.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="修改后"></p>
<h2 id="具体步骤"><a href="#具体步骤" class="headerlink" title="具体步骤"></a>具体步骤</h2><h3 id="1）修改ifcfg-eth0文件"><a href="#1）修改ifcfg-eth0文件" class="headerlink" title="1）修改ifcfg-eth0文件"></a>1）修改ifcfg-eth0文件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[TomAndersen@localhost ~]$ vim /etc/sysconfig/network-scripts/ifcfg-eth0</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>修改和添加字段：将 BOOTPROTO 变量值修改成static，将 ONBOOT 变量设置成yes，并添加IPADDR、NETMASK、GATEWAY等变量</strong></li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">BOOTPROTO=static <span class="comment"># 使用静态IP地址</span></span><br><span class="line">IPADDR=192.168.126.101 <span class="comment">#IP Address</span></span><br><span class="line">NETMASK=255.255.255.0 <span class="comment">#子网掩码</span></span><br><span class="line">GATEWAY=192.168.126.2 <span class="comment"># 网关地址,虚拟机VMnet8中设置了网关为192.168.126.2</span></span><br><span class="line">DNS1=192.168.126.2 <span class="comment">#DNS直接设置成默认网关</span></span><br><span class="line">ONBOOT=yes <span class="comment">#设置开机自动激活</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>然后保存退出</strong></li>
</ul>
<h3 id="2）重启网络服务"><a href="#2）重启网络服务" class="headerlink" title="2）重启网络服务"></a>2）重启网络服务</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[TomAndersen@localhost ~]$ service network restart</span><br></pre></td></tr></table></figure>

<h3 id="3）重启网卡"><a href="#3）重启网卡" class="headerlink" title="3）重启网卡"></a>3）重启网卡</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[TomAndersen@localhost ~]$ sudo ifdown eth0</span><br><span class="line"></span><br><span class="line">[TomAndersen@localhost ~]$ sudo ifup eth0</span><br></pre></td></tr></table></figure>
<h3 id="4）检查IP地址是否更改成功"><a href="#4）检查IP地址是否更改成功" class="headerlink" title="4）检查IP地址是否更改成功"></a>4）检查IP地址是否更改成功</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[TomAndersen@localhost ~]$ ifconfig</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CentOS7</tag>
        <tag>Linux</tag>
        <tag>网卡</tag>
        <tag>IP</tag>
      </tags>
  </entry>
  <entry>
    <title>CentOS7修改网卡名</title>
    <url>/2020/02/05/CentOS7%E4%BF%AE%E6%94%B9%E7%BD%91%E5%8D%A1%E5%90%8D/</url>
    <content><![CDATA[<hr>
<a id="more"></a>

<hr>
<h2 id="修改后："><a href="#修改后：" class="headerlink" title="修改后："></a>修改后：</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[TomAndersen@localhost ~]$ ifconfig </span><br><span class="line">eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500</span><br><span class="line">        ether 00:0c:29:35:00:eb  txqueuelen 1000  (Ethernet)</span><br><span class="line">        RX packets 1  bytes 243 (243.0 B)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br></pre></td></tr></table></figure>
<h2 id="修改步骤："><a href="#修改步骤：" class="headerlink" title="修改步骤："></a>修改步骤：</h2><h3 id="1）修改ifcfg-ens文件"><a href="#1）修改ifcfg-ens文件" class="headerlink" title="1）修改ifcfg-ens文件"></a>1）修改ifcfg-ens文件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[TomAndersen@localhost network-scripts]$ vim /etc/sysconfig/network-scripts/ifcfg-ens33</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>将 NAME 和 DEVICE 变量修改为需要改成的网卡名，变量值必须相同，然后强制保存退出</strong><br><img src="https://img-blog.csdnimg.cn/20200205212705355.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="修改ifcfg-ens文件"></li>
<li><strong>修改ifcfg-ens文件名，将其后缀修改成对应网卡名</strong></li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[TomAndersen@localhost network-scripts]$ mv ifcfg-ens33 ifcfg-eth0</span><br></pre></td></tr></table></figure>

<h3 id="2）修改grub文件"><a href="#2）修改grub文件" class="headerlink" title="2）修改grub文件"></a>2）修改grub文件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[TomAndersen@localhost network-scripts]$ vim /etc/sysconfig/grub</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>在 GRUB_CMDLINE_LINUX 变量中插入 net.ifnames=0 biosdevname=0，然后保存退出</strong><br><img src="https://img-blog.csdnimg.cn/20200205213654256.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="修改grub文件"></li>
<li><strong>重新加载内核参数</strong></li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[TomAndersen@localhost network-scripts]$ grub2-mkconfig -o /boott/grub2/grub.cfg</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>最后重启即可</strong></li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[TomAndersen@localhost network-scripts]$ sudo reboot</span><br></pre></td></tr></table></figure>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CentOS7</tag>
        <tag>Linux</tag>
        <tag>网卡</tag>
      </tags>
  </entry>
  <entry>
    <title>Java中字符串String加法解析</title>
    <url>/2020/02/02/Java%E4%B8%AD%E5%AD%97%E7%AC%A6%E4%B8%B2String%E5%8A%A0%E6%B3%95%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<hr>
<a id="more"></a>

<hr>
<h2 id="测试代码："><a href="#测试代码：" class="headerlink" title="测试代码："></a>测试代码：</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        String a = <span class="string">"hello2"</span>;</span><br><span class="line">        <span class="keyword">final</span> String b = <span class="string">"hello"</span>;</span><br><span class="line">        String d = <span class="string">"hello"</span>;</span><br><span class="line">        String c = b + <span class="number">2</span>;</span><br><span class="line">        <span class="comment">// 由于b是常量所以编译期，编译器会直接将b替换为其值，并直接拼接成"Hello2"赋值给c，这又叫做“常量传播”优化</span></span><br><span class="line">        <span class="comment">// 故在编译期就能确定b和c的值</span></span><br><span class="line">        String e = d + <span class="number">2</span>;</span><br><span class="line">        <span class="comment">// 由于赋值语句右侧使用的不同类型常量赋值，因此会使用StringBuilder实现字符串拼接</span></span><br><span class="line">        <span class="comment">// 会先调用其append方法最后调用toString方法，而toString方法是new String并返回，即在堆中创建对象</span></span><br><span class="line">        <span class="comment">// 故e的值需要运行时确定</span></span><br><span class="line">        String f = <span class="string">"hello"</span> + <span class="string">"2"</span>;</span><br><span class="line">        <span class="comment">// 而f的赋值语句右侧也都是字面量，或者说常量，同字符串c的解释，故在编译期就能确定f值</span></span><br><span class="line">        <span class="comment">// 故最终的输出结果为：</span></span><br><span class="line">        System.out.println(a == c);<span class="comment">// true</span></span><br><span class="line">        System.out.println(a == e);<span class="comment">// false</span></span><br><span class="line">        System.out.println(a == f);<span class="comment">// true</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="反汇编class文件输出："><a href="#反汇编class文件输出：" class="headerlink" title="反汇编class文件输出："></a>反汇编class文件输出：</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">C:\Users\DELL\Desktop&gt;javap -c Solution.class</span><br><span class="line">Compiled from <span class="string">"Solution.java"</span></span><br><span class="line">public class LeetCodeStudy.Solution &#123;</span><br><span class="line">  public LeetCodeStudy.Solution();</span><br><span class="line">    Code:</span><br><span class="line">       0: aload_0</span><br><span class="line">       1: invokespecial <span class="comment">#1                  // Method java/lang/Object."&lt;init&gt;":()V</span></span><br><span class="line">       4: <span class="built_in">return</span></span><br><span class="line"></span><br><span class="line">  public static void main(java.lang.String[]);</span><br><span class="line">    Code:</span><br><span class="line">       0: ldc           <span class="comment">#2                  // String hello2</span></span><br><span class="line">       2: astore_1</span><br><span class="line">       3: ldc           <span class="comment">#3                  // String hello</span></span><br><span class="line">       5: astore_2</span><br><span class="line">       6: ldc           <span class="comment">#3                  // String hello</span></span><br><span class="line">       8: astore_3</span><br><span class="line">       9: ldc           <span class="comment">#2                  // String hello2</span></span><br><span class="line">      11: astore        4</span><br><span class="line">      13: new           <span class="comment">#4                  // class java/lang/StringBuilder</span></span><br><span class="line">      16: dup</span><br><span class="line">      17: invokespecial <span class="comment">#5                  // Method java/lang/StringBuilder."&lt;init&gt;":()V</span></span><br><span class="line">      20: aload_3</span><br><span class="line">      21: invokevirtual <span class="comment">#6                  // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder;</span></span><br><span class="line">      24: iconst_2</span><br><span class="line">      25: invokevirtual <span class="comment">#7                  // Method java/lang/StringBuilder.append:(I)Ljava/lang/StringBuilder;</span></span><br><span class="line">      28: invokevirtual <span class="comment">#8                  // Method java/lang/StringBuilder.toString:()Ljava/lang/String;</span></span><br><span class="line">      31: astore        5</span><br><span class="line">      33: ldc           <span class="comment">#2                  // String hello2</span></span><br><span class="line">      35: astore        6</span><br><span class="line">      37: getstatic     <span class="comment">#9                  // Field java/lang/System.out:Ljava/io/PrintStream;</span></span><br><span class="line">      40: aload_1</span><br><span class="line">      41: aload         4</span><br><span class="line">      43: if_acmpne     50</span><br><span class="line">      46: iconst_1</span><br><span class="line">      47: goto          51</span><br><span class="line">      50: iconst_0</span><br><span class="line">      51: invokevirtual <span class="comment">#10                 // Method java/io/PrintStream.println:(Z)V</span></span><br><span class="line">      54: getstatic     <span class="comment">#9                  // Field java/lang/System.out:Ljava/io/PrintStream;</span></span><br><span class="line">      57: aload_1</span><br><span class="line">      58: aload         5</span><br><span class="line">      60: if_acmpne     67</span><br><span class="line">      63: iconst_1</span><br><span class="line">      64: goto          68</span><br><span class="line">      67: iconst_0</span><br><span class="line">      68: invokevirtual <span class="comment">#10                 // Method java/io/PrintStream.println:(Z)V</span></span><br><span class="line">      71: getstatic     <span class="comment">#9                  // Field java/lang/System.out:Ljava/io/PrintStream;</span></span><br><span class="line">      74: aload_1</span><br><span class="line">      75: aload         6</span><br><span class="line">      77: if_acmpne     84</span><br><span class="line">      80: iconst_1</span><br><span class="line">      81: goto          85</span><br><span class="line">      84: iconst_0</span><br><span class="line">      85: invokevirtual <span class="comment">#10                 // Method java/io/PrintStream.println:(Z)V</span></span><br><span class="line">      88: <span class="built_in">return</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="参考文档："><a href="#参考文档：" class="headerlink" title="参考文档："></a>参考文档：</h2><blockquote>
<p><a href="https://blog.csdn.net/qq_23367963/article/details/81937325" target="_blank" rel="noopener">String 相加解析原理</a><br><a href="https://blog.csdn.net/shandalue/article/details/44020631" target="_blank" rel="noopener">String字符串相加的问题</a><br><a href="https://blog.csdn.net/shfqbluestone/article/details/34188325" target="_blank" rel="noopener">java 字符串拼接为什么要用 StringBuilder 而不直接用 String 相加连接</a></p>
</blockquote>
<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Javap</tag>
        <tag>Javac</tag>
      </tags>
  </entry>
  <entry>
    <title>一种下载Nvidia旧版本显卡驱动的方法（在知道版本号的前提下）</title>
    <url>/2019/04/21/%E4%B8%80%E7%A7%8D%E4%B8%8B%E8%BD%BDNvidia%E6%97%A7%E7%89%88%E6%9C%AC%E6%98%BE%E5%8D%A1%E9%A9%B1%E5%8A%A8%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%88%E5%9C%A8%E7%9F%A5%E9%81%93%E7%89%88%E6%9C%AC%E5%8F%B7%E7%9A%84%E5%89%8D%E6%8F%90%E4%B8%8B%EF%BC%89/</url>
    <content><![CDATA[<hr>
<a id="more"></a>

<hr>
<h2 id="例如"><a href="#例如" class="headerlink" title="例如"></a>例如</h2><h3 id="现在时间：2019年4月21日08-59-12，英伟达官网-notebook-win10-64bit-international-whql-最新的驱动版本号为425-31-，下载地址为："><a href="#现在时间：2019年4月21日08-59-12，英伟达官网-notebook-win10-64bit-international-whql-最新的驱动版本号为425-31-，下载地址为：" class="headerlink" title="现在时间：2019年4月21日08:59:12，英伟达官网 notebook-win10-64bit-international-whql 最新的驱动版本号为425.31 ，下载地址为："></a>现在时间：2019年4月21日08:59:12，英伟达官网 notebook-win10-64bit-international-whql 最新的驱动版本号为425.31 ，下载地址为：</h3><p>​    <a href="https://cn.download.nvidia.cn/Windows/425.31/425.31-notebook-win10-64bit-international-whql.exe" target="_blank" rel="noopener">https://cn.download.nvidia.cn/Windows/425.31/425.31-notebook-win10-64bit-international-whql.exe</a></p>
<h3 id="次新的-notebook-win10-64bit-international-whql-驱动版本号为-419-67-，下载地址为："><a href="#次新的-notebook-win10-64bit-international-whql-驱动版本号为-419-67-，下载地址为：" class="headerlink" title="次新的 notebook-win10-64bit-international-whql 驱动版本号为 419.67  ，下载地址为："></a>次新的 notebook-win10-64bit-international-whql 驱动版本号为 419.67  ，下载地址为：</h3><p>​    <a href="https://cn.download.nvidia.cn/Windows/419.67/419.67-notebook-win10-64bit-international-whql.exe" target="_blank" rel="noopener">https://cn.download.nvidia.cn/Windows/419.67/419.67-notebook-win10-64bit-international-whql.exe</a></p>
<h3 id="观察其中的规律，我们便可以在已经知道版本号的情况下推出其下载地址（除了某些特殊的版本）"><a href="#观察其中的规律，我们便可以在已经知道版本号的情况下推出其下载地址（除了某些特殊的版本）" class="headerlink" title="观察其中的规律，我们便可以在已经知道版本号的情况下推出其下载地址（除了某些特殊的版本）"></a>观察其中的规律，我们便可以在已经知道版本号的情况下推出其下载地址（除了某些特殊的版本）</h3><h3 id="如版本号391-24下载地址可由以上规律推得："><a href="#如版本号391-24下载地址可由以上规律推得：" class="headerlink" title="如版本号391.24下载地址可由以上规律推得："></a>如版本号391.24下载地址可由以上规律推得：</h3><pre><code>https://us.download.nvidia.cn/Windows/391.24/391.24-notebook-win10-64bit-international-whql.exe</code></pre><h3 id="又或者如版本号391-01的下载地址可以由以上规律推得："><a href="#又或者如版本号391-01的下载地址可以由以上规律推得：" class="headerlink" title="又或者如版本号391.01的下载地址可以由以上规律推得："></a>又或者如版本号391.01的下载地址可以由以上规律推得：</h3><pre><code>https://us.download.nvidia.cn/Windows/391.01/391.01-notebook-win10-64bit-international-whql.exe</code></pre><h3 id="又或者不知道版本号码的话可以直接使用以下网址，其中可以检索到一些不太旧的版本驱动："><a href="#又或者不知道版本号码的话可以直接使用以下网址，其中可以检索到一些不太旧的版本驱动：" class="headerlink" title="又或者不知道版本号码的话可以直接使用以下网址，其中可以检索到一些不太旧的版本驱动："></a>又或者不知道版本号码的话可以直接使用以下网址，其中可以检索到一些不太旧的版本驱动：</h3><pre><code>https://www.nvidia.cn/Download/Find.aspx?lang=cn#</code></pre><p><strong>声明：以上规律不保证切实可用，略略略<del>~</del></strong></p>
<p><strong>新人博主，不喜轻喷，但求一赞<del>~</del>。</strong></p>
<hr>
<h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
      <categories>
        <category>驱动</category>
      </categories>
      <tags>
        <tag>Nvidia</tag>
        <tag>显卡驱动</tag>
      </tags>
  </entry>
</search>
