<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Eden</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://tomandersen-cc.github.io/"/>
  <updated>2020-03-12T04:19:16.746Z</updated>
  <id>https://tomandersen-cc.github.io/</id>
  
  <author>
    <name>Tom Andersen</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hexo之NexT主题中设置加载进度条</title>
    <link href="https://tomandersen-cc.github.io/2020/03/06/Hexo%E4%B9%8BNexT%E4%B8%BB%E9%A2%98%E4%B8%AD%E8%AE%BE%E7%BD%AE%E5%8A%A0%E8%BD%BD%E8%BF%9B%E5%BA%A6%E6%9D%A1/"/>
    <id>https://tomandersen-cc.github.io/2020/03/06/Hexo%E4%B9%8BNexT%E4%B8%BB%E9%A2%98%E4%B8%AD%E8%AE%BE%E7%BD%AE%E5%8A%A0%E8%BD%BD%E8%BF%9B%E5%BA%A6%E6%9D%A1/</id>
    <published>2020-03-06T04:14:35.000Z</published>
    <updated>2020-03-12T04:19:16.746Z</updated>
    
    <content type="html"><![CDATA[<hr><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul><li><strong>Hexo版本</strong>：4.2.0</li><li><strong>NexT版本</strong>：7.7.1</li><li><strong>GitHub</strong>：<a href="https://github.com/theme-next/theme-next-pace" target="_blank" rel="noopener">theme-next-pace</a></li><li><strong>各种进度条样式参考</strong>：<a href="https://blog.pangao.vip/Hexo博客NexT主题美化之顶部加载进度条/" target="_blank" rel="noopener">Hexo博客NexT主题美化之顶部加载进度条</a></li></ul><hr><a id="more"></a><h2 id="安装部署"><a href="#安装部署" class="headerlink" title="安装部署"></a>安装部署</h2><h3 id="1）进入NexT主题文件夹"><a href="#1）进入NexT主题文件夹" class="headerlink" title="1）进入NexT主题文件夹"></a>1）进入NexT主题文件夹</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> themes/next/</span><br><span class="line">$ ls</span><br><span class="line">_config.yml  docs/        languages/  LICENSE.md    README.md  <span class="built_in">source</span>/</span><br><span class="line">crowdin.yml  gulpfile.js  layout/     package.json  scripts/</span><br></pre></td></tr></table></figure><h3 id="2）克隆Github仓库（如果使用CDN可跳过此步骤）"><a href="#2）克隆Github仓库（如果使用CDN可跳过此步骤）" class="headerlink" title="2）克隆Github仓库（如果使用CDN可跳过此步骤）"></a>2）克隆Github仓库（如果使用CDN可跳过此步骤）</h3><p><strong>将仓库克隆至<code>themes/next/source/lib</code>路径下</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/theme-next/theme-next-pace <span class="built_in">source</span>/lib/pace</span><br></pre></td></tr></table></figure><h3 id="3）配置NexT中的-config-xml"><a href="#3）配置NexT中的-config-xml" class="headerlink" title="3）配置NexT中的_config.xml"></a>3）配置NexT中的<code>_config.xml</code></h3><p><strong>开启pace选项</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Progress bar in the top during page loading.</span></span><br><span class="line"><span class="comment"># 设置页面加载时顶部进度条</span></span><br><span class="line"><span class="comment"># Dependencies: https://github.com/theme-next/theme-next-pace</span></span><br><span class="line"><span class="comment"># For more information: https://github.com/HubSpot/pace</span></span><br><span class="line">pace:</span><br><span class="line">  <span class="comment"># enable: false</span></span><br><span class="line">  <span class="built_in">enable</span>: <span class="literal">true</span></span><br><span class="line">  <span class="comment"># Themes list:</span></span><br><span class="line">  <span class="comment"># big-counter | bounce | barber-shop | center-atom | center-circle | center-radar | center-simple</span></span><br><span class="line">  <span class="comment"># corner-indicator | fill-left | flat-top | flash | loading-bar | mac-osx | material | minimal</span></span><br><span class="line">  theme: minimal</span><br></pre></td></tr></table></figure><h3 id="4）配置进度条CDN地址"><a href="#4）配置进度条CDN地址" class="headerlink" title="4）配置进度条CDN地址"></a>4）配置进度条CDN地址</h3><p><strong>在NexT主题的<code>_config.xml</code>文件中找到<code>vendors</code>选项，设置pace的cdn地址（本人设置的进度条为黑色主题，可以在<a href="https://www.jsdelivr.com/package/npm/pace-js?path=themes" target="_blank" rel="noopener">jsdelivr</a>中找到对应的样式最新版cdn地址）</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vendors:</span><br><span class="line">  ...</span><br><span class="line">  pace: https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js</span><br><span class="line">  pace_css: https://cdn.jsdelivr.net/npm/pace-js@1.0.2/themes/black/pace-theme-loading-bar.css</span><br></pre></td></tr></table></figure><hr><h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;hr&gt;
&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Hexo版本&lt;/strong&gt;：4.2.0&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NexT版本&lt;/strong&gt;：7.7.1&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GitHub&lt;/strong&gt;：&lt;a href=&quot;https://github.com/theme-next/theme-next-pace&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;theme-next-pace&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;各种进度条样式参考&lt;/strong&gt;：&lt;a href=&quot;https://blog.pangao.vip/Hexo博客NexT主题美化之顶部加载进度条/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo博客NexT主题美化之顶部加载进度条&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
    
    </summary>
    
    
      <category term="个人博客搭建" scheme="https://tomandersen-cc.github.io/categories/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"/>
    
    
      <category term="Blog" scheme="https://tomandersen-cc.github.io/tags/Blog/"/>
    
      <category term="Hexo" scheme="https://tomandersen-cc.github.io/tags/Hexo/"/>
    
      <category term="NexT" scheme="https://tomandersen-cc.github.io/tags/NexT/"/>
    
  </entry>
  
  <entry>
    <title>Hexo之NexT主题中设置canvas-nest特效</title>
    <link href="https://tomandersen-cc.github.io/2020/03/06/Hexo%E4%B9%8BNexT%E4%B8%BB%E9%A2%98%E4%B8%AD%E8%AE%BE%E7%BD%AEcanvas-nest%E7%89%B9%E6%95%88/"/>
    <id>https://tomandersen-cc.github.io/2020/03/06/Hexo%E4%B9%8BNexT%E4%B8%BB%E9%A2%98%E4%B8%AD%E8%AE%BE%E7%BD%AEcanvas-nest%E7%89%B9%E6%95%88/</id>
    <published>2020-03-06T03:06:19.000Z</published>
    <updated>2020-03-12T04:19:53.530Z</updated>
    
    <content type="html"><![CDATA[<hr><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul><li><strong>Hexo版本</strong>：4.2.0</li><li><strong>NexT版本</strong>：7.7.1</li><li><strong>NexT中集成有canvas_nest插件</strong></li><li><strong>GitHub</strong>：<a href="https://github.com/theme-next/theme-next-canvas-nest" target="_blank" rel="noopener">theme-next-canvas-nest</a></li><li><a href="https://git.hust.cc/canvas-nest.js" target="_blank" rel="noopener">canvas-nest特效展示</a></li></ul><a id="more"></a><hr><h2 id="安装部署"><a href="#安装部署" class="headerlink" title="安装部署"></a>安装部署</h2><h3 id="1）配置NexT主题配置文件themes-next-config-yml"><a href="#1）配置NexT主题配置文件themes-next-config-yml" class="headerlink" title="1）配置NexT主题配置文件themes/next/_config.yml"></a>1）配置NexT主题配置文件<code>themes/next/_config.yml</code></h3><p><strong>a）开启canvas_nest</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Canvas-nest</span></span><br><span class="line"><span class="comment"># Dependencies: https://github.com/theme-next/theme-next-canvas-nest</span></span><br><span class="line"><span class="comment"># For more information: https://github.com/hustcc/canvas-nest.js</span></span><br><span class="line"><span class="comment"># 若要开启canvas_nest,除了此处设置成true,还需要设置canvas_nest的vendors提供商</span></span><br><span class="line">canvas_nest:</span><br><span class="line">  <span class="comment">#enable: false</span></span><br><span class="line">  <span class="built_in">enable</span>: <span class="literal">true</span></span><br><span class="line">  onmobile: <span class="literal">true</span> <span class="comment"># Display on mobile or not</span></span><br><span class="line">  color: <span class="string">"0,0,255"</span> <span class="comment"># RGB values, use `,` to separate</span></span><br><span class="line">  opacity: 0.5 <span class="comment"># The opacity of line: 0~1</span></span><br><span class="line">  zIndex: -1 <span class="comment"># z-index property of the background</span></span><br><span class="line">  count: 99 <span class="comment"># The number of lines</span></span><br></pre></td></tr></table></figure><p><strong>b）设置canvas_nest脚本来源：在配置文件vendors选项下取消canvas_nest cdn注释</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Internal version: 1.0.0</span></span><br><span class="line"><span class="comment"># 设置canvas_nest的来源</span></span><br><span class="line">canvas_nest: //cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-nest@1/canvas-nest.min.js</span><br><span class="line">canvas_nest_nomobile: //cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-nest@1/canvas-nest-nomobile.min.js</span><br><span class="line"><span class="comment"># canvas_nest:</span></span><br><span class="line"><span class="comment"># canvas_nest_nomobile:</span></span><br></pre></td></tr></table></figure><hr><h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;hr&gt;
&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Hexo版本&lt;/strong&gt;：4.2.0&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NexT版本&lt;/strong&gt;：7.7.1&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NexT中集成有canvas_nest插件&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GitHub&lt;/strong&gt;：&lt;a href=&quot;https://github.com/theme-next/theme-next-canvas-nest&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;theme-next-canvas-nest&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://git.hust.cc/canvas-nest.js&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;canvas-nest特效展示&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="个人博客搭建" scheme="https://tomandersen-cc.github.io/categories/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"/>
    
    
      <category term="Blog" scheme="https://tomandersen-cc.github.io/tags/Blog/"/>
    
      <category term="Hexo" scheme="https://tomandersen-cc.github.io/tags/Hexo/"/>
    
      <category term="NexT" scheme="https://tomandersen-cc.github.io/tags/NexT/"/>
    
  </entry>
  
  <entry>
    <title>Hexo之NexT主题中设置symbols_count_time统计单词</title>
    <link href="https://tomandersen-cc.github.io/2020/03/06/Hexo%E4%B9%8BNexT%E4%B8%BB%E9%A2%98%E4%B8%AD%E8%AE%BE%E7%BD%AEsymbols-count-time%E7%BB%9F%E8%AE%A1%E5%8D%95%E8%AF%8D/"/>
    <id>https://tomandersen-cc.github.io/2020/03/06/Hexo%E4%B9%8BNexT%E4%B8%BB%E9%A2%98%E4%B8%AD%E8%AE%BE%E7%BD%AEsymbols-count-time%E7%BB%9F%E8%AE%A1%E5%8D%95%E8%AF%8D/</id>
    <published>2020-03-06T02:47:55.000Z</published>
    <updated>2020-03-12T04:19:58.303Z</updated>
    
    <content type="html"><![CDATA[<hr><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul><li><strong>Hexo版本</strong>：4.2.0</li><li><strong>NexT版本</strong>：7.7.1</li><li><strong>symbols_count_time</strong>能够统计页面或者站点的单词以及阅读所需时间</li><li>自NexT 6.0发行版之后第三方插件<strong>hexo-wordcount</strong>就被<strong>symbols_count_time</strong>取缔了，相比之下<strong>symbols_count_time</strong>没有额外的依赖，性能更加强大</li><li><strong>GitHub</strong>：<a href="https://github.com/theme-next/hexo-symbols-count-time" target="_blank" rel="noopener">symbols_count_time</a></li></ul><a id="more"></a><hr><h2 id="安装部署"><a href="#安装部署" class="headerlink" title="安装部署"></a>安装部署</h2><h3 id="1）安装symbols-count-time插件"><a href="#1）安装symbols-count-time插件" class="headerlink" title="1）安装symbols_count_time插件"></a>1）安装symbols_count_time插件</h3><p><strong>在Hexo的根目录下安装symbols_count_time</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-symbols-count-time</span><br></pre></td></tr></table></figure><p><strong>若npm下载速度太慢可以使用淘宝npm镜像</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm install -g cnpm --registry=https://registry.npm.taobao.org</span><br><span class="line">cnpm install hexo-symbols-count-time</span><br></pre></td></tr></table></figure><h3 id="2）配置Hexo站点配置文件-config-yml"><a href="#2）配置Hexo站点配置文件-config-yml" class="headerlink" title="2）配置Hexo站点配置文件_config.yml"></a>2）配置Hexo站点配置文件<code>_config.yml</code></h3><p><strong>在合适位置添加以下配置信息</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置博客单词统计</span></span><br><span class="line">symbols_count_time:</span><br><span class="line">  <span class="comment"># 文章字数统计</span></span><br><span class="line">  symbols: <span class="literal">true</span></span><br><span class="line">  <span class="comment"># 文章阅读时间统计</span></span><br><span class="line">  time: <span class="literal">true</span></span><br><span class="line">  <span class="comment"># 站点总字数统计</span></span><br><span class="line">  total_symbols: <span class="literal">false</span></span><br><span class="line">  <span class="comment"># 站点总阅读时间统计</span></span><br><span class="line">  total_time: <span class="literal">false</span></span><br><span class="line">  exclude_codeblock: <span class="literal">false</span></span><br></pre></td></tr></table></figure><h3 id="3）配置NexT主题配置文件themes-next-config-yml"><a href="#3）配置NexT主题配置文件themes-next-config-yml" class="headerlink" title="3）配置NexT主题配置文件themes/next/_config.yml"></a>3）配置NexT主题配置文件<code>themes/next/_config.yml</code></h3><p><strong>在symbols_count_time选下开启单词统计</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Post wordcount display settings</span></span><br><span class="line"><span class="comment"># Dependencies: https://github.com/theme-next/hexo-symbols-count-time</span></span><br><span class="line"><span class="comment"># 设置博客单词统计</span></span><br><span class="line">symbols_count_time:</span><br><span class="line">  <span class="comment"># 是否另起一行（true的话不和发表时间等同一行）</span></span><br><span class="line">  separated_meta: <span class="literal">true</span></span><br><span class="line">  <span class="comment"># 首页文章统计数量前是否显示文字描述（本文字数、阅读时长）</span></span><br><span class="line">  item_text_post: <span class="literal">true</span></span><br><span class="line">  <span class="comment"># 页面底部统计数量前是否显示文字描述（站点总字数、站点阅读时长）</span></span><br><span class="line">  item_text_total: <span class="literal">false</span></span><br><span class="line">  <span class="comment"># 平均字长</span></span><br><span class="line">  awl: 4</span><br><span class="line">  <span class="comment"># 每分钟阅读字数</span></span><br><span class="line">  wpm: 275</span><br></pre></td></tr></table></figure><h3 id="4）重新生成Hexo即可"><a href="#4）重新生成Hexo即可" class="headerlink" title="4）重新生成Hexo即可"></a>4）重新生成Hexo即可</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g</span><br><span class="line">hexo s</span><br></pre></td></tr></table></figure><hr><h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;hr&gt;
&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Hexo版本&lt;/strong&gt;：4.2.0&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NexT版本&lt;/strong&gt;：7.7.1&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;symbols_count_time&lt;/strong&gt;能够统计页面或者站点的单词以及阅读所需时间&lt;/li&gt;
&lt;li&gt;自NexT 6.0发行版之后第三方插件&lt;strong&gt;hexo-wordcount&lt;/strong&gt;就被&lt;strong&gt;symbols_count_time&lt;/strong&gt;取缔了，相比之下&lt;strong&gt;symbols_count_time&lt;/strong&gt;没有额外的依赖，性能更加强大&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GitHub&lt;/strong&gt;：&lt;a href=&quot;https://github.com/theme-next/hexo-symbols-count-time&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;symbols_count_time&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="个人博客搭建" scheme="https://tomandersen-cc.github.io/categories/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"/>
    
    
      <category term="Blog" scheme="https://tomandersen-cc.github.io/tags/Blog/"/>
    
      <category term="Hexo" scheme="https://tomandersen-cc.github.io/tags/Hexo/"/>
    
      <category term="NexT" scheme="https://tomandersen-cc.github.io/tags/NexT/"/>
    
  </entry>
  
  <entry>
    <title>Flume之使用Loadbalancing Sink Processor实现sink负载均衡</title>
    <link href="https://tomandersen-cc.github.io/2020/03/05/Flume%E4%B9%8B%E4%BD%BF%E7%94%A8Loadbalancing-Sink-Processor%E5%AE%9E%E7%8E%B0sink%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/"/>
    <id>https://tomandersen-cc.github.io/2020/03/05/Flume%E4%B9%8B%E4%BD%BF%E7%94%A8Loadbalancing-Sink-Processor%E5%AE%9E%E7%8E%B0sink%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/</id>
    <published>2020-03-05T13:47:53.000Z</published>
    <updated>2020-03-08T13:45:30.748Z</updated>
    
    <content type="html"><![CDATA[<hr><a id="more"></a><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul><li><strong>Load balancing Sink Processor</strong>，顾名思义，即能够对Sink组中的每个Sink实现负载均衡，默认采用的是轮询<strong>round_robin</strong>的方式，还可以使用随机方式<strong>random</strong>，或者用户自己实现AbstractSinkSelector抽象类定义自己的Sink Selector类，并提供FQCN（Full Qualified Class Name）全类名来进行配置，并且Load balancing Sink Processor还提供了指数退避backoff，即当某个Sink挂掉时，将会将其加入到黑名单，一定时间内不再访问此Sink，退避时间呈指数增长并默认最大值为30000ms，可以手动设置</li></ul><hr><h2 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a>使用示例</h2><h3 id="1）flume1-properties"><a href="#1）flume1-properties" class="headerlink" title="1）flume1.properties"></a>1）flume1.properties</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># flume1:此配置用于监控某个端口将其追加内容输出到flume2和flume3中</span></span><br><span class="line"><span class="comment"># 并将两个Sink组成一个sink group,并将Sink Processor设置成load_balance类型</span></span><br><span class="line"><span class="comment"># a1:Netcat Source-&gt; Memory Channel-&gt; Load balancing Sink Processor-&gt; Avro Sink</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sink groups</span></span><br><span class="line">a1.sinkgroups = g1</span><br><span class="line"><span class="comment"># 设置sink group中的sinks</span></span><br><span class="line">a1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line"><span class="comment"># 配置Load balancing Sink Processor(只有sink group才可以使用sink processor)</span></span><br><span class="line">a1.sinkgroups.g1.processor.type = load_balance</span><br><span class="line"><span class="comment"># 设置开启指数避让</span></span><br><span class="line">a1.sinkgroups.g1.processor.backoff = <span class="literal">true</span></span><br><span class="line"><span class="comment"># 设置Processor的selector为轮询round_robin</span></span><br><span class="line">a1.sinkgroups.g1.processor.selector = round_robin</span><br><span class="line"><span class="comment"># 设置最大避让时间(ms)</span></span><br><span class="line">a1.sinkgroups.g1.processor.maxTimeOut = 10000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Sources</span></span><br><span class="line"><span class="comment"># 配置a1.sources.r1的各项属性参数,类型/绑定主机ip/端口号</span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = hadoop101</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line"><span class="comment"># Channels</span></span><br><span class="line"><span class="comment"># 配置a1.channerls.c1的各项属性参数,缓存方式/最多缓存的Event个数/单次传输的Event个数</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sinks</span></span><br><span class="line"><span class="comment"># sinks.k1</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = hadoop102</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line"><span class="comment"># sinks.k2</span></span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = hadoop103</span><br><span class="line">a1.sinks.k2.port = 4141</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind</span></span><br><span class="line"><span class="comment"># 注意:source可以绑定多个channel,但是sink/sink group只能绑定单个channel</span></span><br><span class="line"><span class="comment"># r1-&gt;c1-&gt;g1</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c1</span><br></pre></td></tr></table></figure><h3 id="2）flume2-properties"><a href="#2）flume2-properties" class="headerlink" title="2）flume2.properties"></a>2）flume2.properties</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># flume2:此配置用于将来自指定Avro端口的数据输出到控制台</span></span><br><span class="line"><span class="comment"># a2:Avro Source-&gt;Memory Channel-&gt;Logger Sink</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent</span></span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.channels = c1</span><br><span class="line">a2.sinks = k1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sources</span></span><br><span class="line"><span class="comment"># a2.sources.r1</span></span><br><span class="line">a2.sources.r1.type = avro</span><br><span class="line"><span class="comment"># 设置监听本地IP</span></span><br><span class="line">a2.sources.r1.bind = 0.0.0.0</span><br><span class="line"><span class="comment"># 设置监听端口号</span></span><br><span class="line">a2.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line"><span class="comment"># Channels</span></span><br><span class="line"><span class="comment"># a2.channels.c1</span></span><br><span class="line"><span class="comment"># 使用内存作为缓存/最多缓存的Event个数/单次传输的Event个数</span></span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sinks</span></span><br><span class="line"><span class="comment"># 运行时设置参数 -Dflume.root.logger=INFO,console 即输出到控制台实时显示</span></span><br><span class="line">a2.sinks.k1.type = logger</span><br><span class="line"><span class="comment"># 设置Event的Body中写入log的最大字节数(默认值为16)</span></span><br><span class="line">a2.sinks.k1.maxBytesToLog = 256</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind</span></span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><h3 id="3）flume3-properties"><a href="#3）flume3-properties" class="headerlink" title="3）flume3.properties"></a>3）flume3.properties</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># flume3:此配置用于将来自指定Avro端口的数据输出到控制台</span></span><br><span class="line"><span class="comment"># a3:Avro Source-&gt;Memory Channel-&gt;Logger Sink</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent</span></span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.channels = c1</span><br><span class="line">a3.sinks = k1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sources</span></span><br><span class="line"><span class="comment"># a3.sources.r1</span></span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line"><span class="comment"># 设置监听本地IP</span></span><br><span class="line">a3.sources.r1.bind = 0.0.0.0</span><br><span class="line"><span class="comment"># 设置监听端口号</span></span><br><span class="line">a3.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line"><span class="comment"># Channels</span></span><br><span class="line"><span class="comment"># a3.channels.c1</span></span><br><span class="line"><span class="comment"># 使用内存作为缓存/最多缓存的Event个数/单次传输的Event个数</span></span><br><span class="line">a3.channels.c1.type = memory</span><br><span class="line">a3.channels.c1.capacity = 1000</span><br><span class="line">a3.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sinks</span></span><br><span class="line"><span class="comment"># 运行时设置参数 -Dflume.root.logger=INFO,console 即输出到控制台实时显示</span></span><br><span class="line">a3.sinks.k1.type = logger</span><br><span class="line"><span class="comment"># 设置Event的Body中写入log的最大字节数(默认值为16)</span></span><br><span class="line">a3.sinks.k1.maxBytesToLog = 256</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind</span></span><br><span class="line">a3.sources.r1.channels = c1</span><br><span class="line">a3.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><h3 id="4）对应功能"><a href="#4）对应功能" class="headerlink" title="4）对应功能"></a>4）对应功能</h3><p><strong>agent a1将指定端口的监听数据采用轮询的方式传输给a2和a3，并分别输出到各自的控制台</strong></p><h3 id="5）启动命令"><a href="#5）启动命令" class="headerlink" title="5）启动命令"></a>5）启动命令</h3><p><strong>Flume Agent a1至a3分别运行在主机hadoop101、hadoop102、hadoop103上</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./bin/flume-ng agent -n a1 -c conf -f flume1.properties</span><br><span class="line">./bin/flume-ng agent -n a2 -c conf -f flume2.properties -Dflume.root.logger=INFO,console</span><br><span class="line">./bin/flume-ng agent -n a3 -c conf -f flume3.properties -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><hr><h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
    
    </summary>
    
    
      <category term="Flume" scheme="https://tomandersen-cc.github.io/categories/Flume/"/>
    
    
      <category term="Linux" scheme="https://tomandersen-cc.github.io/tags/Linux/"/>
    
      <category term="Flume" scheme="https://tomandersen-cc.github.io/tags/Flume/"/>
    
      <category term="大数据" scheme="https://tomandersen-cc.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Flume之使用Failover Sink Processor实现sink故障转移</title>
    <link href="https://tomandersen-cc.github.io/2020/03/05/Flume%E4%B9%8B%E4%BD%BF%E7%94%A8Failover-Sink-Processor%E5%AE%9E%E7%8E%B0sink%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB/"/>
    <id>https://tomandersen-cc.github.io/2020/03/05/Flume%E4%B9%8B%E4%BD%BF%E7%94%A8Failover-Sink-Processor%E5%AE%9E%E7%8E%B0sink%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB/</id>
    <published>2020-03-05T13:29:34.000Z</published>
    <updated>2020-03-08T13:45:11.295Z</updated>
    
    <content type="html"><![CDATA[<hr><a id="more"></a><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul><li><strong>Failover Sink Processor</strong> 维护着Sink组中Sinks的优先级表，根据优先级尝试将Event传输给不同的Sink直到Event成功发送。当优先级高的Sink不可用时，会将Event传输给下一优先级Sink，以此来确保每个Event都能被投递。当Sink不可用时，Failover Sink Processor和<strong>Load balancing Sink Processor</strong>一样，也会进行指数回退backoff，并可以设置最大回退时间（即在黑名单中的保存时间），在倒计时结束后会再次尝试访问之前挂掉的Sink</li></ul><hr><h2 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a>使用示例</h2><h3 id="1）flume1-properties"><a href="#1）flume1-properties" class="headerlink" title="1）flume1.properties"></a>1）flume1.properties</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># flume1:此配置用于监控某个窗口将其追加内容输出到flume2和flume3中</span></span><br><span class="line"><span class="comment"># 并将两个Sink组成一个sink group,并将Sink Processor设置成Failover类型</span></span><br><span class="line"><span class="comment"># a1:Netcat Source-&gt;Memory Channel-&gt;Avro Sink</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sink groups</span></span><br><span class="line">a1.sinkgroups = g1</span><br><span class="line"><span class="comment"># 设置sink group中的sinks</span></span><br><span class="line">a1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line"><span class="comment"># 设置Failover sink processor(只有sink group才可以使用sink processor)</span></span><br><span class="line">a1.sinkgroups.g1.processor.type = failover</span><br><span class="line"><span class="comment"># 设置Failover sink processor优先级表</span></span><br><span class="line">a1.sinkgroups.g1.processor.priority.k1 = 5</span><br><span class="line">a1.sinkgroups.g1.processor.priority.k2 = 10</span><br><span class="line"><span class="comment"># 设置最大避让时间(ms)</span></span><br><span class="line">a1.sinkgroups.g1.processor.maxpenalty = 10000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Sources</span></span><br><span class="line"><span class="comment"># 配置a1.sources.r1的各项属性参数,类型/绑定主机ip/端口号</span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = hadoop101</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line"><span class="comment"># Channels</span></span><br><span class="line"><span class="comment"># 配置a1.channerls.c1的各项属性参数,缓存方式/最多缓存的Event个数/单次传输的Event个数</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sinks</span></span><br><span class="line"><span class="comment"># sinks.k1</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = hadoop102</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line"><span class="comment"># sinks.k2</span></span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = hadoop103</span><br><span class="line">a1.sinks.k2.port = 4141</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind</span></span><br><span class="line"><span class="comment"># 注意:source可以绑定多个channel,但是sink/sink group只能绑定单个channel</span></span><br><span class="line"><span class="comment"># r1-&gt;c1-&gt;g1</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c1</span><br></pre></td></tr></table></figure><h3 id="2）flume2-properties"><a href="#2）flume2-properties" class="headerlink" title="2）flume2.properties"></a>2）flume2.properties</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># flume2:此配置用于将来自指定Avro端口的数据输出到控制台</span></span><br><span class="line"><span class="comment"># a2:Avro Source-&gt;Memory Channel-&gt;Logger Sink</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent</span></span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.channels = c1</span><br><span class="line">a2.sinks = k1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sources</span></span><br><span class="line"><span class="comment"># a2.sources.r1</span></span><br><span class="line">a2.sources.r1.type = avro</span><br><span class="line"><span class="comment"># 设置监听本地IP</span></span><br><span class="line">a2.sources.r1.bind = 0.0.0.0</span><br><span class="line"><span class="comment"># 设置监听端口号</span></span><br><span class="line">a2.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line"><span class="comment"># Channels</span></span><br><span class="line"><span class="comment"># a2.channels.c1</span></span><br><span class="line"><span class="comment"># 使用内存作为缓存/最多缓存的Event个数/单次传输的Event个数</span></span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sinks</span></span><br><span class="line"><span class="comment"># 运行时设置参数 -Dflume.root.logger=INFO,console 即输出到控制台实时显示</span></span><br><span class="line">a2.sinks.k1.type = logger</span><br><span class="line"><span class="comment"># 设置Event的Body中写入log的最大字节数(默认值为16)</span></span><br><span class="line">a2.sinks.k1.maxBytesToLog = 256</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind</span></span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><h3 id="3）flume3-properties"><a href="#3）flume3-properties" class="headerlink" title="3）flume3.properties"></a>3）flume3.properties</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># flume3:此配置用于将来自指定Avro端口的数据输出到控制台</span></span><br><span class="line"><span class="comment"># a3:Avro Source-&gt;Memory Channel-&gt;Logger Sink</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent</span></span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.channels = c1</span><br><span class="line">a3.sinks = k1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sources</span></span><br><span class="line"><span class="comment"># a3.sources.r1</span></span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line"><span class="comment"># 设置监听本地IP</span></span><br><span class="line">a3.sources.r1.bind = 0.0.0.0</span><br><span class="line"><span class="comment"># 设置监听端口号</span></span><br><span class="line">a3.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line"><span class="comment"># Channels</span></span><br><span class="line"><span class="comment"># a3.channels.c1</span></span><br><span class="line"><span class="comment"># 使用内存作为缓存/最多缓存的Event个数/单次传输的Event个数</span></span><br><span class="line">a3.channels.c1.type = memory</span><br><span class="line">a3.channels.c1.capacity = 1000</span><br><span class="line">a3.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sinks</span></span><br><span class="line"><span class="comment"># 运行时设置参数 -Dflume.root.logger=INFO,console 即输出到控制台实时显示</span></span><br><span class="line">a3.sinks.k1.type = logger</span><br><span class="line"><span class="comment"># 设置Event的Body中写入log的最大字节数(默认值为16)</span></span><br><span class="line">a3.sinks.k1.maxBytesToLog = 256</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind</span></span><br><span class="line">a3.sources.r1.channels = c1</span><br><span class="line">a3.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><h3 id="4）对应功能"><a href="#4）对应功能" class="headerlink" title="4）对应功能"></a>4）对应功能</h3><ul><li><strong>Aent a1将指定端口的监听数据输出到a2或者a3的控制台</strong></li><li><strong>当Event从Channel中传输给Sink Group之前，首先会根据配置Failover sink processor优先级表尝试将此Event发送给优先级最高的可用Sink，如果成功则继续处理下一个Event。如果在发送过程中，当前Sink宕机，则将其加入黑名单，一定时间内不再尝试将Event发往此Sink，并且退避时间呈指数增长，直到最大退避时间maxpenalty，以此来实现Sink的故障转移</strong></li></ul><h3 id="5）启动命令"><a href="#5）启动命令" class="headerlink" title="5）启动命令"></a>5）启动命令</h3><p><strong>Flume Agent a1至a3分别运行在主机hadoop101、hadoop102、hadoop103上</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./bin/flume-ng agent -n a1 -c conf -f flume1.properties</span><br><span class="line">./bin/flume-ng agent -n a2 -c conf -f flume2.properties -Dflume.root.logger=INFO,console</span><br><span class="line">./bin/flume-ng agent -n a3 -c conf -f flume3.properties -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><hr><h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
    
    </summary>
    
    
      <category term="Flume" scheme="https://tomandersen-cc.github.io/categories/Flume/"/>
    
    
      <category term="Linux" scheme="https://tomandersen-cc.github.io/tags/Linux/"/>
    
      <category term="Flume" scheme="https://tomandersen-cc.github.io/tags/Flume/"/>
    
      <category term="大数据" scheme="https://tomandersen-cc.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Flume之Multiplexing Channel Selector使用示例</title>
    <link href="https://tomandersen-cc.github.io/2020/03/05/Flume%E4%B9%8BMultiplexing-Channel-Selector%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B/"/>
    <id>https://tomandersen-cc.github.io/2020/03/05/Flume%E4%B9%8BMultiplexing-Channel-Selector%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B/</id>
    <published>2020-03-05T12:58:33.000Z</published>
    <updated>2020-03-08T13:46:57.911Z</updated>
    
    <content type="html"><![CDATA[<hr><a id="more"></a><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul><li><strong>Multiplexing Channe Selector</strong> 的作用就是根据 <strong>Event</strong> 的 <strong>Header</strong> 中的某个或几个字段的值将其映射到指定的 <strong>Channel</strong> ，便于之后 <strong>Channel Processor</strong> 将Event发送至对应的Channel中去。在Flume中，Multiplexing Channel Selector一般都与 <strong>Interceptor</strong> 拦截器搭配使用，因为新鲜的Event数据中Header为空，需要Interceptor去填充所需字段</li></ul><hr><h2 id="具体配置"><a href="#具体配置" class="headerlink" title="具体配置"></a>具体配置</h2><h3 id="1）flume1-properties"><a href="#1）flume1-properties" class="headerlink" title="1）flume1.properties"></a>1）flume1.properties</h3><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># flume1:此配置用于监控单个或多个指定文件将其追加内容生成的Event先通过自定义的TypeInterceptor</span></span><br><span class="line"><span class="comment"># 根据Body中的内容向其Header中添加type字段,然后使用Multiplexing Channel Selector将不同</span></span><br><span class="line"><span class="comment"># type的Event传输到不同的Channel中,最后分别输出到flume2和flume3的控制台</span></span><br><span class="line"><span class="comment"># a1:TailDir Source-&gt; TypeInterceptor -&gt; Multiplexing Channel Selector -&gt;</span></span><br><span class="line"><span class="comment">#   Memory Channel -&gt; Avro Sink</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent</span></span><br><span class="line"><span class="meta">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="meta">a1.channels</span> = <span class="string">c1 c2</span></span><br><span class="line"><span class="meta">a1.sinks</span> = <span class="string">k1 k2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Sources</span></span><br><span class="line"><span class="comment"># a1.sources.r1</span></span><br><span class="line"><span class="meta">a1.sources.r1.type</span> = <span class="string">TAILDIR</span></span><br><span class="line"><span class="comment"># 设置Json文件存储路径(最好使用绝对路径)</span></span><br><span class="line"><span class="comment"># 用于记录文件inode/文件的绝对路径/每个文件的最后读取位置等信息</span></span><br><span class="line"><span class="meta">a1.sources.r1.positionFile</span> = <span class="string">/opt/module/flume-1.8.0/.position/taildir_position.json</span></span><br><span class="line"><span class="comment"># 指定监控的文件组</span></span><br><span class="line"><span class="meta">a1.sources.r1.filegroups</span> = <span class="string">f1</span></span><br><span class="line"><span class="comment"># 配置文件组中的被监控文件</span></span><br><span class="line"><span class="comment"># 设置f2组的监控文件,注意:使用的是正则表达式,而不是Linux通配符</span></span><br><span class="line"><span class="meta">a1.sources.r1.filegroups.f1</span> = <span class="string">/tmp/logs/^.*log$</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Interceptor</span></span><br><span class="line"><span class="comment"># a1.sources.r1.interceptors</span></span><br><span class="line"><span class="comment"># 配置Interceptor链,Interceptor调用顺序与配置循序相同</span></span><br><span class="line"><span class="meta">a1.sources.r1.interceptors</span> = <span class="string">typeInterceptor</span></span><br><span class="line"><span class="comment"># 指定使用的自定义Interceptor全类名,并使用其中的静态内部类Builder</span></span><br><span class="line"><span class="comment"># 要想使用自定义Interceptor,必须将实现的类打包成jar包放入$FLUME_HOME/lib文件夹中</span></span><br><span class="line"><span class="comment"># flume运行Java程序时会将此路径加入到ClassPath中</span></span><br><span class="line"><span class="meta">a1.sources.r1.interceptors.typeInterceptor.type</span> = <span class="string">com.tomandersen.interceptors.TypeInterceptor$Builder</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Channels</span></span><br><span class="line"><span class="comment"># a1.channels.c1</span></span><br><span class="line"><span class="comment"># 使用内存作为缓存/最多缓存的Event个数/单次传输的Event个数</span></span><br><span class="line"><span class="meta">a1.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="meta">a1.channels.c1.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="meta">a1.channels.c1.transactionCapacity</span> = <span class="string">100</span></span><br><span class="line"><span class="comment"># a1.channels.c2</span></span><br><span class="line"><span class="meta">a1.channels.c2.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="meta">a1.channels.c2.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="meta">a1.channels.c2.transactionCapacity</span> = <span class="string">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Channel Selector</span></span><br><span class="line"><span class="comment"># a1.sources.r1.selector</span></span><br><span class="line"><span class="comment"># 使用Multiple Channel Selector</span></span><br><span class="line"><span class="meta">a1.sources.r1.selector.type</span> = <span class="string">multiplexing</span></span><br><span class="line"><span class="comment"># 设置匹配Header的字段</span></span><br><span class="line"><span class="meta">a1.sources.r1.selector.header</span> = <span class="string">type</span></span><br><span class="line"><span class="comment"># 设置不同字段的值映射至各个Channel,其余的Event默认丢弃</span></span><br><span class="line"><span class="meta">a1.sources.r1.selector.mapping.Startup</span> = <span class="string">c1</span></span><br><span class="line"><span class="meta">a1.sources.r1.selector.mapping.Event</span> = <span class="string">c2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Sinks</span></span><br><span class="line"><span class="comment"># a1.sinks.k1</span></span><br><span class="line"><span class="meta">a1.sinks.k1.type</span> = <span class="string">avro</span></span><br><span class="line"><span class="meta">a1.sinks.k1.hostname</span> = <span class="string">hadoop102</span></span><br><span class="line"><span class="meta">a1.sinks.k1.port</span> = <span class="string">4141</span></span><br><span class="line"><span class="comment"># a1.sinks.k2</span></span><br><span class="line"><span class="meta">a1.sinks.k2.type</span> = <span class="string">avro</span></span><br><span class="line"><span class="meta">a1.sinks.k2.hostname</span> = <span class="string">hadoop103</span></span><br><span class="line"><span class="meta">a1.sinks.k2.port</span> = <span class="string">4141</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind</span></span><br><span class="line"><span class="comment"># r1-&gt;TypeInterceptor-&gt;Multiplexing Channel Selector-&gt;c1-&gt;k1</span></span><br><span class="line"><span class="comment"># r1-&gt;TypeInterceptor-&gt;Multiplexing Channel Selector-&gt;c2-&gt;k2</span></span><br><span class="line"><span class="meta">a1.sources.r1.channels</span> = <span class="string">c1 c2</span></span><br><span class="line"><span class="meta">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br><span class="line"><span class="meta">a1.sinks.k2.channel</span> = <span class="string">c2</span></span><br></pre></td></tr></table></figure><h3 id="2）flume2-properties"><a href="#2）flume2-properties" class="headerlink" title="2）flume2.properties"></a>2）flume2.properties</h3><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># flume2:此配置用于将来自指定Avro端口的数据输出到控制台中</span></span><br><span class="line"><span class="comment"># a2:Avro Source-&gt;Memory Channel-&gt;Logger Sink</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent</span></span><br><span class="line"><span class="meta">a2.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="meta">a2.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="meta">a2.sinks</span> = <span class="string">k1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Sources</span></span><br><span class="line"><span class="meta">a2.sources.r1.type</span> = <span class="string">avro</span></span><br><span class="line"><span class="meta">a2.sources.r1.bind</span> = <span class="string">0.0.0.0</span></span><br><span class="line"><span class="meta">a2.sources.r1.port</span> = <span class="string">4141</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Channels</span></span><br><span class="line"><span class="meta">a2.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="meta">a2.channels.c1.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="meta">a2.channels.c1.transactionCapacity</span> = <span class="string">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Sinks</span></span><br><span class="line"><span class="comment"># 运行时设置参数 -Dflume.root.logger=INFO,console 即输出到控制台实时显示</span></span><br><span class="line"><span class="meta">a2.sinks.k1.type</span> = <span class="string">logger</span></span><br><span class="line"><span class="comment"># 设置Event的Body中写入log的最大字节数(默认值为16)</span></span><br><span class="line"><span class="meta">a2.sinks.k1.maxBytesToLog</span> = <span class="string">256</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind</span></span><br><span class="line"><span class="attr">r1-&gt;c1-&gt;k1</span></span><br><span class="line"><span class="meta">a2.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="meta">a2.sinks.k1.channel</span> = <span class="string">c1</span></span><br></pre></td></tr></table></figure><h3 id="3）flume3-properties"><a href="#3）flume3-properties" class="headerlink" title="3）flume3.properties"></a>3）flume3.properties</h3><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># flume3:此配置用于将来自指定Avro端口的数据输出到控制台中</span></span><br><span class="line"><span class="comment"># a3:Avro Source-&gt;Memory Channel-&gt;Logger Sink</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent</span></span><br><span class="line"><span class="meta">a3.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="meta">a3.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="meta">a3.sinks</span> = <span class="string">k1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Sources</span></span><br><span class="line"><span class="meta">a3.sources.r1.type</span> = <span class="string">avro</span></span><br><span class="line"><span class="meta">a3.sources.r1.bind</span> = <span class="string">0.0.0.0</span></span><br><span class="line"><span class="meta">a3.sources.r1.port</span> = <span class="string">4141</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Channels</span></span><br><span class="line"><span class="meta">a3.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="meta">a3.channels.c1.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="meta">a3.channels.c1.transactionCapacity</span> = <span class="string">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Sinks</span></span><br><span class="line"><span class="comment"># 运行时设置参数 -Dflume.root.logger=INFO,console 即输出到控制台实时显示</span></span><br><span class="line"><span class="meta">a3.sinks.k1.type</span> = <span class="string">logger</span></span><br><span class="line"><span class="comment"># 设置Event的Body中写入log的最大字节数(默认值为16)</span></span><br><span class="line"><span class="meta">a3.sinks.k1.maxBytesToLog</span> = <span class="string">256</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind</span></span><br><span class="line"><span class="attr">r1-&gt;c1-&gt;k1</span></span><br><span class="line"><span class="meta">a3.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="meta">a3.sinks.k1.channel</span> = <span class="string">c1</span></span><br></pre></td></tr></table></figure><h3 id="4）对应功能"><a href="#4）对应功能" class="headerlink" title="4）对应功能"></a>4）对应功能</h3><p><strong>Agent a1监听本地指定文件,将监听到的数据组装成Event通过自定义的 TypeInterceptor 来根据其Body中的内容向Header中添加不同的type字段键值，然后通过 Multiplexing Channel Selector将不同type的Event发送给不同的Channel，并最终分别在a2和a3的控制台上输出</strong></p><h3 id="5）启动命令"><a href="#5）启动命令" class="headerlink" title="5）启动命令"></a>5）启动命令</h3><p><strong>Agent a1至a3分别运行在主机hadoop101、hadoop102、hadoop103上</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./bin/flume-ng agent -n a1 -c conf -f flume1.properties</span><br><span class="line">./bin/flume-ng agent -n a2 -c conf -f flume2.properties -Dflume.root.logger=INFO,console</span><br><span class="line">./bin/flume-ng agent -n a3 -c conf -f flume3.properties -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><hr><h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
    
    </summary>
    
    
      <category term="Flume" scheme="https://tomandersen-cc.github.io/categories/Flume/"/>
    
    
      <category term="Linux" scheme="https://tomandersen-cc.github.io/tags/Linux/"/>
    
      <category term="Flume" scheme="https://tomandersen-cc.github.io/tags/Flume/"/>
    
      <category term="大数据" scheme="https://tomandersen-cc.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="HDFS" scheme="https://tomandersen-cc.github.io/tags/HDFS/"/>
    
  </entry>
  
  <entry>
    <title>Flume之实现和使用自定义Interceptor</title>
    <link href="https://tomandersen-cc.github.io/2020/03/05/Flume%E4%B9%8B%E5%AE%9E%E7%8E%B0%E5%92%8C%E4%BD%BF%E7%94%A8%E8%87%AA%E5%AE%9A%E4%B9%89Interceptor/"/>
    <id>https://tomandersen-cc.github.io/2020/03/05/Flume%E4%B9%8B%E5%AE%9E%E7%8E%B0%E5%92%8C%E4%BD%BF%E7%94%A8%E8%87%AA%E5%AE%9A%E4%B9%89Interceptor/</id>
    <published>2020-03-05T12:41:52.000Z</published>
    <updated>2020-03-08T13:45:07.214Z</updated>
    
    <content type="html"><![CDATA[<hr><a id="more"></a><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul><li>Flume Interceptor拦截器的作用在于能够在Event从Source传输到Channel过程中，修改或者删除Event的Header。多个拦截器Interceptor组成一个拦截器链，拦截器的执行顺序与配置顺序相同，上一个拦截器Interceptor处理后的Event List会传给下一个Interceptor</li><li>在Flume中自定义Interceptor时，需要实现org.apache.flume.interceptor.Interceptor接口，以及创建静态内部类去实现org.apache.flume.interceptor.Interceptor.Builder接口</li><li>更多详细内容可以参考《Flume构建高可用、可扩展的海量日志采集系统》</li></ul><hr><h2 id="实现自定义Interceptor拦截器"><a href="#实现自定义Interceptor拦截器" class="headerlink" title="实现自定义Interceptor拦截器"></a>实现自定义Interceptor拦截器</h2><h3 id="1）根据使用场景创建Interceptor类"><a href="#1）根据使用场景创建Interceptor类" class="headerlink" title="1）根据使用场景创建Interceptor类"></a>1）根据使用场景创建Interceptor类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TypeInterceptor</span> <span class="keyword">implements</span> <span class="title">Interceptor</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化时可以不做操作</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// Do nothing</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 单个Event拦截</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Event <span class="title">intercept</span><span class="params">(<span class="keyword">final</span> Event event)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 1.获取Event中的Header</span></span><br><span class="line">        Map&lt;String, String&gt; headers = event.getHeaders();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.获取Event中的Body,将其转换成字符串String</span></span><br><span class="line">        String body = <span class="keyword">new</span> String(event.getBody());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.根据Body中数据向Header添加键值对,表明日志类型</span></span><br><span class="line">        <span class="keyword">if</span> (body.contains(<span class="string">"cm"</span>)) &#123;</span><br><span class="line">            <span class="comment">// 4.添加Header信息</span></span><br><span class="line">            headers.put(<span class="string">"type"</span>, <span class="string">"Startup"</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// 4.添加Header信息</span></span><br><span class="line">            headers.put(<span class="string">"type"</span>, <span class="string">"Event"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> event;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 批量Event拦截</span></span><br><span class="line">    <span class="comment">// 注意:既可以原Event集合进行修改,也可以创建新的Event集合作为成员变量,将此成员变量返回</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Event&gt; <span class="title">intercept</span><span class="params">(<span class="keyword">final</span> List&lt;Event&gt; events)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (Event event : events) &#123;</span><br><span class="line">            <span class="comment">// 1.对每个Event采用单个Event拦截的方式进行处理,忽略其返回值</span></span><br><span class="line">            intercept(event);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 2.返回处理结果</span></span><br><span class="line">        <span class="keyword">return</span> events;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭时可以不作操作</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// Do nothing</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建静态内部类实现Interceptor.Builder接口</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Builder</span> <span class="keyword">implements</span> <span class="title">Interceptor</span>.<span class="title">Builder</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 定义配置信息</span></span><br><span class="line">        <span class="keyword">private</span> Context context;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 定义Interceptor生成器</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Interceptor <span class="title">build</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> TypeInterceptor();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取配置信息</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.context = context;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2）将此Interceptor类打包，并将jar包放入flume-lib路径下"><a href="#2）将此Interceptor类打包，并将jar包放入flume-lib路径下" class="headerlink" title="2）将此Interceptor类打包，并将jar包放入flume/lib路径下"></a>2）将此Interceptor类打包，并将jar包放入<code>flume/lib</code>路径下</h3><p><strong>此路径在Flume运行时的ClassPath中，因而可以在flume配置文件中可以通过全类名指定使用的Interceptor.Builder类</strong></p><p><strong>Maven项目打包插件配置可以参考：</strong><a href="https://blog.csdn.net/TomAndersen/article/details/104245064" target="_blank" rel="noopener">《IDEA中配置Maven项目打包插件》</a></p><hr><h2 id="创建flume-Agent配置文件"><a href="#创建flume-Agent配置文件" class="headerlink" title="创建flume Agent配置文件"></a>创建flume Agent配置文件</h2><h3 id="1）flume1-properties"><a href="#1）flume1-properties" class="headerlink" title="1）flume1.properties"></a>1）flume1.properties</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># flume1:此配置用于监控单个或多个指定文件将其追加内容先通过自定义的TypeInterceptor</span></span><br><span class="line"><span class="comment"># 向Header中添加type字段,然后使用Multiplexing Channel Selector将不同type的Event</span></span><br><span class="line"><span class="comment"># 传输到不同的Channel中,最后分别输出到flume2和flume3的控制台</span></span><br><span class="line"><span class="comment"># a1:TailDir Source-&gt; TypeInterceptor -&gt; Multiplexing Channel Selector -&gt;</span></span><br><span class="line"><span class="comment">#   Memory Channel -&gt; Avro Sink</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1 c2</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sources</span></span><br><span class="line"><span class="comment"># a1.sources.r1</span></span><br><span class="line">a1.sources.r1.type = TAILDIR</span><br><span class="line"><span class="comment"># 设置Json文件存储路径(最好使用绝对路径)</span></span><br><span class="line"><span class="comment"># 用于记录文件inode/文件的绝对路径/每个文件的最后读取位置等信息</span></span><br><span class="line">a1.sources.r1.positionFile = /opt/module/flume-1.8.0/.position/taildir_position.json</span><br><span class="line"><span class="comment"># 指定监控的文件组</span></span><br><span class="line">a1.sources.r1.filegroups = f1</span><br><span class="line"><span class="comment"># 配置文件组中的被监控文件</span></span><br><span class="line"><span class="comment"># 设置f2组的监控文件,注意:使用的是正则表达式,而不是Linux通配符</span></span><br><span class="line">a1.sources.r1.filegroups.f1 = /tmp/logs/^.*<span class="built_in">log</span>$</span><br><span class="line"></span><br><span class="line"><span class="comment"># Interceptor</span></span><br><span class="line"><span class="comment"># a1.sources.r1.interceptors</span></span><br><span class="line"><span class="comment"># 配置Interceptor链,Interceptor调用顺序与配置循序相同</span></span><br><span class="line">a1.sources.r1.interceptors = typeInterceptor</span><br><span class="line"><span class="comment"># 指定使用的自定义Interceptor全类名,并使用其中的静态内部类Builder</span></span><br><span class="line"><span class="comment"># 要想使用自定义Interceptor,必须将实现的类打包成jar包放入$FLUME_HOME/lib文件夹中</span></span><br><span class="line"><span class="comment"># flume运行Java程序时会将此路径加入到ClassPath中</span></span><br><span class="line">a1.sources.r1.interceptors.typeInterceptor.type = com.tomandersen.interceptors.TypeInterceptor<span class="variable">$Builder</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Channels</span></span><br><span class="line"><span class="comment"># a1.channels.c1</span></span><br><span class="line"><span class="comment"># 使用内存作为缓存/最多缓存的Event个数/单次传输的Event个数</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="comment"># a1.channels.c2</span></span><br><span class="line">a1.channels.c2.type = memory</span><br><span class="line">a1.channels.c2.capacity = 1000</span><br><span class="line">a1.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="comment"># Channel Selector</span></span><br><span class="line"><span class="comment"># a1.sources.r1.selector</span></span><br><span class="line"><span class="comment"># 使用Multiple Channel Selector</span></span><br><span class="line">a1.sources.r1.selector.type = multiplexing</span><br><span class="line"><span class="comment"># 设置匹配Header的字段</span></span><br><span class="line">a1.sources.r1.selector.header = <span class="built_in">type</span></span><br><span class="line"><span class="comment"># 设置不同字段的值映射至各个Channel,其余的Event默认丢弃</span></span><br><span class="line">a1.sources.r1.selector.mapping.Startup = c1</span><br><span class="line">a1.sources.r1.selector.mapping.Event = c2</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sinks</span></span><br><span class="line"><span class="comment"># a1.sinks.k1</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = hadoop102</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line"><span class="comment"># a1.sinks.k2</span></span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = hadoop103</span><br><span class="line">a1.sinks.k2.port = 4141</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind</span></span><br><span class="line"><span class="comment"># r1-&gt;TypeInterceptor-&gt;Multiplexing Channel Selector-&gt;c1-&gt;k1</span></span><br><span class="line"><span class="comment"># r1-&gt;TypeInterceptor-&gt;Multiplexing Channel Selector-&gt;c2-&gt;k2</span></span><br><span class="line">a1.sources.r1.channels = c1 c2</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c2</span><br></pre></td></tr></table></figure><h3 id="2）flume2-properties"><a href="#2）flume2-properties" class="headerlink" title="2）flume2.properties"></a>2）flume2.properties</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># flume2:此配置用于将来自指定Avro端口的数据输出到控制台中</span></span><br><span class="line"><span class="comment"># a2:Avro Source-&gt;Memory Channel-&gt;Logger Sink</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent</span></span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.channels = c1</span><br><span class="line">a2.sinks = k1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sources</span></span><br><span class="line">a2.sources.r1.type = avro</span><br><span class="line">a2.sources.r1.bind = 0.0.0.0</span><br><span class="line">a2.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line"><span class="comment"># Channels</span></span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sinks</span></span><br><span class="line"><span class="comment"># 运行时设置参数 -Dflume.root.logger=INFO,console 即输出到控制台实时显示</span></span><br><span class="line">a2.sinks.k1.type = logger</span><br><span class="line"><span class="comment"># 设置Event的Body中写入log的最大字节数(默认值为16)</span></span><br><span class="line">a2.sinks.k1.maxBytesToLog = 256</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind</span></span><br><span class="line">r1-&gt;c1-&gt;k1</span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><h3 id="3）flume3-properties"><a href="#3）flume3-properties" class="headerlink" title="3）flume3.properties"></a>3）flume3.properties</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># flume3:此配置用于将来自指定Avro端口的数据输出到控制台中</span></span><br><span class="line"><span class="comment"># a3:Avro Source-&gt;Memory Channel-&gt;Logger Sink</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent</span></span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.channels = c1</span><br><span class="line">a3.sinks = k1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sources</span></span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = 0.0.0.0</span><br><span class="line">a3.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line"><span class="comment"># Channels</span></span><br><span class="line">a3.channels.c1.type = memory</span><br><span class="line">a3.channels.c1.capacity = 1000</span><br><span class="line">a3.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sinks</span></span><br><span class="line"><span class="comment"># 运行时设置参数 -Dflume.root.logger=INFO,console 即输出到控制台实时显示</span></span><br><span class="line">a3.sinks.k1.type = logger</span><br><span class="line"><span class="comment"># 设置Event的Body中写入log的最大字节数(默认值为16)</span></span><br><span class="line">a3.sinks.k1.maxBytesToLog = 256</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind</span></span><br><span class="line">r1-&gt;c1-&gt;k1</span><br><span class="line">a3.sources.r1.channels = c1</span><br><span class="line">a3.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><h3 id="4）对应功能"><a href="#4）对应功能" class="headerlink" title="4）对应功能"></a>4）对应功能</h3><p><strong>Flume Agent a1监听本地指定文件,将监听到的数据组装成Event通过自定义的 TypeInterceptor 来根据其Body中的内容向Header中添加不同的type字段键值，然后通过 Multiplexing Channel Selector将不同type的Event发送给不同的Channel，并最终分别在Flume Agent a2和a3的控制台上输出</strong></p><h3 id="5）启动命令"><a href="#5）启动命令" class="headerlink" title="5）启动命令"></a>5）启动命令</h3><p><strong>Flume Agent a1至a3分别运行在主机hadoop101、hadoop102、hadoop103上</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./bin/flume-ng agent -n a1 -c conf -f flume1.properties</span><br><span class="line">./bin/flume-ng agent -n a2 -c conf -f flume2.properties -Dflume.root.logger=INFO,console</span><br><span class="line">./bin/flume-ng agent -n a3 -c conf -f flume3.properties -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><hr><h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
    
    </summary>
    
    
      <category term="Flume" scheme="https://tomandersen-cc.github.io/categories/Flume/"/>
    
    
      <category term="Linux" scheme="https://tomandersen-cc.github.io/tags/Linux/"/>
    
      <category term="Flume" scheme="https://tomandersen-cc.github.io/tags/Flume/"/>
    
      <category term="大数据" scheme="https://tomandersen-cc.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Maven之子模块pom.xml继承父模块pom.xml配置</title>
    <link href="https://tomandersen-cc.github.io/2020/03/05/Maven%E4%B9%8B%E5%AD%90%E6%A8%A1%E5%9D%97pom-xml%E7%BB%A7%E6%89%BF%E7%88%B6%E6%A8%A1%E5%9D%97pom-xml%E9%85%8D%E7%BD%AE/"/>
    <id>https://tomandersen-cc.github.io/2020/03/05/Maven%E4%B9%8B%E5%AD%90%E6%A8%A1%E5%9D%97pom-xml%E7%BB%A7%E6%89%BF%E7%88%B6%E6%A8%A1%E5%9D%97pom-xml%E9%85%8D%E7%BD%AE/</id>
    <published>2020-03-05T12:17:42.000Z</published>
    <updated>2020-03-06T12:11:10.228Z</updated>
    
    <content type="html"><![CDATA[<hr><a id="more"></a><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Maven中可以通过继承父模块pom，来实现pom.xml配置的继承和传递，便于各种Maven插件以及程序依赖的统一管理。通过将子类模块的公共配置，抽象聚合生成父类模块，能够避免pom.xml的重复配置。由于父类模块本身并不包含除了POM之外的项目文件，也就不需要src/main/java之类的文件夹了。每当需要对多个子模块进行相同的配置时，只需要在父类模块的pom中进行配置，而子类中声明使用此配置即可，当然子类pom中也可以自定义配置，并覆盖父类中的各项配置，和Java中类的继承类似。</p><hr><h2 id="可继承的POM元素"><a href="#可继承的POM元素" class="headerlink" title="可继承的POM元素"></a>可继承的POM元素</h2><p><strong>1) <code>groupId</code>：项目组ID，项目坐标的核心元素</strong></p><p><strong>2) <code>version</code>：项目版本，项目坐标的核心元素</strong></p><p><strong>3) <code>description</code>：项目的表述信息</strong></p><p><strong>4) <code>organization</code>：项目的组织信息</strong></p><p><strong>5) <code>inception Year</code>：项目的创始年份</strong></p><p><strong>6) <code>url</code>：项目的URL地址</strong></p><p><strong>7) <code>developers</code>：项目的开发者信息</strong></p><p><strong>8) <code>contributors</code>：项目的贡献者信息</strong></p><p><strong>9) <code>distributionManagement</code>：项目的部署管理</strong></p><p><strong>10) <code>issueManagement</code>：项目的缺陷和跟踪系统信息</strong></p><p><strong>11) <code>ciManagement</code>：项目的持续集成信息系统</strong></p><p><strong>12) <code>scm</code>：项目的版本控制系统信息</strong></p><p><strong>13) <code>mailingLists</code>：项目的邮件列表信息</strong></p><p><strong>14) <code>properties</code>：自定义的Maven属性</strong></p><p><strong>15) <code>dependencies</code>：项目的依赖属性</strong></p><p><strong>16) <code>dependencyManagement</code>：项目的依赖管理配置</strong></p><p><strong>17) <code>repositories</code>：项目的仓库配置</strong></p><p><strong>18) <code>build</code>：包括项目的源码目录配置、输出目录配置、插件配置、插件管理配置等</strong></p><p><strong>19) <code>reporting</code>：包括项目的报告输出目录配置、报告插件配置等</strong></p><hr><h2 id="POM继承中的依赖管理和插件管理"><a href="#POM继承中的依赖管理和插件管理" class="headerlink" title="POM继承中的依赖管理和插件管理"></a>POM继承中的依赖管理和插件管理</h2><p>Maven提供的<code>dependencyManagement</code>和<code>pluginManagement</code>元素用于帮助POM继承过程中的依赖管理和插件管理。在父类POM下，此两个元素中的声明的依赖或配置并不会引入实际的依赖或是造成实际的插件调用行为，不过它们能够约束子类POM中的依赖和插件配置的声明。只有当子类POM中配置了真正的<code>dependency</code>或<code>plugin</code>，并且其<code>groupId</code>和<code>artifactId</code>与父类POM中<code>dependencyManagement</code>和<code>pluginManagement</code>相对应时，才会进行实际的依赖引入或插件调用，当然子类中也能够进行自定义配置去覆盖父类，或是额外声明自己的配置</p><hr><h2 id="POM继承示例"><a href="#POM继承示例" class="headerlink" title="POM继承示例"></a>POM继承示例</h2><p><strong>1) 在父类POM中使用<code>dependencyManagement</code>和<code>pluginManagement</code>，声明子类POM中可能用到的依赖和插件</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.tomandersen<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>HadoopCustomModules<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">packaging</span>&gt;</span>pom<span class="tag">&lt;/<span class="name">packaging</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">modules</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">module</span>&gt;</span>flume<span class="tag">&lt;/<span class="name">module</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">module</span>&gt;</span>log-collector<span class="tag">&lt;/<span class="name">module</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">modules</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--事先声明版本属性--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">slf4j.version</span>&gt;</span>1.7.20<span class="tag">&lt;/<span class="name">slf4j.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">logback.version</span>&gt;</span>1.0.7<span class="tag">&lt;/<span class="name">logback.version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--在父类Maven中使用dependencyManagement声明依赖便于子类Module继承使用,也便于进行依赖版本控制--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencyManagement</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--阿里巴巴开源json解析框架--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.alibaba<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>fastjson<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.51<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!--日志生成框架--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>ch.qos.logback<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>logback-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;logback.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>ch.qos.logback<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>logback-classic<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;logback.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencyManagement</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--在父类Maven中使用pluginManagement管理插件便于子类Module继承使用,也便于进行依赖版本控制--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">pluginManagement</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!--Maven项目编译器compiler插件--&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.3.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">&lt;!--Maven项目汇编assembly插件--&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="name">descriptorRef</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">archive</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">manifest</span>&gt;</span></span><br><span class="line">                            <span class="comment">&lt;!--子类Maven通过mainClass标签设置成主类的全类名FQCN--&gt;</span></span><br><span class="line">                            <span class="comment">&lt;!--&lt;mainClass&gt;&lt;/mainClass&gt;--&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">manifest</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">archive</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">id</span>&gt;</span>make-assembly<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>single<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">pluginManagement</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>2) 在子类POM中声明父类POM，并配置实际使用的<code>dependency</code>和<code>plugin</code>，只需要通过声明<code>groupId</code>和<code>artifactId</code>就可以避免配置各种依赖和插件的详细配置，当然也可以自己覆盖父类配置信息</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--声明父类POM--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">parent</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>HadoopCustomModules<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.tomandersen<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">parent</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--子类POM信息--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.tomandersen<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log-collector<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--阿里巴巴开源json解析框架--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.alibaba<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>fastjson<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--日志生成框架--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>ch.qos.logback<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>logback-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>ch.qos.logback<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>logback-classic<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--自定义Maven项目编译器compiler插件相关配置--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!--自定义Maven项目汇编assembly插件相关配置--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">archive</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">manifest</span>&gt;</span></span><br><span class="line">                        <span class="comment">&lt;!--此处设置成主类的全名--&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span>com.tomandersen.appclient.AppMain<span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">manifest</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">archive</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure><hr><h2 id="更多详细内容可以查阅《Maven实战》"><a href="#更多详细内容可以查阅《Maven实战》" class="headerlink" title="更多详细内容可以查阅《Maven实战》"></a>更多详细内容可以查阅《Maven实战》</h2><hr><h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
    
    </summary>
    
    
      <category term="Maven" scheme="https://tomandersen-cc.github.io/categories/Maven/"/>
    
    
      <category term="Maven" scheme="https://tomandersen-cc.github.io/tags/Maven/"/>
    
      <category term="XML" scheme="https://tomandersen-cc.github.io/tags/XML/"/>
    
  </entry>
  
  <entry>
    <title>Flume之HDFS-Sink使用案例</title>
    <link href="https://tomandersen-cc.github.io/2020/03/05/Flume%E4%B9%8BHDFS-Sink%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B/"/>
    <id>https://tomandersen-cc.github.io/2020/03/05/Flume%E4%B9%8BHDFS-Sink%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B/</id>
    <published>2020-03-05T03:39:58.000Z</published>
    <updated>2020-03-08T13:41:28.431Z</updated>
    
    <content type="html"><![CDATA[<hr><a id="more"></a><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul><li>操作系统：CentOS 7</li><li>Java版本：1.8.0_221</li><li>Flume版本：1.8.0</li><li>HDFS版本：2.7.7</li><li>Flume agent配置：Netcat TCP Source、Memory Channel、HDFS Sink</li></ul><hr><h2 id="具体步骤"><a href="#具体步骤" class="headerlink" title="具体步骤"></a>具体步骤</h2><h3 id="a-拷贝Hadoop相关jar包至flume-lib-路径下"><a href="#a-拷贝Hadoop相关jar包至flume-lib-路径下" class="headerlink" title="a) 拷贝Hadoop相关jar包至flume/lib/路径下"></a>a) 拷贝Hadoop相关jar包至<code>flume/lib/</code>路径下</h3><h4 id="在hadoop-2-7-7-share-路径下找到以下对应jar包，并将其拷贝至flume-lib-路径下。Flume启动时，会将此路径添加至ClassPath"><a href="#在hadoop-2-7-7-share-路径下找到以下对应jar包，并将其拷贝至flume-lib-路径下。Flume启动时，会将此路径添加至ClassPath" class="headerlink" title="在hadoop-2.7.7/share/路径下找到以下对应jar包，并将其拷贝至flume/lib/路径下。Flume启动时，会将此路径添加至ClassPath"></a>在<code>hadoop-2.7.7/share/</code>路径下找到以下对应jar包，并将其拷贝至<code>flume/lib/</code>路径下。Flume启动时，会将此路径添加至ClassPath</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">commons-configuration-1.6.jar</span><br><span class="line">commons-io-2.4.jar</span><br><span class="line">hadoop-auth-2.7.7.jar</span><br><span class="line">hadoop-common-2.7.7.jar</span><br><span class="line">hadoop-hdfs-2.7.7.jar</span><br><span class="line">htrace-core-3.1.0-incubating.jar</span><br></pre></td></tr></table></figure><h3 id="b-根据使用场景配置properties文件"><a href="#b-根据使用场景配置properties文件" class="headerlink" title="b) 根据使用场景配置properties文件"></a>b) 根据使用场景配置properties文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用于从netcat指定端口收集数据最终输出到HDFS中</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sources</span></span><br><span class="line"><span class="comment"># a1.sources.r1</span></span><br><span class="line"><span class="comment"># 配置source类型/绑定主机ip/端口号</span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = 0.0.0.0</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sinks</span></span><br><span class="line"><span class="comment"># a1.sinks.k1</span></span><br><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line"><span class="comment"># 设置hdfs文件路径,同时并设置了按照日期创建文件夹</span></span><br><span class="line">a1.sinks.k1.hdfs.path = /flume/logs/%Y-%m-%d/%H-%M-%S</span><br><span class="line"><span class="comment"># 设置flume创建的hdfs文件前缀</span></span><br><span class="line">a1.sinks.k1.hdfs.filePrefix = logs_%Y-%m-%d</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以下三组参数的配置用于控制flume在hdfs中生成文件的滚动方式</span></span><br><span class="line"><span class="comment"># 满足以下三者中任何一个条件都会新生成hdfs文件</span></span><br><span class="line"><span class="comment"># 设置文件滚动的时间间隔,单位(second),置0表示关闭</span></span><br><span class="line">a1.sinks.k1.hdfs.rollInterval = 10</span><br><span class="line"><span class="comment"># 设置文件滚动的最大size阈值,由于是hdfs sink故最好设置成Block Size的倍数</span></span><br><span class="line"><span class="comment"># 本次实验的hadoop版本为2.7.7(2.7.3之后默认Block Size为128MB,之前为64MB)</span></span><br><span class="line"><span class="comment"># 单位(bytes),置0表示关闭</span></span><br><span class="line">a1.sinks.k1.hdfs.rollSize = 134217700</span><br><span class="line"><span class="comment"># 设置滚动文件存储的最大Event个数</span></span><br><span class="line"><span class="comment"># 此参数一般设置为0,即关闭,除非有严格生产需求并且知道Event大小能够自主控制</span></span><br><span class="line">a1.sinks.k1.hdfs.rollCount = 0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置flume每批次刷到hdfs中的Event个数(超过一定时长也会进行刷新,并非要等满一批次)</span></span><br><span class="line">a1.sinks.k1.hdfs.batchSize = 100</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置hdfs文件格式,目前只支持(SequenceFile/DataStream/CompressedStream)</span></span><br><span class="line"><span class="comment"># CompressedStream类型需要配合hdfs.codeC参数来指定具体的压缩方式</span></span><br><span class="line"><span class="comment"># SequenceFile表示按照HDFS序列文件SequenceFile的方式进行压缩</span></span><br><span class="line"><span class="comment"># DataStream则表示不进行压缩</span></span><br><span class="line">a1.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以下三组参数的配置配合转义序列(如%y %m %d %H %M %S等)能够自定义时间轮转最小刻度</span></span><br><span class="line"><span class="comment"># 设置hdfs时间向下取整</span></span><br><span class="line"><span class="comment"># 设置向下取整之后文件夹将按照一定时间大小的刻度进行创建文件夹</span></span><br><span class="line"><span class="comment"># 否则都是按照之前设置每分钟进行文件夹的创建</span></span><br><span class="line">a1.sinks.k1.hdfs.round = <span class="literal">true</span></span><br><span class="line"><span class="comment"># 设置hdfs时间向下取整的最小单元倍数</span></span><br><span class="line">a1.sinks.k1.hdfs.roundValue = 30</span><br><span class="line"><span class="comment"># 设置hdfs时间向下取整的最小单位</span></span><br><span class="line">a1.sinks.k1.hdfs.roundUnit = second</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设定是否使用本地时间戳,默认为false(即使用Event的Header中的时间戳)</span></span><br><span class="line"><span class="comment"># 本次实验中Event的Header为空,需要使用本地时间戳</span></span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Channels</span></span><br><span class="line"><span class="comment"># 定义a2的channerls.c1的类型为memory,即使用内存作为缓存/最多缓存的Event个数/单次传输的Event个数</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind</span></span><br><span class="line"><span class="comment"># 注意:source可以绑定多个channel,但是sink只能绑定单个channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><h3 id="c-使用此配置文件启动agent"><a href="#c-使用此配置文件启动agent" class="headerlink" title="c) 使用此配置文件启动agent"></a>c) 使用此配置文件启动agent</h3><h4 id="启动脚本前保证HDFS集群正常运行"><a href="#启动脚本前保证HDFS集群正常运行" class="headerlink" title="启动脚本前保证HDFS集群正常运行"></a>启动脚本前保证HDFS集群正常运行</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[tomandersen@hadoop101 flume-1.8.0]$ call-cluster.sh jps</span><br><span class="line">----------hadoop103----------</span><br><span class="line">18272 Jps</span><br><span class="line">17794 DataNode</span><br><span class="line">17987 NodeManager</span><br><span class="line">18105 JobHistoryServer</span><br><span class="line">17868 SecondaryNameNode</span><br><span class="line">----------hadoop102----------</span><br><span class="line">17826 DataNode</span><br><span class="line">18457 Jps</span><br><span class="line">17950 ResourceManager</span><br><span class="line">18079 NodeManager</span><br><span class="line">----------hadoop101----------</span><br><span class="line">10321 DataNode</span><br><span class="line">10785 Jps</span><br><span class="line">10619 NodeManager</span><br><span class="line">10205 NameNode</span><br><span class="line"></span><br><span class="line">----------execute <span class="string">"jps"</span> <span class="keyword">in</span> cluster takes 6 seconds----------</span><br><span class="line"></span><br><span class="line">[tomandersen@hadoop101 flume-1.8.0]$</span><br></pre></td></tr></table></figure><h4 id="在Flume安装路径下通过bin-flume-ng脚本启动agent"><a href="#在Flume安装路径下通过bin-flume-ng脚本启动agent" class="headerlink" title="在Flume安装路径下通过bin/flume-ng脚本启动agent"></a>在Flume安装路径下通过<code>bin/flume-ng</code>脚本启动agent</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/flume-ng agent -n a1 -c conf/ -f job/netcat-memory-hdfs.properties</span><br></pre></td></tr></table></figure><h3 id="d-发送测试数据并检查HDFS中是否成功上传对应数据"><a href="#d-发送测试数据并检查HDFS中是否成功上传对应数据" class="headerlink" title="d) 发送测试数据并检查HDFS中是否成功上传对应数据"></a>d) 发送测试数据并检查HDFS中是否成功上传对应数据</h3><h4 id="发送测试数据"><a href="#发送测试数据" class="headerlink" title="发送测试数据"></a>发送测试数据</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[tomandersen@hadoop101 ~]$ <span class="built_in">echo</span> Hello World! | nc localhost 44444</span><br><span class="line">OK</span><br><span class="line">[tomandersen@hadoop101 ~]$ </span><br><span class="line">[tomandersen@hadoop101 ~]$</span><br></pre></td></tr></table></figure><h4 id="进入NameNode-Web-UI页面查看HDFS文件"><a href="#进入NameNode-Web-UI页面查看HDFS文件" class="headerlink" title="进入NameNode Web UI页面查看HDFS文件"></a>进入NameNode Web UI页面查看HDFS文件</h4><p><img src="https://img-blog.csdnimg.cn/20200305113842376.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="查看HDFS文件"></p><h4 id="下载并查看查看HDFS文件内容"><a href="#下载并查看查看HDFS文件内容" class="headerlink" title="下载并查看查看HDFS文件内容"></a>下载并查看查看HDFS文件内容</h4><p><img src="https://img-blog.csdnimg.cn/20200305113859854.png" alt="查看HDFS文件"></p><hr><h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
    
    </summary>
    
    
      <category term="Flume" scheme="https://tomandersen-cc.github.io/categories/Flume/"/>
    
    
      <category term="Linux" scheme="https://tomandersen-cc.github.io/tags/Linux/"/>
    
      <category term="Flume" scheme="https://tomandersen-cc.github.io/tags/Flume/"/>
    
      <category term="大数据" scheme="https://tomandersen-cc.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="HDFS" scheme="https://tomandersen-cc.github.io/tags/HDFS/"/>
    
  </entry>
  
  <entry>
    <title>Flume之入门级安装部署</title>
    <link href="https://tomandersen-cc.github.io/2020/03/05/Flume%E4%B9%8B%E5%85%A5%E9%97%A8%E7%BA%A7%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"/>
    <id>https://tomandersen-cc.github.io/2020/03/05/Flume%E4%B9%8B%E5%85%A5%E9%97%A8%E7%BA%A7%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/</id>
    <published>2020-03-05T01:30:13.000Z</published>
    <updated>2020-03-06T11:43:56.498Z</updated>
    
    <content type="html"><![CDATA[<hr><a id="more"></a><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul><li>操作系统：CentOS 7</li><li>Java版本：1.8.0_221</li><li>Flume版本：1.8.0</li></ul><hr><h2 id="具体步骤"><a href="#具体步骤" class="headerlink" title="具体步骤"></a>具体步骤</h2><h3 id="a-下载flume"><a href="#a-下载flume" class="headerlink" title="a) 下载flume"></a>a) 下载flume</h3><ul><li>flume官网：<a href="https://flume.apache.org/" target="_blank" rel="noopener">https://flume.apache.org/</a></li><li>flume最新版：<a href="https://flume.apache.org/download.html" target="_blank" rel="noopener">https://flume.apache.org/download.html</a></li><li>flume各个历史版本：<a href="http://archive.apache.org/dist/flume/" target="_blank" rel="noopener">http://archive.apache.org/dist/flume/</a></li></ul><h3 id="b-安装flume"><a href="#b-安装flume" class="headerlink" title="b) 安装flume"></a>b) 安装flume</h3><h4 id="将flume解压到指定路径下"><a href="#将flume解压到指定路径下" class="headerlink" title="将flume解压到指定路径下"></a>将flume解压到指定路径下</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -xzvf apache-flume-1.8.0-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure><h4 id="修改flume默认文件夹名（可选）"><a href="#修改flume默认文件夹名（可选）" class="headerlink" title="修改flume默认文件夹名（可选）"></a>修改flume默认文件夹名（可选）</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv apache-flume-1.8.0-bin flume-1.8.0</span><br></pre></td></tr></table></figure><h3 id="c-配置flume-env-sh文件"><a href="#c-配置flume-env-sh文件" class="headerlink" title="c) 配置flume-env.sh文件"></a>c) 配置flume-env.sh文件</h3><h4 id="拷贝flume-1-8-0-conf-flume-env-sh-template，创建flume-env-sh文件"><a href="#拷贝flume-1-8-0-conf-flume-env-sh-template，创建flume-env-sh文件" class="headerlink" title="拷贝flume-1.8.0/conf/flume-env.sh.template，创建flume-env.sh文件"></a>拷贝<code>flume-1.8.0/conf/flume-env.sh.template</code>，创建<code>flume-env.sh</code>文件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp flume-env.sh.template flume-env.sh</span><br></pre></td></tr></table></figure><h4 id="或者直接重命名"><a href="#或者直接重命名" class="headerlink" title="或者直接重命名"></a>或者直接重命名</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv flume-env.sh.template flume-env.sh</span><br></pre></td></tr></table></figure><h4 id="修改flume-env-sh中JAVA-HOME，将其设置成Java安装绝对路径"><a href="#修改flume-env-sh中JAVA-HOME，将其设置成Java安装绝对路径" class="headerlink" title="修改flume-env.sh中JAVA_HOME，将其设置成Java安装绝对路径"></a>修改<code>flume-env.sh</code>中<code>JAVA_HOME</code>，将其设置成Java安装绝对路径</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vim flume-env.sh</span><br><span class="line"><span class="comment"># 修改前</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/java-8-oracle</span><br><span class="line"><span class="comment"># 修改后</span></span><br><span class="line"><span class="comment"># export JAVA_HOME=/usr/lib/jvm/java-8-oracle</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_221</span><br></pre></td></tr></table></figure><h4 id="至此flume就已经配置完成了，flume运行日志的默认数据追加路径为-logs-flume-log"><a href="#至此flume就已经配置完成了，flume运行日志的默认数据追加路径为-logs-flume-log" class="headerlink" title="至此flume就已经配置完成了，flume运行日志的默认数据追加路径为./logs/flume.log"></a>至此flume就已经配置完成了，flume运行日志的默认数据追加路径为<code>./logs/flume.log</code></h4><h4 id="若要修改flume运行日志输出路径，修改并配置flume-1-8-0-conf-log4j-properties文件即可"><a href="#若要修改flume运行日志输出路径，修改并配置flume-1-8-0-conf-log4j-properties文件即可" class="headerlink" title="若要修改flume运行日志输出路径，修改并配置flume-1.8.0/conf/log4j.properties文件即可"></a>若要修改flume运行日志输出路径，修改并配置<code>flume-1.8.0/conf/log4j.properties</code>文件即可</h4><hr><h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
    
    </summary>
    
    
      <category term="Flume" scheme="https://tomandersen-cc.github.io/categories/Flume/"/>
    
    
      <category term="Linux" scheme="https://tomandersen-cc.github.io/tags/Linux/"/>
    
      <category term="Flume" scheme="https://tomandersen-cc.github.io/tags/Flume/"/>
    
      <category term="大数据" scheme="https://tomandersen-cc.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>GitHub Page绑定至个人域名</title>
    <link href="https://tomandersen-cc.github.io/2020/03/04/Github-Page%E7%BB%91%E5%AE%9A%E8%87%B3%E4%B8%AA%E4%BA%BA%E5%9F%9F%E5%90%8D/"/>
    <id>https://tomandersen-cc.github.io/2020/03/04/Github-Page%E7%BB%91%E5%AE%9A%E8%87%B3%E4%B8%AA%E4%BA%BA%E5%9F%9F%E5%90%8D/</id>
    <published>2020-03-04T15:15:03.000Z</published>
    <updated>2020-03-08T13:51:13.280Z</updated>
    
    <content type="html"><![CDATA[<hr><a id="more"></a><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul><li><strong>此教程主要用于将个人域名映射至Github Page，同时实现Github Page在个人域名的发布</strong></li><li><strong>本次所使用的个人域名是在阿里云上申请注册所得</strong></li><li><strong>注意：如果想要取消Github Page发布，删除CNAME文件即可。然后清除浏览器缓存，就又可以使用原来的Github Page网址对其进行访问</strong></li></ul><hr><h2 id="具体步骤："><a href="#具体步骤：" class="headerlink" title="具体步骤："></a>具体步骤：</h2><h3 id="1-注册个人域名"><a href="#1-注册个人域名" class="headerlink" title="1. 注册个人域名"></a>1. 注册个人域名</h3><ul><li><strong>本人是在阿里云平台申请的个人域名，首年价格是19元，注册步骤就不再赘述了</strong></li><li><strong>实名认证到注册审批本人实测大概3个小时左右完成</strong></li></ul><h3 id="2-添加域名解析"><a href="#2-添加域名解析" class="headerlink" title="2. 添加域名解析"></a>2. 添加域名解析</h3><h4 id="a）进入阿里云控制台—域名列表，然后找到需要添加映射的个人域名"><a href="#a）进入阿里云控制台—域名列表，然后找到需要添加映射的个人域名" class="headerlink" title="a）进入阿里云控制台—域名列表，然后找到需要添加映射的个人域名"></a>a）进入阿里云控制台—域名列表，然后找到需要添加映射的个人域名</h4><p><img src="https://img-blog.csdnimg.cn/20200304231152820.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="添加域名解析"></p><h4 id="b）然后点击添加域名解析记录"><a href="#b）然后点击添加域名解析记录" class="headerlink" title="b）然后点击添加域名解析记录"></a>b）然后点击添加域名解析记录</h4><p><img src="https://img-blog.csdnimg.cn/20200304231210309.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="添加域名解析"></p><h4 id="c）配置域名映射"><a href="#c）配置域名映射" class="headerlink" title="c）配置域名映射"></a>c）配置域名映射</h4><ul><li><p><strong>“记录类型”设置成<code>CNAME</code></strong></p></li><li><p><strong>“主机记录”可以设置成<code>@</code>即直接使用主域名作映射，或者<code>www</code>即使用<code>www.主域名</code>的形式进行映射，因为本人是使用此域名映射个人博客，所以设置成<code>blog</code>即使用blog作为子域名且用于映射</strong></p></li><li><p><strong>“记录值”则设置成Github Page的网址即可</strong></p></li><li><p><strong>等待几分钟后生效</strong></p></li></ul><p><img src="https://img-blog.csdnimg.cn/20200304231230144.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="添加域名解析"></p><ul><li><strong>至此就建立了个人域名至Github Page的映射，但此时还不能使建立映射的域名直接访问Github Page，因为还需要在Github上发布自己的Github Page，直接访问的话会出现Github Pages 404</strong></li></ul><p><img src="https://img-blog.csdnimg.cn/20200304231251195.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="添加域名解析"></p><h3 id="3-发布Github-Page"><a href="#3-发布Github-Page" class="headerlink" title="3. 发布Github Page"></a>3. 发布Github Page</h3><h4 id="a）进入Github-Page仓库页面，点击Setting"><a href="#a）进入Github-Page仓库页面，点击Setting" class="headerlink" title="a）进入Github Page仓库页面，点击Setting"></a>a）进入Github Page仓库页面，点击Setting</h4><p><img src="https://img-blog.csdnimg.cn/20200304231309815.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="Setting"></p><h4 id="b）在其中找到Github-Page，通过编辑Custom-domain并保存来实现Github-Page发布"><a href="#b）在其中找到Github-Page，通过编辑Custom-domain并保存来实现Github-Page发布" class="headerlink" title="b）在其中找到Github Page，通过编辑Custom domain并保存来实现Github Page发布"></a>b）在其中找到Github Page，通过编辑<code>Custom domain</code>并保存来实现Github Page发布</h4><p><img src="https://img-blog.csdnimg.cn/20200304231323491.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="发布GithubPage"></p><h4 id="同时会发现在仓库中多出了CNAME文件，后续如果想要取消发布直接删除此文件即可，同时要清除浏览器缓存，否则会一直访问之前设置的页面"><a href="#同时会发现在仓库中多出了CNAME文件，后续如果想要取消发布直接删除此文件即可，同时要清除浏览器缓存，否则会一直访问之前设置的页面" class="headerlink" title="同时会发现在仓库中多出了CNAME文件，后续如果想要取消发布直接删除此文件即可，同时要清除浏览器缓存，否则会一直访问之前设置的页面"></a>同时会发现在仓库中多出了CNAME文件，后续如果想要取消发布直接删除此文件即可，同时要清除浏览器缓存，否则会一直访问之前设置的页面</h4><p><img src="https://img-blog.csdnimg.cn/20200304231342835.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="CNAME文件"></p><h4 id="至此就实现了个人域名和映射和Github-Page的发布，通过之前设置的个人域名就能直接访问Github-Page"><a href="#至此就实现了个人域名和映射和Github-Page的发布，通过之前设置的个人域名就能直接访问Github-Page" class="headerlink" title="至此就实现了个人域名和映射和Github Page的发布，通过之前设置的个人域名就能直接访问Github Page"></a>至此就实现了个人域名和映射和Github Page的发布，通过之前设置的个人域名就能直接访问Github Page</h4><h3 id="4-在Hexo-source路径下创建CNAME文件"><a href="#4-在Hexo-source路径下创建CNAME文件" class="headerlink" title="4. 在Hexo/source路径下创建CNAME文件"></a>4. 在<code>Hexo/source</code>路径下创建CNAME文件</h3><p><strong>由于每次Deploy本地Hexo仓库时，都会清除GitHub上仓库中的CNAME文件，所以需要在Hexo根目录的source文件夹下创建CNAME文件，文件内容为个人域名，如：<code>blog.tomandersen.cn</code></strong></p><p><strong>这样每次部署时都会将CNMAE文件同时打包至远端Repository中，避免每次都要设置GitHub Page发布域名</strong></p><hr><h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
    
    </summary>
    
    
      <category term="个人博客搭建" scheme="https://tomandersen-cc.github.io/categories/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"/>
    
    
      <category term="GitHub" scheme="https://tomandersen-cc.github.io/tags/GitHub/"/>
    
      <category term="Blog" scheme="https://tomandersen-cc.github.io/tags/Blog/"/>
    
  </entry>
  
  <entry>
    <title>Shell中冒号的特殊用法</title>
    <link href="https://tomandersen-cc.github.io/2020/02/27/Shell%E4%B8%AD%E5%86%92%E5%8F%B7%E7%9A%84%E7%89%B9%E6%AE%8A%E7%94%A8%E6%B3%95/"/>
    <id>https://tomandersen-cc.github.io/2020/02/27/Shell%E4%B8%AD%E5%86%92%E5%8F%B7%E7%9A%84%E7%89%B9%E6%AE%8A%E7%94%A8%E6%B3%95/</id>
    <published>2020-02-27T01:37:46.000Z</published>
    <updated>2020-03-06T12:22:09.771Z</updated>
    
    <content type="html"><![CDATA[<hr><a id="more"></a><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul><li>本文主要是参考了鸟哥写的《鸟哥的Linux私房菜》。因为总是在各种脚本中见到<code>:-</code>的用法而只是了解其大概含义，所以翻阅了相关资料，编写此博文以作记录</li></ul><hr><h2 id="Shell中冒号在变量赋值时的各种特殊用法"><a href="#Shell中冒号在变量赋值时的各种特殊用法" class="headerlink" title="Shell中冒号在变量赋值时的各种特殊用法"></a>Shell中冒号在变量赋值时的各种特殊用法</h2><table><thead><tr><th align="center">变量设定方式</th><th align="center">str变量没有设定时</th><th align="center">str为空字符串时</th><th align="center">str已经设定为非空字符串时</th></tr></thead><tbody><tr><td align="center">var=${str-expr}</td><td align="center">var=expr</td><td align="center">var=””</td><td align="center">var=$str</td></tr><tr><td align="center">var=${str:-expr}</td><td align="center">var=expr</td><td align="center">var=expr</td><td align="center">var=$str</td></tr><tr><td align="center">var=${str+expr}</td><td align="center">var=””</td><td align="center">var=expr</td><td align="center">var=expr</td></tr><tr><td align="center">var=${str:+expr}</td><td align="center">var=””</td><td align="center">var=””</td><td align="center">var=expr</td></tr><tr><td align="center">var=${str=expr}</td><td align="center">str=expr<br/>var=expr</td><td align="center">str 不变<br/>var=””</td><td align="center">str 不变<br/>var=$str</td></tr><tr><td align="center">var=${str:=expr}</td><td align="center">str=expr<br/>var=expr</td><td align="center">str=expr<br/>var=expr</td><td align="center">str 不变<br/>var=$str</td></tr><tr><td align="center">var=${str?expr}</td><td align="center">expr 输出至 stderr</td><td align="center">var=””</td><td align="center">var=$str</td></tr><tr><td align="center">var=${str:?expr}</td><td align="center">expr 输出至 stderr</td><td align="center">expr 输出至 stderr</td><td align="center">var=$str</td></tr></tbody></table><hr><h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
    
    </summary>
    
    
      <category term="Shell" scheme="https://tomandersen-cc.github.io/categories/Shell/"/>
    
    
      <category term="CentOS7" scheme="https://tomandersen-cc.github.io/tags/CentOS7/"/>
    
      <category term="Linux" scheme="https://tomandersen-cc.github.io/tags/Linux/"/>
    
      <category term="Shell" scheme="https://tomandersen-cc.github.io/tags/Shell/"/>
    
  </entry>
  
  <entry>
    <title>Shell中逻辑与 &amp;&amp; 与逻辑或 || 的使用</title>
    <link href="https://tomandersen-cc.github.io/2020/02/26/Shell%E4%B8%AD%E9%80%BB%E8%BE%91%E4%B8%8E-%E4%B8%8E%E9%80%BB%E8%BE%91%E6%88%96-%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
    <id>https://tomandersen-cc.github.io/2020/02/26/Shell%E4%B8%AD%E9%80%BB%E8%BE%91%E4%B8%8E-%E4%B8%8E%E9%80%BB%E8%BE%91%E6%88%96-%E7%9A%84%E4%BD%BF%E7%94%A8/</id>
    <published>2020-02-26T14:54:32.000Z</published>
    <updated>2020-03-06T12:22:06.748Z</updated>
    
    <content type="html"><![CDATA[<hr><a id="more"></a><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul><li>本文主要是参考了鸟哥写的《鸟哥的Linux私房菜》。由于看到别人脚本中有相关运用，因此翻阅了相关资料，编写此博文以作记录</li></ul><hr><h2 id="Shell中-amp-amp-与-的运行规则"><a href="#Shell中-amp-amp-与-的运行规则" class="headerlink" title="Shell中&amp;&amp;与||的运行规则"></a>Shell中&amp;&amp;与||的运行规则</h2><table><thead><tr><th align="center">指令</th><th>执行说明</th></tr></thead><tbody><tr><td align="center"><strong>command1 &amp;&amp; command2</strong></td><td>若command1正确执行，即退出码为0（$?==0），则执行command2，整体退出码以command2执行结果为准；若command1执行错误，即退出码非0（$?!=0），则不执行command2，整体退出码为0</td></tr><tr><td align="center"><strong>command1 || command2</strong></td><td>若command1正确执行，即退出码为0（$?==0），则不执行command2，整体退出码为0；若command1执行错误，即退出码非0（$?!=0），则执行command2，整体退出码以command2执行结果为准</td></tr></tbody></table><hr><h2 id="推广"><a href="#推广" class="headerlink" title="推广"></a>推广</h2><table><thead><tr><th>指令</th><th>执行说明</th></tr></thead><tbody><tr><td><strong>command1 &amp;&amp; command2 || command3</strong></td><td>等价于(command1 &amp;&amp; command2 )|| command3，前面括号中的命令为一个整体，具体执行规则参考上表</td></tr><tr><td><strong>command1 || command2 &amp;&amp; command3</strong></td><td>等价于(command1 || command2) &amp;&amp; command3，前面括号中的命令为一个整体，具体执行规则参考上表</td></tr></tbody></table><hr><h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
    
    </summary>
    
    
      <category term="Shell" scheme="https://tomandersen-cc.github.io/categories/Shell/"/>
    
    
      <category term="CentOS7" scheme="https://tomandersen-cc.github.io/tags/CentOS7/"/>
    
      <category term="Linux" scheme="https://tomandersen-cc.github.io/tags/Linux/"/>
    
      <category term="Shell" scheme="https://tomandersen-cc.github.io/tags/Shell/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop之配置历史服务器并开启日志聚集</title>
    <link href="https://tomandersen-cc.github.io/2020/02/26/Hadoop%E4%B9%8B%E9%85%8D%E7%BD%AE%E5%8E%86%E5%8F%B2%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%B9%B6%E5%BC%80%E5%90%AF%E6%97%A5%E5%BF%97%E8%81%9A%E9%9B%86/"/>
    <id>https://tomandersen-cc.github.io/2020/02/26/Hadoop%E4%B9%8B%E9%85%8D%E7%BD%AE%E5%8E%86%E5%8F%B2%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%B9%B6%E5%BC%80%E5%90%AF%E6%97%A5%E5%BF%97%E8%81%9A%E9%9B%86/</id>
    <published>2020-02-26T14:24:56.000Z</published>
    <updated>2020-03-06T12:21:25.864Z</updated>
    
    <content type="html"><![CDATA[<hr><a id="more"></a><hr><h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h2><p><strong>Hadoop中设置任务历史服务器并开启日志聚集的必要性</strong>：一般情况下在YARN的Web UI中只能查看本次YARN运行期间执行的Application的首个Container的运行日志，即ApplicationMaster的运行日志（MR任务一般是分成多个task通过多个Container分别执行，这些Container分布在集群的任意主机上，首个Container用于运行ApplicationMaster，而AppMaster则负责分发调度task，以及启动其他Container），既不能查看历史job的日志，也不能查看程序输出的Counter统计信息（即命令行提交jar程序的最终执行输出，如job执行过程中的读写数据量、Map/Reduce task数量、各个阶段执行时间等一系列参数），所以需要开启jobhistory-server以及yarn-log-aggregation，即开启任务历史服务器以及日志聚集功能，来实现历史任务日志和作业运行日志，开启jobhistory-server之后，任务结束后运行日志会聚集到HDFS指定目录中，而不再保存本地文件系统中。</p><hr><h2 id="2-集群规划"><a href="#2-集群规划" class="headerlink" title="2. 集群规划"></a>2. 集群规划</h2><table><thead><tr><th align="center"></th><th align="center">hadoop101</th><th align="center">hadoop102</th><th align="center">hadoop103</th></tr></thead><tbody><tr><td align="center"><strong>NameNode</strong></td><td align="center"><strong>√</strong></td><td align="center"></td><td align="center"></td></tr><tr><td align="center"><strong>DataNode</strong></td><td align="center"><strong>√</strong></td><td align="center"><strong>√</strong></td><td align="center"><strong>√</strong></td></tr><tr><td align="center"><strong>SecondaryNameNode</strong></td><td align="center"></td><td align="center"></td><td align="center"><strong>√</strong></td></tr><tr><td align="center"><strong>ResourceManager</strong></td><td align="center"></td><td align="center"><strong>√</strong></td><td align="center"></td></tr><tr><td align="center"><strong>NodeManager</strong></td><td align="center"><strong>√</strong></td><td align="center"><strong>√</strong></td><td align="center"><strong>√</strong></td></tr><tr><td align="center"><strong>JobHistoryServer</strong></td><td align="center"></td><td align="center"></td><td align="center"><strong>√</strong></td></tr></tbody></table><hr><h2 id="3-具体配置"><a href="#3-具体配置" class="headerlink" title="3. 具体配置"></a>3. 具体配置</h2><h3 id="a-core-site-xml"><a href="#a-core-site-xml" class="headerlink" title="a) core-site.xml"></a>a) core-site.xml</h3><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定HDFS文件系统访问地址,将其设置为NameNode的地址--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;hdfs:/</span><span class="regexp">/hadoop101:9000&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--指定Hadoop运行时产生文件的存储目录--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;property&gt;</span></span><br><span class="line"><span class="regexp">        &lt;name&gt;hadoop.tmp.dir&lt;/</span>name&gt;</span><br><span class="line">        &lt;value&gt;<span class="regexp">/opt/m</span>odule/hadoop<span class="number">-2.7</span><span class="number">.7</span>/tmp&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">&lt;<span class="regexp">/configuration&gt;</span></span><br></pre></td></tr></table></figure><h3 id="b-hdfs-site-xml"><a href="#b-hdfs-site-xml" class="headerlink" title="b) hdfs-site.xml"></a>b) hdfs-site.xml</h3><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定HDFS副本因子数--&gt;</span><br><span class="line">    &lt;!--由于实验主机磁盘空间不足,本次实验中设置为<span class="number">1</span>,一般需要设置为<span class="number">3</span>--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;1&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--以下是NameNode配置--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;!--指定NameNode节点的Web UI地址--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;property&gt;</span></span><br><span class="line"><span class="regexp">        &lt;name&gt;dfs.namenode.http-address&lt;/</span>name&gt;</span><br><span class="line">        &lt;value&gt;hadoop101:<span class="number">50070</span>&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定NameNode节点上存储name table(fsimage)文件的本地路径--&gt;</span><br><span class="line">    &lt;!--默认值:file:<span class="comment">//$&#123;hadoop.tmp.dir&#125;/dfs/name--&gt;</span></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.name.dir&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;file:/</span><span class="regexp">/$&#123;hadoop.tmp.dir&#125;/</span>dfs/namenode/fsimage&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定NameNode节点上存储transaction(edits)文件的本地路径--&gt;</span><br><span class="line">    &lt;!--默认值:$&#123;dfs.namenode.name.dir&#125;--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.edits.dir&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;file:/</span><span class="regexp">/$&#123;hadoop.tmp.dir&#125;/</span>dfs/namenode/edits&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定DataNode节点上存储Blocks文件的本地路径,此处为修改--&gt;</span><br><span class="line">    &lt;!--默认值:file:<span class="comment">//$&#123;hadoop.tmp.dir&#125;/dfs/data--&gt;</span></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.data.dir&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;file:/</span><span class="regexp">/$&#123;hadoop.tmp.dir&#125;/</span>dfs/datanode/data&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--以下是SecondaryNameNode配置--&gt;</span><br><span class="line">    &lt;!--指定NameNode辅助名称节点SecondaryNameNode的Web UI地址--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.secondary.http-address&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;hadoop103:50090&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--指定SecondaryNameNode节点上存储temporary images文件的本地路径--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;!--默认值:file:/</span><span class="regexp">/$&#123;hadoop.tmp.dir&#125;/</span>dfs/namesecondary--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.checkpoint.dir&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;file:/</span><span class="regexp">/$&#123;hadoop.tmp.dir&#125;/</span>dfs/namesecondary/fsimage&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定SecondaryNameNode节点上存储temporary edits文件的本地路径--&gt;</span><br><span class="line">    &lt;!--默认值:$&#123;dfs.namenode.checkpoint.dir&#125;--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.checkpoint.edits.dir&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;file:/</span><span class="regexp">/$&#123;hadoop.tmp.dir&#125;/</span>dfs/namesecondary/edits&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">&lt;<span class="regexp">/configuration&gt;</span></span><br></pre></td></tr></table></figure><h3 id="c-mapred-site-xml"><a href="#c-mapred-site-xml" class="headerlink" title="c) mapred-site.xml"></a>c) mapred-site.xml</h3><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定MR程序运行框架,设置为YARN上运行,默认是在本地运行--&gt;</span><br><span class="line">    &lt;!--默认值:local--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;yarn&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--指定历史服务器JobHistoryServer进程间通信IPC地址--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;property&gt;</span></span><br><span class="line"><span class="regexp">        &lt;name&gt;mapreduce.jobhistory.address&lt;/</span>name&gt;</span><br><span class="line">        &lt;value&gt;hadoop103:<span class="number">10020</span>&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定历史服务器JobHistoryServer的Web UI地址--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;hadoop103:19888&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp">    </span></span><br><span class="line"><span class="regexp">&lt;/</span>configuration&gt;</span><br></pre></td></tr></table></figure><h3 id="d-yarn-site-xml"><a href="#d-yarn-site-xml" class="headerlink" title="d) yarn-site.xml"></a>d) yarn-site.xml</h3><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--Site specific YARN configuration properties--&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--设置Reducer获取数据的方式--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;mapreduce_shuffle&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--指定YARN中ResourceManager的ip地址--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;property&gt;</span></span><br><span class="line"><span class="regexp">        &lt;name&gt;yarn.resourcemanager.hostname&lt;/</span>name&gt;</span><br><span class="line">        &lt;value&gt;hadoop102&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--此参数指的是nodemanager的可用内存大小,单位为Mb,设置为主机内存大小--&gt;</span><br><span class="line">    &lt;!--本次实验主机内存大小为<span class="number">2</span>GB,此参数根据各机器分配的物理内存大小设置,若大于物理内存值会影响程序运行效率--&gt;</span><br><span class="line">    &lt;!--默认值:<span class="number">8192</span>--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;2048&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--开启日志聚集功能--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;property&gt;</span></span><br><span class="line"><span class="regexp">        &lt;name&gt;yarn.log-aggregation-enable&lt;/</span>name&gt;</span><br><span class="line">        &lt;value&gt;<span class="literal">true</span>&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--日志聚集位置,默认为HDFS文件系统的/tmp/logs路径下,默认格式为/tmp/logs/$&#123;user&#125;/logs--&gt;</span><br><span class="line">    &lt;!--默认值:<span class="regexp">/tmp/</span>logs--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.remote-app-log-dir&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;/</span>tmp/logs&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--聚集日志保留时间设置<span class="number">7</span>天--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;604800&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">&lt;/</span>configuration&gt;</span><br></pre></td></tr></table></figure><hr><h2 id="4-启动HDFS和YARN集群"><a href="#4-启动HDFS和YARN集群" class="headerlink" title="4. 启动HDFS和YARN集群"></a>4. 启动HDFS和YARN集群</h2><h3 id="在hadoop101上启动HDFS集群："><a href="#在hadoop101上启动HDFS集群：" class="headerlink" title="在hadoop101上启动HDFS集群："></a>在hadoop101上启动HDFS集群：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sbin/start-dfs.sh</span><br></pre></td></tr></table></figure><h3 id="在hadoop102上启动YARN集群："><a href="#在hadoop102上启动YARN集群：" class="headerlink" title="在hadoop102上启动YARN集群："></a>在hadoop102上启动YARN集群：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sbin/start-yarn.sh</span><br></pre></td></tr></table></figure><h3 id="在hadoop103上启动JobHistoryServer："><a href="#在hadoop103上启动JobHistoryServer：" class="headerlink" title="在hadoop103上启动JobHistoryServer："></a>在hadoop103上启动JobHistoryServer：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sbin/mr-jobhistory-daemon.sh start historyserver</span><br></pre></td></tr></table></figure><h3 id="启动脚本："><a href="#启动脚本：" class="headerlink" title="启动脚本："></a>启动脚本：</h3><ul><li>最好是自己自定义启动脚本，这里提供个参考脚本<code>hadoop-ctl.sh</code>，操作参数为<code>start</code>和<code>stop</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment"># 用于启动Hadoop集群,包括HDFS/YARN/JobHistoryServer</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 判断参数个数</span></span><br><span class="line"><span class="keyword">if</span> ((<span class="variable">$#</span> != 1)); <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> -e <span class="string">"\nWorng Parameter!"</span></span><br><span class="line">    <span class="built_in">exit</span> 1</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取当前时间(相对时间)</span></span><br><span class="line">start_time=$(date +%s)</span><br><span class="line"><span class="comment"># 获取操作方式</span></span><br><span class="line">operate=<span class="variable">$1</span></span><br><span class="line"><span class="comment"># 设定HDFS客户端,即NameNode节点地址</span></span><br><span class="line">HDFS_Client=<span class="string">"hadoop101"</span></span><br><span class="line"><span class="comment"># 设定YARN客户端,即ResourceManager节点地址</span></span><br><span class="line">YARN_Client=<span class="string">"hadoop102"</span></span><br><span class="line"><span class="comment"># 设置任务历史服务器客户端,即JobHistoryServer节点地址</span></span><br><span class="line">JobHistoryServer=<span class="string">"hadoop103"</span></span><br><span class="line"><span class="comment"># 指定启动用户</span></span><br><span class="line">user=<span class="string">"tomandersen"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$operate</span> <span class="keyword">in</span></span><br><span class="line">start)</span><br><span class="line">    <span class="comment"># 启动HDFS集群</span></span><br><span class="line">    <span class="built_in">echo</span> -e <span class="string">"\n----------Starting HDFS cluster----------"</span></span><br><span class="line">    ssh <span class="variable">$user</span>@<span class="variable">$HDFS_Client</span> <span class="string">"source /etc/profile;</span></span><br><span class="line"><span class="string">    HADOOP_HOME=<span class="variable">$&#123;HADOOP_HOME:-/opt/module/hadoop-2.7.7&#125;</span>;</span></span><br><span class="line"><span class="string">    <span class="variable">$HADOOP_HOME</span>/sbin/start-dfs.sh"</span></span><br><span class="line">    <span class="comment"># 启动YARN集群</span></span><br><span class="line">    <span class="built_in">echo</span> -e <span class="string">"\n----------Starting YARN cluster----------"</span></span><br><span class="line">    ssh <span class="variable">$user</span>@<span class="variable">$YARN_Client</span> <span class="string">"source /etc/profile;</span></span><br><span class="line"><span class="string">    HADOOP_HOME=<span class="variable">$&#123;HADOOP_HOME:-/opt/module/hadoop-2.7.7&#125;</span>;</span></span><br><span class="line"><span class="string">    <span class="variable">$HADOOP_HOME</span>/sbin/start-yarn.sh"</span></span><br><span class="line">    <span class="comment"># 启动历史服务器</span></span><br><span class="line">    <span class="built_in">echo</span> -e <span class="string">"\n----------Starting JobHistoryServer----------"</span></span><br><span class="line">    ssh <span class="variable">$user</span>@<span class="variable">$JobHistoryServer</span> <span class="string">"source /etc/profile;</span></span><br><span class="line"><span class="string">    HADOOP_HOME=<span class="variable">$&#123;HADOOP_HOME:-/opt/module/hadoop-2.7.7&#125;</span>;</span></span><br><span class="line"><span class="string">    <span class="variable">$HADOOP_HOME</span>/sbin/mr-jobhistory-daemon.sh start historyserver"</span></span><br><span class="line">    ;;</span><br><span class="line">stop)</span><br><span class="line">    <span class="comment"># 关闭HDFS集群</span></span><br><span class="line">    <span class="built_in">echo</span> -e <span class="string">"\n----------Stopping HDFS cluster----------"</span></span><br><span class="line">    ssh <span class="variable">$user</span>@<span class="variable">$HDFS_Client</span> <span class="string">"source /etc/profile;</span></span><br><span class="line"><span class="string">    HADOOP_HOME=<span class="variable">$&#123;HADOOP_HOME:-/opt/module/hadoop-2.7.7&#125;</span>;</span></span><br><span class="line"><span class="string">    <span class="variable">$HADOOP_HOME</span>/sbin/stop-dfs.sh"</span></span><br><span class="line">    <span class="comment"># 关闭YARN集群</span></span><br><span class="line">    <span class="built_in">echo</span> -e <span class="string">"\n----------Stopping YARN cluster----------"</span></span><br><span class="line">    ssh <span class="variable">$user</span>@<span class="variable">$YARN_Client</span> <span class="string">"source /etc/profile;</span></span><br><span class="line"><span class="string">    HADOOP_HOME=<span class="variable">$&#123;HADOOP_HOME:-/opt/module/hadoop-2.7.7&#125;</span>;</span></span><br><span class="line"><span class="string">    <span class="variable">$HADOOP_HOME</span>/sbin/stop-yarn.sh"</span></span><br><span class="line">    <span class="comment"># 关闭历史服务器</span></span><br><span class="line">    <span class="built_in">echo</span> -e <span class="string">"\n----------Stopping JobHistoryServer----------"</span></span><br><span class="line">    ssh <span class="variable">$user</span>@<span class="variable">$JobHistoryServer</span> <span class="string">"source /etc/profile;</span></span><br><span class="line"><span class="string">    HADOOP_HOME=<span class="variable">$&#123;HADOOP_HOME:-/opt/module/hadoop-2.7.7&#125;</span>;</span></span><br><span class="line"><span class="string">    <span class="variable">$HADOOP_HOME</span>/sbin/mr-jobhistory-daemon.sh stop historyserver"</span></span><br><span class="line">    ;;</span><br><span class="line">*)</span><br><span class="line">    <span class="built_in">echo</span> -e <span class="string">"\nWorng Parameter!"</span></span><br><span class="line">    <span class="built_in">exit</span> 1</span><br><span class="line">    ;;</span><br><span class="line"><span class="keyword">esac</span></span><br><span class="line"></span><br><span class="line">end_time=$(date +%s)</span><br><span class="line">execution_time=$((<span class="variable">$&#123;end_time&#125;</span> - <span class="variable">$&#123;start_time&#125;</span>))</span><br><span class="line"><span class="built_in">echo</span> -e <span class="string">"\n----------<span class="variable">$operate</span> Hadoop cluster takes <span class="variable">$&#123;execution_time&#125;</span> seconds----------\n"</span></span><br></pre></td></tr></table></figure><hr><h2 id="5-jps检查进程"><a href="#5-jps检查进程" class="headerlink" title="5. jps检查进程"></a>5. jps检查进程</h2><ul><li>本次配置的hadoop集群，启动后各主机进程情况如下所示：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[tomandersen@hadoop101 hadoop]$ call-cluster.sh jps</span><br><span class="line">----------hadoop103----------</span><br><span class="line">38738 SecondaryNameNode</span><br><span class="line">39077 Jps</span><br><span class="line">38825 NodeManager</span><br><span class="line">38622 DataNode</span><br><span class="line">38958 JobHistoryServer</span><br><span class="line">----------hadoop102----------</span><br><span class="line">44720 ResourceManager</span><br><span class="line">45175 Jps</span><br><span class="line">44570 DataNode</span><br><span class="line">44862 NodeManager</span><br><span class="line">----------hadoop101----------</span><br><span class="line">48291 NameNode</span><br><span class="line">48438 DataNode</span><br><span class="line">48694 NodeManager</span><br><span class="line">48856 Jps</span><br><span class="line"></span><br><span class="line">----------execute <span class="string">"jps"</span> <span class="keyword">in</span> cluster takes 3 seconds----------</span><br><span class="line"></span><br><span class="line">[tomandersen@hadoop101 hadoop]$ </span><br><span class="line">[tomandersen@hadoop101 hadoop]$</span><br></pre></td></tr></table></figure><h2 id="6-通过JobHistoryServer查看任务运行情况"><a href="#6-通过JobHistoryServer查看任务运行情况" class="headerlink" title="6. 通过JobHistoryServer查看任务运行情况"></a>6. 通过JobHistoryServer查看任务运行情况</h2><h3 id="a-查看JobHistoryServer的Web-UI："><a href="#a-查看JobHistoryServer的Web-UI：" class="headerlink" title="a) 查看JobHistoryServer的Web UI："></a>a) 查看JobHistoryServer的Web UI：</h3><p><img src="https://img-blog.csdnimg.cn/20200226222121569.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="查看JobHistoryServer的Web UI"></p><h3 id="b-查看Job运行情况："><a href="#b-查看Job运行情况：" class="headerlink" title="b) 查看Job运行情况："></a>b) 查看Job运行情况：</h3><p><img src="https://img-blog.csdnimg.cn/20200226222143202.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="查看Job运行情况"></p><p><img src="https://img-blog.csdnimg.cn/20200226222210552.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="查看Job运行情况"></p><h3 id="c-查看Job的运行日志"><a href="#c-查看Job的运行日志" class="headerlink" title="c) 查看Job的运行日志"></a>c) 查看Job的运行日志</h3><p><img src="https://img-blog.csdnimg.cn/20200226222232809.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="查看Job的运行日志"></p><p><img src="https://img-blog.csdnimg.cn/2020022622225166.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="查看Job的运行日志"></p><h3 id="d-查看Job的Counter统计信息"><a href="#d-查看Job的Counter统计信息" class="headerlink" title="d) 查看Job的Counter统计信息"></a>d) 查看Job的Counter统计信息</h3><p><img src="https://img-blog.csdnimg.cn/20200226222335229.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="查看Job的Counter统计信息"></p><p><img src="https://img-blog.csdnimg.cn/20200226222349289.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="查看Job的Counter统计信息"></p><hr><h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
    
    </summary>
    
    
      <category term="Hadoop" scheme="https://tomandersen-cc.github.io/categories/Hadoop/"/>
    
    
      <category term="CentOS7" scheme="https://tomandersen-cc.github.io/tags/CentOS7/"/>
    
      <category term="大数据" scheme="https://tomandersen-cc.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Hadoop" scheme="https://tomandersen-cc.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop中HA模式配置（HDFS HA &amp; YARN HA）</title>
    <link href="https://tomandersen-cc.github.io/2020/02/26/Hadoop%E4%B8%ADHA%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE%EF%BC%88HDFS-HA-YARN-HA%EF%BC%89/"/>
    <id>https://tomandersen-cc.github.io/2020/02/26/Hadoop%E4%B8%ADHA%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE%EF%BC%88HDFS-HA-YARN-HA%EF%BC%89/</id>
    <published>2020-02-26T12:58:17.000Z</published>
    <updated>2020-03-08T13:52:16.877Z</updated>
    
    <content type="html"><![CDATA[<hr><a id="more"></a><hr><h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1.前言"></a>1.前言</h2><ul><li>操作系统：CentOS 7</li><li>Hadoop版本：2.7.7</li><li>Zookeeper版本：3.4.14</li><li>Java版本：1.8.0_221</li><li>Hadoop HA模式分为HDFS HA（NameNode HA）和YARN HA（ResourceManager HA）两个部分</li><li>在本次配置中同时配置了HDFS HA和YARN HA下的自动故障转移Automatic Failover，以及历史服务器JobHistoryServer</li><li>所有配置均在同一用户下操作</li></ul><hr><h2 id="2-集群规划"><a href="#2-集群规划" class="headerlink" title="2.集群规划"></a>2.集群规划</h2><table><thead><tr><th align="left"></th><th align="center">hadoop101</th><th align="center">hadoop102</th><th align="center">hadoop103</th></tr></thead><tbody><tr><td align="left"><strong>NameNode</strong></td><td align="center"><strong>√</strong></td><td align="center"><strong>√</strong></td><td align="center"></td></tr><tr><td align="left"><strong>DataNode</strong></td><td align="center"><strong>√</strong></td><td align="center"><strong>√</strong></td><td align="center"><strong>√</strong></td></tr><tr><td align="left"><strong>ResourceManager</strong></td><td align="center"><strong>√</strong></td><td align="center"><strong>√</strong></td><td align="center"></td></tr><tr><td align="left"><strong>NodeManager</strong></td><td align="center"><strong>√</strong></td><td align="center"><strong>√</strong></td><td align="center"><strong>√</strong></td></tr><tr><td align="left"><strong>JobHistoryServer</strong></td><td align="center"></td><td align="center"></td><td align="center"><strong>√</strong></td></tr><tr><td align="left"><strong>JournalNode</strong></td><td align="center"><strong>√</strong></td><td align="center"><strong>√</strong></td><td align="center"><strong>√</strong></td></tr><tr><td align="left"><strong>DFSZKFailoverController</strong></td><td align="center"><strong>√</strong></td><td align="center"><strong>√</strong></td><td align="center"></td></tr><tr><td align="left"><strong>Zookeeper</strong></td><td align="center"><strong>√</strong></td><td align="center"><strong>√</strong></td><td align="center"><strong>√</strong></td></tr></tbody></table><hr><h2 id="3-具体配置"><a href="#3-具体配置" class="headerlink" title="3.具体配置"></a>3.具体配置</h2><h3 id="a-core-site-xml"><a href="#a-core-site-xml" class="headerlink" title="a) core-site.xml"></a>a) core-site.xml</h3><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定HDFS文件系统访问地址,将其设置为NameNode的地址--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;hdfs:/</span><span class="regexp">/dfsHAcluster&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--指定Hadoop运行时产生文件的存储目录--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;property&gt;</span></span><br><span class="line"><span class="regexp">        &lt;name&gt;hadoop.tmp.dir&lt;/</span>name&gt;</span><br><span class="line">        &lt;value&gt;<span class="regexp">/opt/m</span>odule/HA/hadoop<span class="number">-2.7</span><span class="number">.7</span>/tmp&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定JournalNode在本地存储edits文件的绝对路径--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.journalnode.edits.dir&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;$&#123;hadoop.tmp.dir&#125;/</span>dfs/journalnode/localdata&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--为HDFS(NameNode) HA模式实现自动故障转移,设置Zookeeper服务器--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;ha.zookeeper.quorum&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;zkServer1:2181,zkServer2:2181,zkServer3:2181&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;    </span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">&lt;/</span>configuration&gt;</span><br></pre></td></tr></table></figure><h3 id="b-hdfs-site-xml"><a href="#b-hdfs-site-xml" class="headerlink" title="b) hdfs-site.xml"></a>b) hdfs-site.xml</h3><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定HDFS副本因子数--&gt;</span><br><span class="line">    &lt;!--由于实验主机磁盘空间不足,本次实验中设置为<span class="number">1</span>,一般需要设置为<span class="number">3</span>--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;1&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--以下是关于HDFS(NameNode) HA模式的配置--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;!--设置集群的nameservice name--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;property&gt;</span></span><br><span class="line"><span class="regexp">        &lt;name&gt;dfs.nameservices&lt;/</span>name&gt;</span><br><span class="line">        &lt;value&gt;dfsHAcluster&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--设置HDFS HA Cluster的节点--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.ha.namenodes.dfsHAcluster&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;nn1,nn2&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--关于HDFS HA集群nn1节点的配置--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;!--设置HDFS HA集群nn1节点的RPC地址--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;property&gt;</span></span><br><span class="line"><span class="regexp">        &lt;name&gt;dfs.namenode.rpc-address.dfsHAcluster.nn1&lt;/</span>name&gt;</span><br><span class="line">        &lt;value&gt;hadoop101:<span class="number">8020</span>&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--设置HDFS HA集群nn1节点的Web UI地址--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.http-address.dfsHAcluster.nn1&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;hadoop101:50070&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">        &lt;!--指定HDFS HA集群dfsHAcluster中nn1节点上存储name table(fsimage)文件的本地路径--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;property&gt;</span></span><br><span class="line"><span class="regexp">        &lt;name&gt;dfs.namenode.name.dir.dfsHAcluster.nn1&lt;/</span>name&gt;</span><br><span class="line">        &lt;value&gt;$&#123;hadoop.tmp.dir&#125;/dfs/dfsHAcluster/nn1/fsimage&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定HDFS HA集群dfsHAcluster中nn1节点上存储transaction(edits)文件的本地路径--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.edits.dir.dfsHAcluster.nn1&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;$&#123;hadoop.tmp.dir&#125;/</span>dfs/dfsHAcluster/nn1/edits&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定HDFS HA集群dfsHAcluster中nn2节点上存储Blocks文件的本地路径--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.data.dir.dfsHAcluster.nn1&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;$&#123;hadoop.tmp.dir&#125;/</span>dfs/dfsHAcluster/nn1/data&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--关于HDFS HA集群nn2节点的配置--&gt;</span><br><span class="line">    &lt;!--设置HDFS HA集群nn2节点的RPC地址--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.rpc-address.dfsHAcluster.nn2&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;hadoop102:8020&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--设置HDFS HA集群nn2节点的Web UI地址--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;property&gt;</span></span><br><span class="line"><span class="regexp">        &lt;name&gt;dfs.namenode.http-address.dfsHAcluster.nn2&lt;/</span>name&gt;</span><br><span class="line">        &lt;value&gt;hadoop102:<span class="number">50070</span>&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定HDFS HA集群dfsHAcluster中nn2节点上存储name table(fsimage)文件的本地路径--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.name.dir.dfsHAcluster.nn2&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;$&#123;hadoop.tmp.dir&#125;/</span>dfs/dfsHAcluster/nn2/fsimage&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定HDFS HA集群dfsHAcluster中nn2节点上存储transaction(edits)文件的本地路径--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.edits.dir.dfsHAcluster.nn2&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;$&#123;hadoop.tmp.dir&#125;/</span>dfs/dfsHAcluster/nn2/edits&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定HDFS HA集群dfsHAcluster中nn2节点上存储Blocks文件的本地路径--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.data.dir.dfsHAcluster.nn2&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;$&#123;hadoop.tmp.dir&#125;/</span>dfs/dfsHAcluster/nn2/data&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &lt;!--设置Active NameNode向StandBy NameNode共享edits文件的URI--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.shared.edits.dir&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;qjournal:/</span><span class="regexp">/zkServer1:8485;zkServer2:8485;zkServer3:8485/</span>dfsHAcluster&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--设置HDFS用于联络Active NameNode的Java <span class="class"><span class="keyword">class</span>--&gt;</span></span><br><span class="line"><span class="class">    &lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="class">        &lt;<span class="title">name</span>&gt;<span class="title">dfs</span>.<span class="title">client</span>.<span class="title">failover</span>.<span class="title">proxy</span>.<span class="title">provider</span>.<span class="title">dfsHAcluster</span>&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="class">        &lt;<span class="title">value</span>&gt;<span class="title">org</span>.<span class="title">apache</span>.<span class="title">hadoop</span>.<span class="title">hdfs</span>.<span class="title">server</span>.<span class="title">namenode</span>.<span class="title">ha</span>.<span class="title">ConfiguredFailoverProxyProvider</span>&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="class">    &lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    &lt;!--设置当进行故障转移<span class="title">failover</span>时通过何种方式隔离<span class="title">Active</span> <span class="title">NameNode</span>--&gt;</span></span><br><span class="line"><span class="class">    &lt;!--本次设置成使用<span class="title">ssh</span>隔离--&gt;</span></span><br><span class="line"><span class="class">    &lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="class">        &lt;<span class="title">name</span>&gt;<span class="title">dfs</span>.<span class="title">ha</span>.<span class="title">fencing</span>.<span class="title">methods</span>&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="class">        &lt;<span class="title">value</span>&gt;<span class="title">sshfence</span>&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="class">    &lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    &lt;!--设置<span class="title">ssh</span>隔离就必须设置成当前用户<span class="title">ssh</span>对其他<span class="title">NameNode</span>免密登录,</span></span><br><span class="line"><span class="class">    同时需要在此提供私钥路径--&gt;</span></span><br><span class="line"><span class="class">    &lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="class">        &lt;<span class="title">name</span>&gt;<span class="title">dfs</span>.<span class="title">ha</span>.<span class="title">fencing</span>.<span class="title">ssh</span>.<span class="title">private</span>-<span class="title">key</span>-<span class="title">files</span>&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="class">        &lt;<span class="title">value</span>&gt;/<span class="title">home</span>/<span class="title">TomAndersen</span>/.<span class="title">ssh</span>/<span class="title">id_rsa</span>&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="class">    &lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    &lt;!--设置<span class="title">ssh</span>连接超时时间--&gt;</span></span><br><span class="line"><span class="class">    &lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="class">        &lt;<span class="title">name</span>&gt;<span class="title">dfs</span>.<span class="title">ha</span>.<span class="title">fencing</span>.<span class="title">ssh</span>.<span class="title">connect</span>-<span class="title">timeout</span>&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="class">        &lt;<span class="title">value</span>&gt;30000&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="class">    &lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    &lt;!--以下通过<span class="title">Zookeeper</span>设置自动故障转移<span class="title">automatic</span> <span class="title">failover</span>--&gt;</span></span><br><span class="line"><span class="class">    &lt;!--设置开启故障自动转移--&gt;</span></span><br><span class="line"><span class="class">    &lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="class">        &lt;<span class="title">name</span>&gt;<span class="title">dfs</span>.<span class="title">ha</span>.<span class="title">automatic</span>-<span class="title">failover</span>.<span class="title">enabled</span>&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="class">        &lt;<span class="title">value</span>&gt;<span class="title">true</span>&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="class">    &lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">&lt;/<span class="title">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="c-mapred-site-xml"><a href="#c-mapred-site-xml" class="headerlink" title="c) mapred-site.xml"></a>c) mapred-site.xml</h3><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定MR程序运行框架,设置为YARN上运行,默认是在本地运行--&gt;</span><br><span class="line">    &lt;!--默认值:local--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;yarn&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--指定历史服务器JobHistoryServer进程间通信IPC地址--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;property&gt;</span></span><br><span class="line"><span class="regexp">        &lt;name&gt;mapreduce.jobhistory.address&lt;/</span>name&gt;</span><br><span class="line">        &lt;value&gt;hadoop103:<span class="number">10020</span>&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定历史服务器JobHistoryServer的Web UI地址--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;hadoop103:19888&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp">    </span></span><br><span class="line"><span class="regexp">&lt;/</span>configuration&gt;</span><br></pre></td></tr></table></figure><h3 id="d-yarn-site-xml"><a href="#d-yarn-site-xml" class="headerlink" title="d) yarn-site.xml"></a>d) yarn-site.xml</h3><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--Site specific YARN configuration properties--&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--设置Reducer获取数据的方式--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;mapreduce_shuffle&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--此参数指的是nodemanager的可用内存大小,单位为Mb,设置为主机内存大小--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;!--本次实验主机内存大小为2GB,此参数根据各机器分配的物理内存大小设置,若大于物理内存值会影响程序运行效率--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;!--默认值:8192--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;property&gt;</span></span><br><span class="line"><span class="regexp">        &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/</span>name&gt;</span><br><span class="line">        &lt;value&gt;<span class="number">2048</span>&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--开启日志聚集功能--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.log-aggregation-enable&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;true&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--日志聚集位置,默认为HDFS文件系统的/</span>tmp/logs路径下,默认格式为/tmp/logs/$&#123;user&#125;/logs--&gt;</span><br><span class="line">    &lt;!--默认值:<span class="regexp">/tmp/</span>logs--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.remote-app-log-dir&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;/</span>tmp/logs&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--聚集日志保留时间设置<span class="number">7</span>天--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;604800&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--以下是关于ResourceMangaer HA模式的配置--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;!--设置开启YARN HA模式--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;property&gt;</span></span><br><span class="line"><span class="regexp">        &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/</span>name&gt;</span><br><span class="line">        &lt;value&gt;<span class="literal">true</span>&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--设置ResourceManager Cluster ID即RM集群名--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.cluster-id&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;RMcluster&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--设置RM集群中的RM节点ID--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;property&gt;</span></span><br><span class="line"><span class="regexp">        &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/</span>name&gt;</span><br><span class="line">        &lt;value&gt;rm1,rm2&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--以下是关于RM集群中rm1节点的配置--&gt;</span><br><span class="line">    &lt;!--指定RM集群中rm1节点的ip地址--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;hadoop102&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--指定RM集群中rm1节点的Web UI地址--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;property&gt;</span></span><br><span class="line"><span class="regexp">        &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/</span>name&gt;</span><br><span class="line">        &lt;value&gt;hadoop102:<span class="number">8088</span>&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--以下是关于RM集群中rm2节点的配置--&gt;</span><br><span class="line">    &lt;!--指定RM集群中rm2节点的ip地址--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;hadoop101&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--指定RM集群中rm2节点的Web UI地址--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;property&gt;</span></span><br><span class="line"><span class="regexp">        &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/</span>name&gt;</span><br><span class="line">        &lt;value&gt;hadoop101:<span class="number">8088</span>&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定RM集群使用的Zookeeper集群所提供的Client端口--&gt;</span><br><span class="line">    &lt;!--注意与Zookeeper集群中设置的客户端端口一致--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.zk-address&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;zkServer1:2181,zkServer2:2181,zkServer3:2181&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &lt;!--启用RM自动恢复--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;!--需同时设置yarn.resourcemanager.store.class--&gt;</span></span><br><span class="line"><span class="regexp">    &lt;property&gt;</span></span><br><span class="line"><span class="regexp">        &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/</span>name&gt;</span><br><span class="line">        &lt;value&gt;<span class="literal">true</span>&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--指定resourcemanager的状态信息存储在zookeeper集群的工具类--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.store.class&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">        &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/</span>value&gt;</span><br><span class="line">    &lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">&lt;/</span>configuration&gt;</span><br></pre></td></tr></table></figure><hr><h2 id="4-启动HA集群"><a href="#4-启动HA集群" class="headerlink" title="4.启动HA集群"></a>4.启动HA集群</h2><h3 id="a-首次启动集群"><a href="#a-首次启动集群" class="headerlink" title="a) 首次启动集群"></a>a) 首次启动集群</h3><h4 id="注意："><a href="#注意：" class="headerlink" title="注意："></a>注意：</h4><ul><li><p><strong>首次启动一个新的Hadoop HA集群时需要对NameNodes进行格式化，若已经格式化则直接正常启动即可</strong></p></li><li><p><strong>若想重新格式化NameNodes，需要将所有NameNodes节点上hadoop中的<code>tmp</code>和<code>logs</code>文件夹都删除，同时最好也删除Zookeeper集群上的所有hadoop-ha相关节点</strong></p></li></ul><h4 id="格式化NameNodes步骤："><a href="#格式化NameNodes步骤：" class="headerlink" title="格式化NameNodes步骤："></a>格式化NameNodes步骤：</h4><h5 id="1-启动Zookeeper集群"><a href="#1-启动Zookeeper集群" class="headerlink" title="(1) 启动Zookeeper集群"></a>(1) 启动<code>Zookeeper</code>集群</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/zkServer.sh</span><br></pre></td></tr></table></figure><h5 id="2-在HDFS集群中所有节点上启动JournalNode"><a href="#2-在HDFS集群中所有节点上启动JournalNode" class="headerlink" title="(2) 在HDFS集群中所有节点上启动JournalNode"></a>(2) 在HDFS集群中所有节点上启动<code>JournalNode</code></h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sbin/hadoop-daemon.sh start journalnode</span><br></pre></td></tr></table></figure><h5 id="3-在首个NameNode节点上格式化NameNode（dfsHAcluster-nn1）"><a href="#3-在首个NameNode节点上格式化NameNode（dfsHAcluster-nn1）" class="headerlink" title="(3) 在首个NameNode节点上格式化NameNode（dfsHAcluster.nn1）"></a>(3) 在首个NameNode节点上格式化<code>NameNode</code>（dfsHAcluster.nn1）</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hdfs namenode -format</span><br></pre></td></tr></table></figure><h5 id="4-在此节点上格式化zkNode即创建zkNode节点（dfsHAcluster-nn1）"><a href="#4-在此节点上格式化zkNode即创建zkNode节点（dfsHAcluster-nn1）" class="headerlink" title="(4) 在此节点上格式化zkNode即创建zkNode节点（dfsHAcluster.nn1）"></a>(4) 在此节点上格式化zkNode即创建zkNode节点（dfsHAcluster.nn1）</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hdfs zkfc -formatZK</span><br></pre></td></tr></table></figure><h5 id="5-在此节点上启动首个NameNode进程（dfsHAcluster-nn1）"><a href="#5-在此节点上启动首个NameNode进程（dfsHAcluster-nn1）" class="headerlink" title="(5) 在此节点上启动首个NameNode进程（dfsHAcluster.nn1）"></a>(5) 在此节点上启动首个<code>NameNode</code>进程（dfsHAcluster.nn1）</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sbin/hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure><h5 id="6-同时启动DFSZKFailoverController、DataNode进程（dfsHAcluster-nn1）"><a href="#6-同时启动DFSZKFailoverController、DataNode进程（dfsHAcluster-nn1）" class="headerlink" title="(6) 同时启动DFSZKFailoverController、DataNode进程（dfsHAcluster.nn1）"></a>(6) 同时启动<code>DFSZKFailoverController、DataNode</code>进程（dfsHAcluster.nn1）</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">./sbin/hadoop-daemon.sh start zkfc</span><br><span class="line">./sbin/hadoop-daemon.sh start datanode</span><br></pre></td></tr></table></figure><h5 id="7-在其他所有NameNode节点上进行同步格式化"><a href="#7-在其他所有NameNode节点上进行同步格式化" class="headerlink" title="(7) 在其他所有NameNode节点上进行同步格式化"></a>(7) 在其他所有NameNode节点上进行同步格式化</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hdfs namenode -bootstrapStandby</span><br></pre></td></tr></table></figure><h5 id="8-同时在其他所有NameNode节点上启动NameNode、DFSZKFailoverController、DataNode进程"><a href="#8-同时在其他所有NameNode节点上启动NameNode、DFSZKFailoverController、DataNode进程" class="headerlink" title="(8) 同时在其他所有NameNode节点上启动NameNode、DFSZKFailoverController、DataNode进程"></a>(8) 同时在其他所有NameNode节点上启动<code>NameNode、DFSZKFailoverController、DataNode</code>进程</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./sbin/hadoop-daemon.sh start namenode</span><br><span class="line">./sbin/hadoop-daemon.sh start zkfc</span><br><span class="line">./sbin/hadoop-daemon.sh start datanode</span><br></pre></td></tr></table></figure><h5 id="9-启动RM集群：在YARN集群中所有RM节点上启动RM进程，第二个启动的RM为Active（可以不启动）"><a href="#9-启动RM集群：在YARN集群中所有RM节点上启动RM进程，第二个启动的RM为Active（可以不启动）" class="headerlink" title="(9) 启动RM集群：在YARN集群中所有RM节点上启动RM进程，第二个启动的RM为Active（可以不启动）"></a>(9) 启动RM集群：在YARN集群中所有RM节点上启动RM进程，第二个启动的RM为Active（可以不启动）</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sbin/yarn-daemon.sh start resourcemanager</span><br></pre></td></tr></table></figure><h5 id="10-启动NM集群：在YARN集群中所有节点上启动NM进程（可以不启动）"><a href="#10-启动NM集群：在YARN集群中所有节点上启动NM进程（可以不启动）" class="headerlink" title="(10) 启动NM集群：在YARN集群中所有节点上启动NM进程（可以不启动）"></a>(10) 启动NM集群：在YARN集群中所有节点上启动NM进程（可以不启动）</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sbin/yarn-daemon.sh start nodemanager</span><br></pre></td></tr></table></figure><h5 id="11-在历史服务器节点上，启动JobHistoryServer进程（可以不启动）"><a href="#11-在历史服务器节点上，启动JobHistoryServer进程（可以不启动）" class="headerlink" title="(11) 在历史服务器节点上，启动JobHistoryServer进程（可以不启动）"></a>(11) 在历史服务器节点上，启动<code>JobHistoryServer</code>进程（可以不启动）</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sbin/mr-jobhistory-daemon.sh</span><br></pre></td></tr></table></figure><h4 id="正常情况下此时集群已经正常运行，各节点具体进程情况如下所示："><a href="#正常情况下此时集群已经正常运行，各节点具体进程情况如下所示：" class="headerlink" title="正常情况下此时集群已经正常运行，各节点具体进程情况如下所示："></a>正常情况下此时集群已经正常运行，各节点具体进程情况如下所示：</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">[tomandersen@hadoop101 hadoop-2.7.7]$ call-cluster.sh jps</span><br><span class="line">----------hadoop103----------</span><br><span class="line">32688 Jps</span><br><span class="line">23553 QuorumPeerMain</span><br><span class="line">32418 JobHistoryServer</span><br><span class="line">32291 JournalNode</span><br><span class="line">32520 NodeManager</span><br><span class="line">32186 DataNode</span><br><span class="line">----------hadoop102----------</span><br><span class="line">36963 JournalNode</span><br><span class="line">37669 Jps</span><br><span class="line">36775 NameNode</span><br><span class="line">37209 ResourceManager</span><br><span class="line">36858 DataNode</span><br><span class="line">25612 QuorumPeerMain</span><br><span class="line">37308 NodeManager</span><br><span class="line">37102 DFSZKFailoverController</span><br><span class="line">----------hadoop101----------</span><br><span class="line">40129 NameNode</span><br><span class="line">40451 JournalNode</span><br><span class="line">41075 Jps</span><br><span class="line">40788 ResourceManager</span><br><span class="line">40246 DataNode</span><br><span class="line">40891 NodeManager</span><br><span class="line">40653 DFSZKFailoverController</span><br><span class="line">28798 QuorumPeerMain</span><br><span class="line"></span><br><span class="line">----------execute <span class="string">"jps"</span> <span class="keyword">in</span> cluster takes 4 seconds----------</span><br><span class="line"></span><br><span class="line">[tomandersen@hadoop101 hadoop-2.7.7]$ </span><br><span class="line">[tomandersen@hadoop101 hadoop-2.7.7]$</span><br></pre></td></tr></table></figure><h4 id="可以通过haadmin和rmadmin命令查看和更改HA节点状态，集群中只有一个NN和RM处于Active，其余皆为StandBy："><a href="#可以通过haadmin和rmadmin命令查看和更改HA节点状态，集群中只有一个NN和RM处于Active，其余皆为StandBy：" class="headerlink" title="可以通过haadmin和rmadmin命令查看和更改HA节点状态，集群中只有一个NN和RM处于Active，其余皆为StandBy："></a>可以通过<code>haadmin</code>和<code>rmadmin</code>命令查看和更改HA节点状态，集群中只有一个NN和RM处于Active，其余皆为StandBy：</h4><h5 id="1-haadmin-获取HDFS-HA模式中指定NN节点的状态，如："><a href="#1-haadmin-获取HDFS-HA模式中指定NN节点的状态，如：" class="headerlink" title="(1) haadmin 获取HDFS HA模式中指定NN节点的状态，如："></a>(1) haadmin 获取HDFS HA模式中指定NN节点的状态，如：</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs haadmin -getServiceState nn1</span><br></pre></td></tr></table></figure><h5 id="2-haadmin-强制修改HDFS-HA模式中指定NN节点的状态，如："><a href="#2-haadmin-强制修改HDFS-HA模式中指定NN节点的状态，如：" class="headerlink" title="(2) haadmin 强制修改HDFS HA模式中指定NN节点的状态，如："></a>(2) haadmin 强制修改HDFS HA模式中指定NN节点的状态，如：</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs haadmin -transitionToActive --forcemanual nn1</span><br></pre></td></tr></table></figure><h5 id="3-rmadmin-获取YARN-HA模式中指定节点的状态，如："><a href="#3-rmadmin-获取YARN-HA模式中指定节点的状态，如：" class="headerlink" title="(3) rmadmin 获取YARN HA模式中指定节点的状态，如："></a>(3) rmadmin 获取YARN HA模式中指定节点的状态，如：</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/yarn rmadmin -getServiceState rm1</span><br></pre></td></tr></table></figure><h5 id="4-rmadmin-强制修改YARN-HA模式中指定节点的状态，如："><a href="#4-rmadmin-强制修改YARN-HA模式中指定节点的状态，如：" class="headerlink" title="(4) rmadmin 强制修改YARN HA模式中指定节点的状态，如："></a>(4) rmadmin 强制修改YARN HA模式中指定节点的状态，如：</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/yarn rmadmin -transitionToStandby --forcemanual rm2</span><br></pre></td></tr></table></figure><h4 id="若要关闭HDFS集群，直接使用对应命令关闭即可："><a href="#若要关闭HDFS集群，直接使用对应命令关闭即可：" class="headerlink" title="若要关闭HDFS集群，直接使用对应命令关闭即可："></a>若要关闭HDFS集群，直接使用对应命令关闭即可：</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sbin/stop-dfs.sh</span><br></pre></td></tr></table></figure><h4 id="若要关闭YARN集群，除了在其中的一个RM节点上执行关闭YARN命令之外还需要在其他节点上手动关闭RM进程："><a href="#若要关闭YARN集群，除了在其中的一个RM节点上执行关闭YARN命令之外还需要在其他节点上手动关闭RM进程：" class="headerlink" title="若要关闭YARN集群，除了在其中的一个RM节点上执行关闭YARN命令之外还需要在其他节点上手动关闭RM进程："></a>若要关闭YARN集群，除了在其中的一个RM节点上执行关闭YARN命令之外还需要在其他节点上手动关闭RM进程：</h4><h5 id="关闭YARN集群："><a href="#关闭YARN集群：" class="headerlink" title="关闭YARN集群："></a>关闭YARN集群：</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sbin/stop-yarn.sh</span><br></pre></td></tr></table></figure><h5 id="在其他节点上关闭RM："><a href="#在其他节点上关闭RM：" class="headerlink" title="在其他节点上关闭RM："></a>在其他节点上关闭RM：</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sbin/yarn-daemon.sh stop resourcemanager</span><br></pre></td></tr></table></figure><h3 id="b-正常启动集群"><a href="#b-正常启动集群" class="headerlink" title="b) 正常启动集群"></a>b) 正常启动集群</h3><h4 id="启动HDFS-HA集群："><a href="#启动HDFS-HA集群：" class="headerlink" title="启动HDFS HA集群："></a>启动HDFS HA集群：</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sbin/start-dfs.sh</span><br></pre></td></tr></table></figure><h4 id="启动YARN-HA集群："><a href="#启动YARN-HA集群：" class="headerlink" title="启动YARN HA集群："></a>启动YARN HA集群：</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sbin/start-yarn.sh</span><br></pre></td></tr></table></figure><h4 id="同时并在其他RM节点上手动启动RM进程进而开启RM-HA，第二个启动的RM为Active："><a href="#同时并在其他RM节点上手动启动RM进程进而开启RM-HA，第二个启动的RM为Active：" class="headerlink" title="同时并在其他RM节点上手动启动RM进程进而开启RM HA，第二个启动的RM为Active："></a>同时并在其他RM节点上手动启动RM进程进而开启RM HA，第二个启动的RM为Active：</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sbin/yarn-daemon.sh start resourcemanager</span><br></pre></td></tr></table></figure><hr><h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
    
    </summary>
    
    
      <category term="Hadoop" scheme="https://tomandersen-cc.github.io/categories/Hadoop/"/>
    
    
      <category term="CentOS7" scheme="https://tomandersen-cc.github.io/tags/CentOS7/"/>
    
      <category term="大数据" scheme="https://tomandersen-cc.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Hadoop" scheme="https://tomandersen-cc.github.io/tags/Hadoop/"/>
    
      <category term="HA" scheme="https://tomandersen-cc.github.io/tags/HA/"/>
    
  </entry>
  
  <entry>
    <title>Zookeeper使用ssh远程启动脚本失败的解决方案</title>
    <link href="https://tomandersen-cc.github.io/2020/02/20/Zookeeper%E4%BD%BF%E7%94%A8ssh%E8%BF%9C%E7%A8%8B%E5%90%AF%E5%8A%A8%E8%84%9A%E6%9C%AC%E5%A4%B1%E8%B4%A5%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"/>
    <id>https://tomandersen-cc.github.io/2020/02/20/Zookeeper%E4%BD%BF%E7%94%A8ssh%E8%BF%9C%E7%A8%8B%E5%90%AF%E5%8A%A8%E8%84%9A%E6%9C%AC%E5%A4%B1%E8%B4%A5%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/</id>
    <published>2020-02-20T09:57:46.000Z</published>
    <updated>2020-03-06T12:23:16.491Z</updated>
    
    <content type="html"><![CDATA[<hr><a id="more"></a><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul><li><strong>本文主要记录一次解决问题的经历</strong></li></ul><hr><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><ul><li>在编写zookeeper群起脚本时，想要基于ssh命令来启动集群中所有zookeeper服务器节点。但是在使用ssh远程执行远端脚本时，控制台输出显示远端脚本已经正常运行结束，远端zookeeper进程实际上却未能运行。使用的ssh命令如下：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[tomandersen@hadoop101 bin]$ ssh hadoop102 <span class="string">"<span class="variable">$ZOOKEEPER_HOME</span>/bin/zkServer.sh start"</span></span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.4.14/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br></pre></td></tr></table></figure><hr><h2 id="解决过程"><a href="#解决过程" class="headerlink" title="解决过程"></a>解决过程</h2><h3 id="1）查看远端主机的运行日志zookeeper-out："><a href="#1）查看远端主机的运行日志zookeeper-out：" class="headerlink" title="1）查看远端主机的运行日志zookeeper.out："></a>1）查看远端主机的运行日志zookeeper.out：</h3><ul><li>zookeeper.out默认输出在启动zkServer.sh脚本的当前路径下，查看其中内容，如下所示：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[tomandersen@hadoop102 runtime]$ cat zookeeper.out </span><br><span class="line">nohup: 无法运行命令<span class="string">"java"</span>: 没有那个文件或目录</span><br></pre></td></tr></table></figure><ul><li>发现脚本中无法找到<code>java</code>命令（猜测可能是环境变量的问题）</li></ul><h3 id="2）尝试在脚本zkServer-sh中定位java命令使用位置："><a href="#2）尝试在脚本zkServer-sh中定位java命令使用位置：" class="headerlink" title="2）尝试在脚本zkServer.sh中定位java命令使用位置："></a>2）尝试在脚本zkServer.sh中定位java命令使用位置：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[tomandersen@hadoop102 bin]$ cat zkServer.sh | grep java -n</span><br><span class="line">20:<span class="comment"># it should be linked to and not copied. Things like java jar files are found</span></span><br><span class="line">39:<span class="comment"># http://java.sun.com/javase/6/docs/technotes/guides/management/agent.html</span></span><br></pre></td></tr></table></figure><ul><li>但发现其中都是注释，并没有显示调用java命令。然后查看zkServer.sh中的内容，发现此脚本在开头还使用<code>. zkEnv.sh</code>的方式运行了zkEnv.sh脚本，因此我们再去此脚本中定位java命令</li></ul><h3 id="3）尝试在脚本zkEnv-sh中定位java命令使用位置："><a href="#3）尝试在脚本zkEnv-sh中定位java命令使用位置：" class="headerlink" title="3）尝试在脚本zkEnv.sh中定位java命令使用位置："></a>3）尝试在脚本zkEnv.sh中定位java命令使用位置：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[tomandersen@hadoop102 bin]$ cat zkEnv.sh | grep java -n</span><br><span class="line">49:<span class="keyword">if</span> [ -f <span class="string">"<span class="variable">$ZOOCFGDIR</span>/java.env"</span> ]</span><br><span class="line">51:    . <span class="string">"<span class="variable">$ZOOCFGDIR</span>/java.env"</span></span><br><span class="line">69:  JAVA=<span class="string">"<span class="variable">$JAVA_HOME</span>/bin/java"</span></span><br><span class="line">71:  JAVA=java</span><br></pre></td></tr></table></figure><ul><li>查看具体程序段：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">68 <span class="keyword">if</span> [ <span class="string">"<span class="variable">$JAVA_HOME</span>"</span> != <span class="string">""</span> ]; <span class="keyword">then</span></span><br><span class="line">69   JAVA=<span class="string">"<span class="variable">$JAVA_HOME</span>/bin/java"</span></span><br><span class="line">70 <span class="keyword">else</span></span><br><span class="line">71   JAVA=java</span><br><span class="line">72 <span class="keyword">fi</span></span><br></pre></td></tr></table></figure><ul><li>结果发现此段程序并没有什么异常，于是尝试验证之前的推测，可能是环境变量导致此次错误</li></ul><h3 id="4）查看远端环境变量主机设置："><a href="#4）查看远端环境变量主机设置：" class="headerlink" title="4）查看远端环境变量主机设置："></a>4）查看远端环境变量主机设置：</h3><ul><li>使用ssh登录远端主机后，查看Java环境变量：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[tomandersen@hadoop102 bin]$ <span class="built_in">echo</span> <span class="variable">$JAVA_HOME</span></span><br><span class="line">/opt/module/jdk1.8.0_221</span><br><span class="line">[tomandersen@hadoop102 bin]$ <span class="built_in">which</span> java</span><br><span class="line">/opt/module/jdk1.8.0_221/bin/java</span><br><span class="line">[tomandersen@hadoop102 bin]$ <span class="built_in">echo</span> <span class="variable">$PATH</span> | grep <span class="variable">$JAVA_HOME</span></span><br><span class="line">/usr/<span class="built_in">local</span>/bin:/usr/bin:/usr/<span class="built_in">local</span>/sbin:/usr/sbin:/opt/module/jdk1.8.0_221/bin:/opt/module/hadoop-2.7.7/bin:/opt/module/hadoop-2.7.7/sbin:/opt/module/zookeeper-3.4.14/bin:/home/TomAndersen/.<span class="built_in">local</span>/bin:/home/TomAndersen/bin</span><br></pre></td></tr></table></figure><ul><li>结果发现环境变量配置正常，然后尝试使用ssh命令远程调用Java环境变量，查看是否能正常输出：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[tomandersen@hadoop101 bin]$ ssh hadoop102 <span class="string">"echo <span class="variable">$JAVA_HOME</span>"</span></span><br><span class="line">/opt/module/jdk1.8.0_221</span><br><span class="line">[tomandersen@hadoop101 bin]$ ssh hadoop102 <span class="string">"which java"</span></span><br><span class="line"><span class="built_in">which</span>: no java <span class="keyword">in</span> (/usr/<span class="built_in">local</span>/bin:/usr/bin)</span><br><span class="line">[tomandersen@hadoop101 bin]$ ssh hadoop102 <span class="string">"echo <span class="variable">$PATH</span>"</span></span><br><span class="line">/usr/<span class="built_in">local</span>/bin:/usr/bin:/usr/<span class="built_in">local</span>/sbin:/usr/sbin:/opt/module/jdk1.8.0_221/bin:/opt/module/hadoop-2.7.7/bin:/opt/module/hadoop-2.7.7/sbin:/opt/module/zookeeper-3.4.14/bin:/home/TomAndersen/.<span class="built_in">local</span>/bin:/home/TomAndersen/bin</span><br></pre></td></tr></table></figure><ul><li>显然使用ssh远程执行Shell命令能够读取环境变量，且远端主机各个环境变量设置正常，但是却无法使用java命令</li><li>于是猜测是不是使用ssh工具执行远端脚本时，没有加载环境变量。</li></ul><h3 id="5）验证猜想"><a href="#5）验证猜想" class="headerlink" title="5）验证猜想"></a>5）验证猜想</h3><ul><li>接下来在远端主机上创建测试脚本<code>test.sh</code>，用于测试是否使用ssh工具远端执行此脚本时无法读取环境变量，其中测试脚本内容如下：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1 <span class="built_in">echo</span> <span class="variable">$PATH</span></span><br><span class="line">2 <span class="built_in">echo</span> <span class="variable">$JAVA_HOME</span></span><br><span class="line">3 <span class="built_in">which</span> java</span><br></pre></td></tr></table></figure><ul><li>使用ssh远程执行此脚本：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[tomandersen@hadoop101 ~]$ ssh hadoop102 <span class="string">"~/test.sh"</span></span><br><span class="line">/usr/<span class="built_in">local</span>/bin:/usr/bin</span><br><span class="line"></span><br><span class="line"><span class="built_in">which</span>: no java <span class="keyword">in</span> (/usr/<span class="built_in">local</span>/bin:/usr/bin)</span><br></pre></td></tr></table></figure><ul><li>至此我们可以得出结果，使用ssh执行远端脚本时，在脚本内是没有加载环境变量的，即没有加载<code>/etc/profile</code>文件，因此我们修改原始ssh远程执行命令，在前面加上<code>source /etc/profile</code>，即主动加载系统环境变量，然后发现脚本能够正常调用环境变量：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[tomandersen@hadoop101 ~]$ ssh hadoop102 <span class="string">"source /etc/profile;~/test.sh"</span></span><br><span class="line">/usr/<span class="built_in">local</span>/bin:/usr/bin:/usr/<span class="built_in">local</span>/sbin:/usr/sbin:/opt/module/jdk1.8.0_221/bin:/opt/module/hadoop-2.7.7/bin:/opt/module/hadoop-2.7.7/sbin:/opt/module/zookeeper-3.4.14/bin</span><br><span class="line">/opt/module/jdk1.8.0_221</span><br><span class="line">/opt/module/jdk1.8.0_221/bin/java</span><br></pre></td></tr></table></figure><ul><li>至此问题解决</li></ul><hr><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><h3 id="①-在ssh远程执行的正式命令之前加上source-etc-profile-，即主动加载环境变量"><a href="#①-在ssh远程执行的正式命令之前加上source-etc-profile-，即主动加载环境变量" class="headerlink" title="① 在ssh远程执行的正式命令之前加上source /etc/profile;，即主动加载环境变量"></a>① 在ssh远程执行的正式命令之前加上<code>source /etc/profile;</code>，即主动加载环境变量</h3><h3 id="②-同理可以在对应用户的-bashrc文件末尾加入source-etc-profile，也是同样的效果"><a href="#②-同理可以在对应用户的-bashrc文件末尾加入source-etc-profile，也是同样的效果" class="headerlink" title="② 同理可以在对应用户的.bashrc文件末尾加入source /etc/profile，也是同样的效果"></a>② 同理可以在对应用户的.bashrc文件末尾加入<code>source /etc/profile</code>，也是同样的效果</h3><hr><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="http://feihu.me/blog/2014/env-problem-when-ssh-executing-command-on-remote/" target="_blank" rel="noopener">ssh连接远程主机执行脚本的环境变量问题</a></p><hr><h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
    
    </summary>
    
    
      <category term="Zookeeper" scheme="https://tomandersen-cc.github.io/categories/Zookeeper/"/>
    
    
      <category term="CentOS7" scheme="https://tomandersen-cc.github.io/tags/CentOS7/"/>
    
      <category term="大数据" scheme="https://tomandersen-cc.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Zookeeper" scheme="https://tomandersen-cc.github.io/tags/Zookeeper/"/>
    
      <category term="ssh" scheme="https://tomandersen-cc.github.io/tags/ssh/"/>
    
  </entry>
  
  <entry>
    <title>Zookeeper分布式安装配置过程</title>
    <link href="https://tomandersen-cc.github.io/2020/02/20/Zookeeper%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%E8%BF%87%E7%A8%8B/"/>
    <id>https://tomandersen-cc.github.io/2020/02/20/Zookeeper%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%E8%BF%87%E7%A8%8B/</id>
    <published>2020-02-20T08:58:34.000Z</published>
    <updated>2020-03-08T14:05:05.540Z</updated>
    
    <content type="html"><![CDATA[<hr><a id="more"></a><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul><li>集群主机：hadoop101、hadoop102、hadoop103</li><li>操作系统：CentOS 7</li><li>Zookeeper版本：3.4.14</li><li>JDK版本：1.8.0_221</li><li>所有配置操作都必须在同一用户下</li></ul><hr><h2 id="1-下载Zookeeper"><a href="#1-下载Zookeeper" class="headerlink" title="1. 下载Zookeeper"></a>1. 下载Zookeeper</h2><h3 id="在Zookeeper官网提供的镜像网站上下载合适版本，本次下载的版本是3-4-14"><a href="#在Zookeeper官网提供的镜像网站上下载合适版本，本次下载的版本是3-4-14" class="headerlink" title="在Zookeeper官网提供的镜像网站上下载合适版本，本次下载的版本是3.4.14"></a>在<a href="http://zookeeper.apache.org/" target="_blank" rel="noopener">Zookeeper官网</a>提供的<a href="http://archive.apache.org/dist/zookeeper/" target="_blank" rel="noopener">镜像网站</a>上下载合适版本，本次下载的版本是3.4.14</h3><ul><li><img src="https://img-blog.csdnimg.cn/2020022016535789.png" alt="下载Zookeeper"></li></ul><ul><li><p><img src="https://img-blog.csdnimg.cn/20200220165429871.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RvbUFuZGVyc2Vu,size_16,color_FFFFFF,t_70" alt="下载Zookeeper"></p></li><li><p><img src="https://img-blog.csdnimg.cn/20200220165542460.png" alt="下载Zookeeper"></p></li></ul><hr><h2 id="2-安装Zookeeper"><a href="#2-安装Zookeeper" class="headerlink" title="2. 安装Zookeeper"></a>2. 安装Zookeeper</h2><h3 id="直接使用普通用户将压缩包解压到目标路径下即可，本次解压路径为-opt-module-，具体命令如下："><a href="#直接使用普通用户将压缩包解压到目标路径下即可，本次解压路径为-opt-module-，具体命令如下：" class="headerlink" title="直接使用普通用户将压缩包解压到目标路径下即可，本次解压路径为/opt/module/，具体命令如下："></a>直接使用普通用户将压缩包解压到目标路径下即可，本次解压路径为<code>/opt/module/</code>，具体命令如下：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[tomandersen@hadoop101 software]$ tar -xzvf zookeeper-3.4.14.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure><p><strong>注意：本次实验中此路径已经使用root用户将所有权给了用户tomandersen，默认是root用户才能访问，如果要使用相同路径，记得先排查确保文件所属用户，避免后续因为权限而出现混乱。为了避免麻烦，建议使用自定义路径</strong></p><hr><h2 id="3-配置conf-zoo-cfg"><a href="#3-配置conf-zoo-cfg" class="headerlink" title="3. 配置conf/zoo.cfg"></a>3. 配置conf/zoo.cfg</h2><h3 id="1）将conf文件夹下的zoo-sample-cfg文件更名为zoo-cfg："><a href="#1）将conf文件夹下的zoo-sample-cfg文件更名为zoo-cfg：" class="headerlink" title="1）将conf文件夹下的zoo_sample.cfg文件更名为zoo.cfg："></a>1）将<code>conf</code>文件夹下的<code>zoo_sample.cfg</code>文件更名为<code>zoo.cfg</code>：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[tomandersen@hadoop101 software]$ mv zoo_sample.cfg zoo.cfg</span><br></pre></td></tr></table></figure><h3 id="2）然后编辑zoo-cfg，修改其中参数，具体参数可参考如下："><a href="#2）然后编辑zoo-cfg，修改其中参数，具体参数可参考如下：" class="headerlink" title="2）然后编辑zoo.cfg，修改其中参数，具体参数可参考如下："></a>2）然后编辑<code>zoo.cfg</code>，修改其中参数，具体参数可参考如下：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">tickTime=2000</span><br><span class="line">initLimit=10</span><br><span class="line">syncLimit=5</span><br><span class="line">dataDir=/opt/module/zookeeper-3.4.14/zkData</span><br><span class="line">dataLogDir=/opt/module/zookeeper-3.4.14/logs/transaction</span><br><span class="line">clientPort=2181</span><br><span class="line">autopurge.snapRetainCount=3</span><br><span class="line">autopurge.purgeInterval=1</span><br><span class="line"><span class="comment"># The zookeeper cluster setting</span></span><br><span class="line">server.1=hadoop101:2888:3888</span><br><span class="line">server.2=hadoop102:2888:3888</span><br><span class="line">server.3=hadoop103:2888:3888</span><br></pre></td></tr></table></figure><h3 id="其中需要特别说明的参数："><a href="#其中需要特别说明的参数：" class="headerlink" title="其中需要特别说明的参数："></a>其中需要特别说明的参数：</h3><ul><li><strong>dataLogDir</strong>：在默认的配置中并没有此参数的相关描述，但在官方文档中描述到，此参数用于设置 <strong>事务日志（transaction log）</strong> 存放路径，将<code>dataLogDir</code>和<code>dataDir</code>设置于不同存储设备对于Zookeeper的吞吐率有较大影响。所以这里即便是没有设置多余的设备用于存储日志，也建议将其分开存储</li><li><strong>autopurge.snapRetainCount</strong>和<strong>autopurge.purgeInterval</strong>：这两个参数配合起来就是在每间隔1个小时清洗一次快照snapshot和对应的事务日志transaction log，只保存最新的三个版本的快照和其对应的事务日志</li><li><strong>server.&lt;myid&gt;=&lt;host&gt;:2888:3888</strong>：此参数专门用于设置Zookeeper集群，其中<code>&lt;myid&gt;</code>指的是zookeeper服务器ID，主要通过<code>dataDir</code>路径下的<code>myid</code>文件来确定，此文件需要自己手动创建，文件中内容只能有对应服务器ID，不能有任何多余内容。<code>&lt;host&gt;</code>则指的是zookeeper服务器主机ip，由于此前已经在<code>/etc/hosts</code>中设置了映射，所有此处直接使用设置的主机名<code>hadoop101 hadoop102 hadoop103</code>，其中<code>2888</code>和<code>3888</code>分别指的是zookeeper集群仲裁模式（完全分布式）下<code>Leader</code>和<code>Follower</code>通信端口，以及zookeeper集群选取Leader时各个服务器之间的通信端口</li></ul><hr><h2 id="4-设置myid"><a href="#4-设置myid" class="headerlink" title="4. 设置myid"></a>4. 设置myid</h2><h3 id="官方文档原文："><a href="#官方文档原文：" class="headerlink" title="官方文档原文："></a>官方文档原文：</h3><ul><li>You attribute the server id to each machine by creating a file named <strong>myid</strong>, one for each server, which resides in that server’s data directory, as specified by the configuration file parameter <strong>dataDir</strong>.</li><li>The <strong>myid</strong> file consists of a single line containing only the text of that machine’s id. So <strong>myid of server 1 would contain the text “1” and nothing else</strong>. The id must be unique within the ensemble and should have a value <strong>between 1 and 255</strong>. <strong>IMPORTANT:</strong> if you enable extended features such as TTL Nodes (see below) the id must be between 1 and 254 due to internal limitations.</li></ul><h3 id="总而言之："><a href="#总而言之：" class="headerlink" title="总而言之："></a>总而言之：</h3><ul><li>文件<code>myid</code>必须在<code>dataDir</code>路径下，其中内容只能是单个整数，且<code>myid</code>的值必须与<code>zoo.cfg</code>中的配置相对应，<code>myid</code>的值必须在1~255之间，且唯一</li></ul><h3 id="1）在dataDir路径下创建myid："><a href="#1）在dataDir路径下创建myid：" class="headerlink" title="1）在dataDir路径下创建myid："></a>1）在dataDir路径下创建myid：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[tomandersen@hadoop101 zookeeper-3.4.14]$ <span class="built_in">echo</span> 1 &gt; zkData/myid</span><br></pre></td></tr></table></figure><h3 id="2）将zookeeper文件夹分发给集群主机："><a href="#2）将zookeeper文件夹分发给集群主机：" class="headerlink" title="2）将zookeeper文件夹分发给集群主机："></a>2）将zookeeper文件夹分发给集群主机：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[tomandersen@hadoop101 module]$ scp -r zookeeper-3.4.14/ hadoop102:/opt/module/</span><br><span class="line">[tomandersen@hadoop101 module]$ scp -r zookeeper-3.4.14/ hadoop103:/opt/module/</span><br></pre></td></tr></table></figure><p><strong>这里可以写个分发脚本，因为本次实验集群数量较少，所以直接手敲了</strong></p><h3 id="3）分别设置集群各个主机的myid："><a href="#3）分别设置集群各个主机的myid：" class="headerlink" title="3）分别设置集群各个主机的myid："></a>3）分别设置集群各个主机的myid：</h3><p><strong>这里就不再赘述了，设置方式和规则与前述相同</strong></p><hr><h2 id="5-配置群起脚本"><a href="#5-配置群起脚本" class="headerlink" title="5. 配置群起脚本"></a>5. 配置群起脚本</h2><h3 id="1）Zookeeper中没有设置群起脚本，需要自行编写。本次使用ssh工具来实现远程调用zkServer脚本启动Zookeeper集群。具体可参考如下："><a href="#1）Zookeeper中没有设置群起脚本，需要自行编写。本次使用ssh工具来实现远程调用zkServer脚本启动Zookeeper集群。具体可参考如下：" class="headerlink" title="1）Zookeeper中没有设置群起脚本，需要自行编写。本次使用ssh工具来实现远程调用zkServer脚本启动Zookeeper集群。具体可参考如下："></a>1）Zookeeper中没有设置群起脚本，需要自行编写。本次使用ssh工具来实现远程调用zkServer脚本启动Zookeeper集群。具体可参考如下：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"> 1 <span class="comment">#!/bin/bash</span></span><br><span class="line"> 2 <span class="comment"># 此脚本用于启动zookeeper集群，使用方式和zkServer.sh相同</span></span><br><span class="line"> 3 </span><br><span class="line"> 4 <span class="comment"># 判断输入参数个数</span></span><br><span class="line"> 5 <span class="keyword">if</span> ((<span class="variable">$#</span> != 1)); <span class="keyword">then</span></span><br><span class="line"> 6     <span class="built_in">echo</span> <span class="string">"Wrong parameters!"</span></span><br><span class="line"> 7     <span class="built_in">exit</span> 1</span><br><span class="line"> 8 <span class="keyword">fi</span></span><br><span class="line"> 9 <span class="comment"># 获取当前用户</span></span><br><span class="line">10 user=$(whoami)</span><br><span class="line">11 <span class="comment"># 集群的ip地址</span></span><br><span class="line">12 cluster=<span class="string">"hadoop101 hadoop102 hadoop103"</span></span><br><span class="line">13 </span><br><span class="line">14 <span class="comment"># 根据输入参数调用对应功能</span></span><br><span class="line">15 <span class="keyword">case</span> <span class="variable">$1</span> <span class="keyword">in</span></span><br><span class="line">16 <span class="string">"start"</span>)</span><br><span class="line">17     <span class="keyword">for</span> host <span class="keyword">in</span> <span class="variable">$cluster</span>; <span class="keyword">do</span></span><br><span class="line">18         ssh <span class="variable">$user</span>@<span class="variable">$host</span> <span class="string">"source /etc/profile;<span class="variable">$ZOOKEEPER_HOME</span>/bin/zkServer.sh start"</span></span><br><span class="line">19     <span class="keyword">done</span></span><br><span class="line">20     ;;</span><br><span class="line">21 <span class="string">"status"</span>)</span><br><span class="line">22     <span class="keyword">for</span> host <span class="keyword">in</span> <span class="variable">$cluster</span>; <span class="keyword">do</span></span><br><span class="line">23         ssh <span class="variable">$user</span>@<span class="variable">$host</span> <span class="string">"source /etc/profile;<span class="variable">$ZOOKEEPER_HOME</span>/bin/zkServer.sh status"</span></span><br><span class="line">24     <span class="keyword">done</span></span><br><span class="line">25     ;;</span><br><span class="line">26 <span class="string">"stop"</span>)</span><br><span class="line">27     <span class="keyword">for</span> host <span class="keyword">in</span> <span class="variable">$cluster</span>; <span class="keyword">do</span></span><br><span class="line">28         ssh <span class="variable">$user</span>@<span class="variable">$host</span> <span class="string">"source /etc/profile;<span class="variable">$ZOOKEEPER_HOME</span>/bin/zkServer.sh stop"</span></span><br><span class="line">29     <span class="keyword">done</span></span><br><span class="line">30     ;;</span><br><span class="line">31 *)</span><br><span class="line">32     <span class="built_in">echo</span> <span class="string">"Worong parameter!"</span></span><br><span class="line">33     <span class="built_in">exit</span> 1</span><br><span class="line">34     ;;</span><br><span class="line">35 <span class="keyword">esac</span></span><br></pre></td></tr></table></figure><p><strong>注意：其中环境变量<code>$ZOOKEEPER_HOME</code>已经在<code>/etc/profile</code>中配置为zookeeper对应安装路径</strong></p><h3 id="2）运行群起脚本，测试集群是否正常运行"><a href="#2）运行群起脚本，测试集群是否正常运行" class="headerlink" title="2）运行群起脚本，测试集群是否正常运行"></a>2）运行群起脚本，测试集群是否正常运行</h3><ul><li><strong>启动集群</strong></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[tomandersen@hadoop101 bin]$ zkCluster-server.sh start</span><br></pre></td></tr></table></figure><ul><li><strong>查看集群状态</strong></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[tomandersen@hadoop101 bin]$ zkCluster-server.sh status</span><br></pre></td></tr></table></figure><ul><li><strong>关闭集群</strong></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[tomandersen@hadoop101 bin]$ zkCluster-server.sh stop</span><br></pre></td></tr></table></figure><p><strong>如果启动失败，可以查看启动脚本的当前路径下的zookeeper.out文件，其中记录有错误原因。此文件为Zookeeper的运行日志。相关zookeeper日志介绍可以参考：<a href="https://blog.csdn.net/TomAndersen/article/details/104406465" target="_blank" rel="noopener">Zookeeper中日志文件种类</a></strong></p><hr><h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
    
    </summary>
    
    
      <category term="Zookeeper" scheme="https://tomandersen-cc.github.io/categories/Zookeeper/"/>
    
    
      <category term="CentOS7" scheme="https://tomandersen-cc.github.io/tags/CentOS7/"/>
    
      <category term="大数据" scheme="https://tomandersen-cc.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Zookeeper" scheme="https://tomandersen-cc.github.io/tags/Zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>Zookeeper中stat结构体参数介绍</title>
    <link href="https://tomandersen-cc.github.io/2020/02/20/Zookeeper%E4%B8%ADstat%E7%BB%93%E6%9E%84%E4%BD%93%E5%8F%82%E6%95%B0%E4%BB%8B%E7%BB%8D/"/>
    <id>https://tomandersen-cc.github.io/2020/02/20/Zookeeper%E4%B8%ADstat%E7%BB%93%E6%9E%84%E4%BD%93%E5%8F%82%E6%95%B0%E4%BB%8B%E7%BB%8D/</id>
    <published>2020-02-20T03:45:29.000Z</published>
    <updated>2020-03-08T14:05:50.492Z</updated>
    
    <content type="html"><![CDATA[<hr><a id="more"></a><hr><h2 id="stat结构体示例"><a href="#stat结构体示例" class="headerlink" title="stat结构体示例"></a>stat结构体示例</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] <span class="built_in">stat</span> /</span><br><span class="line">cZxid = 0x0</span><br><span class="line">ctime = Thu Jan 01 08:00:00 CST 1970</span><br><span class="line">mZxid = 0x0</span><br><span class="line">mtime = Thu Jan 01 08:00:00 CST 1970</span><br><span class="line">pZxid = 0x300000002</span><br><span class="line">cversion = 0</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 0</span><br><span class="line">numChildren = 2</span><br></pre></td></tr></table></figure><hr><h2 id="参数介绍"><a href="#参数介绍" class="headerlink" title="参数介绍"></a>参数介绍</h2><h3 id="1-cZxid"><a href="#1-cZxid" class="headerlink" title="1.  cZxid"></a>1.  cZxid</h3><ul><li><p>创建节点的事务zxid</p></li><li><p>每次修改zookeeper状态都会收到一个zxid形式的时间戳,也就是zookeeper事务ID。事务ID是zookeeper中所有修改总的次序。每个修改都有唯一的zxid,如果zxid1小于zxid,那么zxid在zxid2之前发生。</p></li></ul><h3 id="2-ctime"><a href="#2-ctime" class="headerlink" title="2.  ctime"></a>2.  ctime</h3><ul><li>znode被创建的毫秒数(从1970年开始)</li></ul><h3 id="3-mZxid"><a href="#3-mZxid" class="headerlink" title="3.  mZxid"></a>3.  mZxid</h3><ul><li>znode最后更新的事务zxid</li></ul><h3 id="4-mtime"><a href="#4-mtime" class="headerlink" title="4.  mtime"></a>4.  mtime</h3><ul><li>znode最后修改的毫秒数(从1970年开始)</li></ul><h3 id="5-pZxid"><a href="#5-pZxid" class="headerlink" title="5.  pZxid"></a>5.  pZxid</h3><ul><li>最后更新的子节点zxid</li></ul><h3 id="6-cversion"><a href="#6-cversion" class="headerlink" title="6.  cversion"></a>6.  cversion</h3><ul><li>znode子节点变化号,znode子节点修改次数</li></ul><h3 id="7-dataversion"><a href="#7-dataversion" class="headerlink" title="7.  dataversion"></a>7.  dataversion</h3><ul><li>znode数据变化号</li></ul><h3 id="8-aclVersion"><a href="#8-aclVersion" class="headerlink" title="8.  aclVersion"></a>8.  aclVersion</h3><ul><li>znode访问控制列表的变化号</li></ul><h3 id="9-ephemeralOwner"><a href="#9-ephemeralOwner" class="headerlink" title="9.  ephemeralOwner"></a>9.  ephemeralOwner</h3><ul><li>如果是临时节点,这个是znode拥有session id。如果不是临时节点则是0。</li></ul><h3 id="10-dataLength"><a href="#10-dataLength" class="headerlink" title="10.  dataLength"></a>10.  dataLength</h3><ul><li>znode的数据长度</li></ul><h3 id="11-numChildren"><a href="#11-numChildren" class="headerlink" title="11.  numChildren"></a>11.  numChildren</h3><ul><li>znode子节点数量</li></ul><hr><h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
    
    </summary>
    
    
      <category term="Zookeeper" scheme="https://tomandersen-cc.github.io/categories/Zookeeper/"/>
    
    
      <category term="CentOS7" scheme="https://tomandersen-cc.github.io/tags/CentOS7/"/>
    
      <category term="大数据" scheme="https://tomandersen-cc.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Zookeeper" scheme="https://tomandersen-cc.github.io/tags/Zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>Zookeeper中日志文件种类</title>
    <link href="https://tomandersen-cc.github.io/2020/02/20/Zookeeper%E4%B8%AD%E6%97%A5%E5%BF%97%E6%96%87%E4%BB%B6%E7%A7%8D%E7%B1%BB/"/>
    <id>https://tomandersen-cc.github.io/2020/02/20/Zookeeper%E4%B8%AD%E6%97%A5%E5%BF%97%E6%96%87%E4%BB%B6%E7%A7%8D%E7%B1%BB/</id>
    <published>2020-02-20T02:45:46.000Z</published>
    <updated>2020-03-06T12:25:01.616Z</updated>
    
    <content type="html"><![CDATA[<hr><a id="more"></a><hr><h2 id="日志种类"><a href="#日志种类" class="headerlink" title="日志种类"></a>日志种类</h2><ul><li>Zookeeper在运行期间主要输出三类文件：<strong>快照（snapshot）</strong>、<strong>事务日志（transaction log）</strong>、<strong>运行日志</strong>。其中snapshot存放于<code>dataDir</code>中，事务日志在未设置<code>dataLogDir</code>参数时默认与snapshot存放路径相同，运行日志zookeeper.out默认存放在启动脚本的当前路径下</li><li>snapshot是zookeeper节点存储的数据的永久备份，而事务日志transaction log则是用于存储Znode的变化信息。当事务日志文件变得较大时，Zookeeper会将当前所有znode节点的最新状态生成快照snapshot存储到dataDir中，同时生成新的事务日志，用于接收最新的znodes变化。在生成新快照期间，也许会有新的事务被追加到旧的事务日志中，因此有些比快照更新的事务也许会存放在上一个版本的事务日志中</li><li>在Zookeeper的默认设置中，默认将snapshot和事务日志放置在同一个文件夹中，且默认会永久保存这两种文件。在Zookeeper 3.4.0之后可以通过设置<code>conf/zoo.cfg</code>中的<code>autopurge.snapRetainCount</code>和<code>autopurge.purgeInterval</code>参数来开启定时清理，每次保留固定数量的快照与其对应的事务日志</li><li>官方文档中建议将快照snapshot和事务日志transaction log存放于不同存储设备中，即单独设置<code>dataLogDir</code>存放事务日志，并且不与<code>dataDir</code>在同一设备下，因为是否设置独立存储设备用于存储日志，将会对Zookeeper吞吐量有较大影响</li><li>zookeeper.out文件在官方文档中并没有被称为运行日志，在此只是一种通俗的便于理解的称呼，zookeeper的默认存放路径在启动zkServer.sh脚本的当前路径下，这就导致每次启动zkServer.sh时都很难记住生成的zookeeper文件存放位置，因此建议修改其生成路径，便于每次出错后能快速定位错误</li><li>Zookeeper使用<code>SLF4J(Simple Logging Facade for Java)</code>作为其日志的基本框架（接口），为了向后兼容，Zookeeper绑定使用的是<code>LOG4J(Log for Java)</code>作为其具体的日志解决方案，log4j是Apache的开源项目之一。当然也可以在<code>SLF4J</code>的基础上使用其他的日志框架，这取决于你的具体应用</li></ul><hr><h2 id="独立存储事务日志"><a href="#独立存储事务日志" class="headerlink" title="独立存储事务日志"></a>独立存储事务日志</h2><ul><li><code>conf/zoo.cfg</code>的默认参数设置中没有<code>dataLogDir</code>参数，也没有相关介绍，但是在官方文档的Advanced Configuration部分有具体描述。具体设置可参考：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">15 <span class="comment"># 自定义Zookeeper集群数据本地存储路径</span></span><br><span class="line">16 dataDir=/opt/module/zookeeper-3.4.14/zkData</span><br><span class="line">17 <span class="comment"># 设置事务日志transaction log的存储路径,默认是dateDir,但是理应存储在不同设备下,避免对吞吐量造成负面影响</span></span><br><span class="line">18 dataLogDir=/opt/module/zookeeper-3.4.14/logs/transaction</span><br></pre></td></tr></table></figure><hr><h2 id="快照和事务日志定期清理"><a href="#快照和事务日志定期清理" class="headerlink" title="快照和事务日志定期清理"></a>快照和事务日志定期清理</h2><ul><li>在Zookeeper 3.4.0之后可以通过设置<code>conf/zoo.cfg</code>中的<code>autopurge.snapRetainCount</code>和<code>autopurge.purgeInterval</code>参数来开启定时清理，每次保留固定数量的快照与其对应的事务日志。具体设置可参考：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">34 <span class="comment"># The number of snapshots to retain in dataDir</span></span><br><span class="line">35 autopurge.snapRetainCount=3</span><br><span class="line">36 <span class="comment"># Purge task interval in hours</span></span><br><span class="line">37 <span class="comment"># Set to "0" to disable auto purge feature</span></span><br><span class="line">38 autopurge.purgeInterval=1</span><br></pre></td></tr></table></figure><hr><h2 id="事务日志可视化"><a href="#事务日志可视化" class="headerlink" title="事务日志可视化"></a>事务日志可视化</h2><ul><li>事务日志transaction log默认都是二进制文件，关于日志格式化输出，官方文档中也有相应介绍，基本都是基于zookeeper和slf4j提供的API实现二进制日志文件格式转换，命令使用格式为<code>java -cp zookeeper-3.4.14.jar:lib/slf4j-api-1.7.25.jar org.apache.zookeeper.server.LogFormatter  &lt;事务日志文件路径&gt;</code>，注意：其中所使用的包的版本需要根据自己的zookeeper调整，使用的LogFormatter类应该都是固定的。具体使用可参考：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[tomandersen@hadoop101 zookeeper-3.4.14]$ </span><br><span class="line">[tomandersen@hadoop101 zookeeper-3.4.14]$ </span><br><span class="line">[tomandersen@hadoop101 zookeeper-3.4.14]$ java -cp zookeeper-3.4.14.jar:lib/slf4j-api-1.7.25.jar  org.apache.zookeeper.server.LogFormatter   logs/transaction/version-2/log.300000001</span><br><span class="line">SLF4J: Failed to load class <span class="string">"org.slf4j.impl.StaticLoggerBinder"</span>.</span><br><span class="line">SLF4J: Defaulting to no-operation (NOP) logger implementation</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html<span class="comment">#StaticLoggerBinder for further details.</span></span><br><span class="line">ZooKeeper Transactional Log File with dbid 0 txnlog format version 2</span><br><span class="line">20-2-19 上午10时02分46秒 session 0x1000034fab40000 cxid 0x0 zxid 0x300000001 createSession 30000</span><br><span class="line"></span><br><span class="line">20-2-19 上午10时03分38秒 session 0x1000034fab40000 cxid 0x2 zxid 0x300000002 create <span class="string">'/TomAndersen,#5468697320697320546f6d416e64657273656e277320706572736f6e616c20736974652e,v&#123;s&#123;31,s&#123;'</span>world,<span class="string">'anyone&#125;&#125;&#125;,F,1</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">20-2-19 上午10时03分44秒 session 0x1000034fab40000 cxid 0x4 zxid 0x300000003 closeSession null</span></span><br><span class="line"><span class="string">EOF reached after 3 txns.</span></span><br><span class="line"><span class="string">[tomandersen@hadoop101 zookeeper-3.4.14]$ </span></span><br><span class="line"><span class="string">[tomandersen@hadoop101 zookeeper-3.4.14]$ </span></span><br><span class="line"><span class="string">[tomandersen@hadoop101 zookeeper-3.4.14]$</span></span><br></pre></td></tr></table></figure><hr><h2 id="修改运行日志输出路径"><a href="#修改运行日志输出路径" class="headerlink" title="修改运行日志输出路径"></a>修改运行日志输出路径</h2><ul><li>具体修改zookeeper.out的输出路径，可以参考：<a href="https://blog.csdn.net/TomAndersen/article/details/104405017" target="_blank" rel="noopener">Zookeeper修改运行日志zookeeper.out输出路径</a></li></ul><p><strong>以上是个人结合各种资料的理解，如果存在错误恳请各位留言指正，感谢！</strong></p><hr><h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
    
    </summary>
    
    
      <category term="Zookeeper" scheme="https://tomandersen-cc.github.io/categories/Zookeeper/"/>
    
    
      <category term="CentOS7" scheme="https://tomandersen-cc.github.io/tags/CentOS7/"/>
    
      <category term="大数据" scheme="https://tomandersen-cc.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Zookeeper" scheme="https://tomandersen-cc.github.io/tags/Zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>Zookeeper修改运行日志zookeeper.out输出路径</title>
    <link href="https://tomandersen-cc.github.io/2020/02/20/Zookeeper%E4%BF%AE%E6%94%B9%E8%BF%90%E8%A1%8C%E6%97%A5%E5%BF%97zookeeper-out%E8%BE%93%E5%87%BA%E8%B7%AF%E5%BE%84/"/>
    <id>https://tomandersen-cc.github.io/2020/02/20/Zookeeper%E4%BF%AE%E6%94%B9%E8%BF%90%E8%A1%8C%E6%97%A5%E5%BF%97zookeeper-out%E8%BE%93%E5%87%BA%E8%B7%AF%E5%BE%84/</id>
    <published>2020-02-20T01:19:33.000Z</published>
    <updated>2020-03-06T12:24:20.972Z</updated>
    
    <content type="html"><![CDATA[<hr><a id="more"></a><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul><li>Zookeeper中运行日志 <strong>zookeeper.out</strong> 文件的输出路径默认为启动脚本的当前路径，导致Zookeeper集群启动失败时总是不记得输出日志在哪儿，不便于查找错误原因，因此很有必要设置固定路径来保存运行日志</li><li>在本次实验之前已经将 <strong>dataDir</strong> 和 <strong>dataLogDir</strong> 分别设置为<code>$ZOOKEEPER_HOME/zkData</code>和<code>$ZOOKEEPER_HOME/logs/transaction</code>，都是在<code>$ZOOKEEPER_HOME/conf/zoo.cfg</code>中配置。值得注意的是在此<code>zoo.cfg</code>文件中配置必须使用<strong>绝对路径</strong>，不能使用环境变量<code>$ZOOKEEPER_HOME</code>，此处是为了描述方便才使用此变量名</li><li>本次实验将把运行日志文件 <strong>zookeeper.out</strong> 输出路径指定为<code>$ZOOKEEPER_HOME/logs/runtime/</code>路径下</li></ul><hr><h2 id="简单配置"><a href="#简单配置" class="headerlink" title="简单配置"></a>简单配置</h2><h3 id="修改-ZOOKEEPER-HOME-bin-zkEnv-sh"><a href="#修改-ZOOKEEPER-HOME-bin-zkEnv-sh" class="headerlink" title="修改$ZOOKEEPER_HOME/bin/zkEnv.sh"></a>修改<code>$ZOOKEEPER_HOME/bin/zkEnv.sh</code></h3><ul><li>将<code>ZOO_LOG_DIR</code>设置成自定义路径，本次设置为<code>$ZOOBINDIR/../logs/runtime</code>，其中<code>ZOOBINDIR</code>变量是此脚本开头获取的Zookeeper的bin路径，我们直接以此来定位自己的日志路径即可。具体如下：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改前</span></span><br><span class="line"> 54 <span class="keyword">if</span> [ <span class="string">"x<span class="variable">$&#123;ZOO_LOG_DIR&#125;</span>"</span> = <span class="string">"x"</span> ]</span><br><span class="line"> 55 <span class="keyword">then</span></span><br><span class="line"> 56     ZOO_LOG_DIR=<span class="string">"."</span></span><br><span class="line"> 57 <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改后</span></span><br><span class="line"> 54 <span class="keyword">if</span> [ <span class="string">"x<span class="variable">$&#123;ZOO_LOG_DIR&#125;</span>"</span> = <span class="string">"x"</span> ]</span><br><span class="line"> 55 <span class="keyword">then</span></span><br><span class="line"> 56     <span class="comment">#ZOO_LOG_DIR="."</span></span><br><span class="line"> 57     <span class="comment">#自定义运行日志文件输出路径</span></span><br><span class="line"> 58     ZOO_LOG_DIR=<span class="string">"<span class="variable">$ZOOBINDIR</span>/../logs/runtime"</span></span><br><span class="line"> 59 <span class="keyword">fi</span></span><br></pre></td></tr></table></figure><ul><li>这样在每次使用<code>zkSever</code>的时候，都能将运行日志 <strong>zookeeper.out</strong> 输出到指定路径下，但这样配置有个问题，就是每次运行Zookeeper时，此日志都会被覆盖，而不是append到文件中，故每次运行结束后只会保存有本次运行日志，若单次运行时间很长也会导致日志文件也很大。</li></ul><hr><h2 id="进阶配置"><a href="#进阶配置" class="headerlink" title="进阶配置"></a>进阶配置</h2><ul><li>在之前的配置中，我们只是实现了保存本次Zookeeper运行日志在指定路径下，这次我们通过配置<code>$ZOOKEEPER_HOME/conf/log4j.properties</code>来使用log4j日志框架将Zookeeper每次的运行日志都保存到指定路径下</li></ul><h3 id="配置-ZOOKEEPER-HOME-conf-log4j-properties文件"><a href="#配置-ZOOKEEPER-HOME-conf-log4j-properties文件" class="headerlink" title="配置$ZOOKEEPER_HOME/conf/log4j.properties文件"></a>配置<code>$ZOOKEEPER_HOME/conf/log4j.properties</code>文件</h3><ul><li>修改其中的<code>zookeeper.root.logger zookeeper.log.dir log4j.appender.ROLLINGFILE.MaxBackupIndex</code>共三个参数。具体如下：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改前</span></span><br><span class="line">zookeeper.root.logger=INFO, CONSOLE</span><br><span class="line">...</span><br><span class="line">zookeeper.log.dir=.</span><br><span class="line">...</span><br><span class="line"><span class="comment">#log4j.appender.ROLLINGFILE.MaxBackupIndex=10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改后</span></span><br><span class="line">zookeeper.root.logger=INFO, CONSOLE, ROLLINGFILE</span><br><span class="line">zookeeper.log.dir=/opt/module/zookeeper-3.4.14/logs/runtime</span><br><span class="line">log4j.appender.ROLLINGFILE.MaxBackupIndex=5</span><br></pre></td></tr></table></figure><ul><li>其中参数<code>zookeeper.root.logger</code>是设置日志优先级和打印方式，默认为控制台打印<code>CONSOLE</code>，而在<code>zkServer.sh</code>中会启动后台命令，将控制台输出的日志输出重定向到 <strong>zookeeper.out</strong> 文件中。修改后添加了滚动产生文件输出方式<code>ROLLINGFILE</code>；通过参数<code>zookeeper.log.dir</code>设置日志文件 <strong>zookeeper.log</strong> 的存储路径，这里直接采用绝对路径，相对路径可能不识别；通过参数<code>log4j.appender.ROLLINGFILE.MaxBackupIndex</code>设置最大日志数量，每个日志文件大小最大默认为10MB，以此文件大小进行分割，默认日志文件名为 <strong>zookeeper.log</strong>，其中所有参数都可以自定义修改。其余参数不再赘述。</li></ul><h3 id="修改-ZOOKEEPER-HOME-bin-zkEnv-sh-1"><a href="#修改-ZOOKEEPER-HOME-bin-zkEnv-sh-1" class="headerlink" title="修改$ZOOKEEPER_HOME/bin/zkEnv.sh"></a>修改<code>$ZOOKEEPER_HOME/bin/zkEnv.sh</code></h3><ul><li>将其中的<code>ZOO_LOG4J_PROP</code>设置成与log4j配置文件中相同，避免参数覆盖。具体如下：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改前</span></span><br><span class="line"> 61 <span class="keyword">if</span> [ <span class="string">"x<span class="variable">$&#123;ZOO_LOG4J_PROP&#125;</span>"</span> = <span class="string">"x"</span> ]</span><br><span class="line"> 62 <span class="keyword">then</span></span><br><span class="line"> 63     ZOO_LOG4J_PROP=<span class="string">"INFO,CONSOLE"</span></span><br><span class="line"> 64 <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改后</span></span><br><span class="line"> 61 <span class="keyword">if</span> [ <span class="string">"x<span class="variable">$&#123;ZOO_LOG4J_PROP&#125;</span>"</span> = <span class="string">"x"</span> ]</span><br><span class="line"> 62 <span class="keyword">then</span></span><br><span class="line"> 63     <span class="comment">#ZOO_LOG4J_PROP="INFO,CONSOLE"</span></span><br><span class="line"> 64     <span class="comment">#自定义运行日志信息输出方式,增加了滚动输出的方式,初始只有控制台输出</span></span><br><span class="line"> 65     ZOO_LOG4J_PROP=<span class="string">"INFO,CONSOLE,ROLLINGFILE"</span></span><br><span class="line"> 66 <span class="keyword">fi</span></span><br></pre></td></tr></table></figure><h3 id="修改-ZOOKEEPER-HOME-bin-zkServer-sh"><a href="#修改-ZOOKEEPER-HOME-bin-zkServer-sh" class="headerlink" title="修改$ZOOKEEPER_HOME/bin/zkServer.sh"></a>修改<code>$ZOOKEEPER_HOME/bin/zkServer.sh</code></h3><ul><li>修改原始输出逻辑，不再将本次运行日志信息所有输出到 <strong>zookeeper.out</strong> 中，而是使用log4j框架输出到 <strong>zookeeper.log</strong> 中，便于管理。而 <strong>zookeeper.out</strong> 只用于输出标准错误。具体如下所示：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改前</span></span><br><span class="line">141     nohup <span class="string">"<span class="variable">$JAVA</span>"</span> <span class="string">"-Dzookeeper.log.dir=<span class="variable">$&#123;ZOO_LOG_DIR&#125;</span>"</span> <span class="string">"-Dzookeeper.root.logger=<span class="variable">$&#123;ZOO_LOG4J_PROP&#125;</span>"</span> \</span><br><span class="line">142     -cp <span class="string">"<span class="variable">$CLASSPATH</span>"</span> <span class="variable">$JVMFLAGS</span> <span class="variable">$ZOOMAIN</span> <span class="string">"<span class="variable">$ZOOCFG</span>"</span> &gt; <span class="string">"<span class="variable">$_ZOO_DAEMON_OUT</span>"</span> 2&gt;&amp;1 &lt; /dev/null &amp;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将原始指令注释,设置新的输出逻辑,只讲标准错误输出到zookeeper.out中.修改后:</span></span><br><span class="line">141 <span class="comment">#    nohup "$JAVA" "-Dzookeeper.log.dir=$&#123;ZOO_LOG_DIR&#125;" "-Dzookeeper.root.logger=$&#123;ZOO_LOG4J_PROP&#125;" \</span></span><br><span class="line">142 <span class="comment">#    -cp "$CLASSPATH" $JVMFLAGS $ZOOMAIN "$ZOOCFG" &gt; "$_ZOO_DAEMON_OUT" 2&gt;&amp;1 &lt; /dev/null &amp;</span></span><br><span class="line">143     nohup <span class="string">"<span class="variable">$JAVA</span>"</span> <span class="string">"-Dzookeeper.log.dir=<span class="variable">$&#123;ZOO_LOG_DIR&#125;</span>"</span> <span class="string">"-Dzookeeper.root.logger=<span class="variable">$&#123;ZOO_LOG4J_PROP&#125;</span>"</span> \</span><br><span class="line">144     -cp <span class="string">"<span class="variable">$CLASSPATH</span>"</span> <span class="variable">$JVMFLAGS</span> <span class="variable">$ZOOMAIN</span> <span class="string">"<span class="variable">$ZOOCFG</span>"</span> 2&gt; <span class="string">"<span class="variable">$_ZOO_DAEMON_OUT</span>"</span> 1&gt; /dev/null &amp;</span><br></pre></td></tr></table></figure><hr><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>这样就能保证历史运行日志都能输出到指定文件夹中，并且不会因为运行日志文件堆积造成负载点爆炸。而标准错误都会输出到 <strong>zookeeper.out</strong> 文件中，和运行日志放置在同一文件夹中。当需要查看脚本命令错误时，可以查看 <strong>zookeeper.out</strong> 文件，当需要查看程序错误时就查看 <strong>zookeeper.log</strong> 运行日志文件</li></ul><hr><h2 id="End"><a href="#End" class="headerlink" title="End~"></a>End~</h2>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
    
    </summary>
    
    
      <category term="Zookeeper" scheme="https://tomandersen-cc.github.io/categories/Zookeeper/"/>
    
    
      <category term="CentOS7" scheme="https://tomandersen-cc.github.io/tags/CentOS7/"/>
    
      <category term="大数据" scheme="https://tomandersen-cc.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Zookeeper" scheme="https://tomandersen-cc.github.io/tags/Zookeeper/"/>
    
  </entry>
  
</feed>
